{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8762d55-2694-4595-a97c-8a11137b1c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==2.7.0 in /opt/conda/lib/python3.11/site-packages (from -r mlops-workflow/pipeline/requirements.txt (line 1)) (2.7.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from -r mlops-workflow/pipeline/requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (0.15)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.12.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.13.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (4.25.4)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.11/site-packages (from kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (1.26.18)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (1.61.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.11/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (68.2.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (1.6.4)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.7.0->-r mlops-workflow/pipeline/requirements.txt (line 1)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r mlops-workflow/pipeline/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b78c66b-718c-42aa-a430-c85213c967ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'x-powered-by': 'Express', 'content-type': 'application/json', 'date': 'Mon, 12 Aug 2024 18:37:38 GMT', 'content-length': '811', 'x-envoy-upstream-service-time': '43', 'server': 'envoy'})\nHTTP response body: {\"error\":\"List experiments failed: Failed to authorize with API: Failed to authorize with API: Unauthenticated: Failed to check authorization. User identity is empty in the request header: Unauthenticated: Request header error: there is no user identity header.: Request header error: there is no user identity header.\",\"code\":16,\"message\":\"List experiments failed: Failed to authorize with API: Failed to authorize with API: Unauthenticated: Failed to check authorization. User identity is empty in the request header: Unauthenticated: Request header error: there is no user identity header.: Request header error: there is no user identity header.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.Status\",\"code\":16,\"message\":\"Failed to check authorization. User identity is empty in the request header\"}]}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 443\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Submit the pipeline to the KFP cluster\u001b[39;00m\n\u001b[1;32m    441\u001b[0m client \u001b[38;5;241m=\u001b[39m kfp\u001b[38;5;241m.\u001b[39mClient(host\u001b[38;5;241m=\u001b[39mKFP_HOST)  \u001b[38;5;66;03m# Use the configured KFP host\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run_from_pipeline_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmy_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_caching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepo_url\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mREPO_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcloned_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLONED_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbranch_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mBRANCH_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgithub_username\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGITHUB_USERNAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgithub_token\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mGITHUB_TOKEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mremote_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mREMOTE_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mremote_url\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mREMOTE_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mminio_url\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMINIO_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccess_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mACCESS_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msecret_key\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSECRET_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdvc_file_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDVC_FILE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdvc_file_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDVC_FILE_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mframework\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mFRAMEWORK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnamespace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mNAMESPACE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvc_account\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSVC_ACCOUNT\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m#upload to Kubeflow \u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# client.upload_pipeline(pipeline_package_path=pipeline_filename,\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m#                        pipeline_name=\"mlops\",\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m#                        namespace = \"kubeflow\")\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:1023\u001b[0m, in \u001b[0;36mClient.create_run_from_pipeline_func\u001b[0;34m(self, pipeline_func, arguments, run_name, experiment_name, namespace, pipeline_root, enable_caching, service_account, experiment_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m pipeline_package_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpipeline.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1018\u001b[0m compiler\u001b[38;5;241m.\u001b[39mCompiler()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1019\u001b[0m     pipeline_func\u001b[38;5;241m=\u001b[39mpipeline_func,\n\u001b[1;32m   1020\u001b[0m     package_path\u001b[38;5;241m=\u001b[39mpipeline_package_path,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_run_from_pipeline_package\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_package_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_caching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_caching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:1090\u001b[0m, in \u001b[0;36mClient.create_run_from_pipeline_package\u001b[0;34m(self, pipeline_file, arguments, run_name, experiment_name, namespace, pipeline_root, enable_caching, service_account, experiment_id)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1087\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChanging experiment name from \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moverridden_experiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1088\u001b[0m         )\n\u001b[1;32m   1089\u001b[0m     experiment_name \u001b[38;5;241m=\u001b[39m overridden_experiment_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefault\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1090\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m     experiment_id \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mexperiment_id\n\u001b[1;32m   1094\u001b[0m run_name \u001b[38;5;241m=\u001b[39m run_name \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1095\u001b[0m     pipeline_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1096\u001b[0m     datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:462\u001b[0m, in \u001b[0;36mClient.create_experiment\u001b[0;34m(self, name, description, namespace)\u001b[0m\n\u001b[1;32m    460\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Ignore error if the experiment does not exist.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo experiment is found with name\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:590\u001b[0m, in \u001b[0;36mClient.get_experiment\u001b[0;34m(self, experiment_id, experiment_name, namespace)\u001b[0m\n\u001b[1;32m    582\u001b[0m experiment_filter \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps({\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicates\u001b[39m\u001b[38;5;124m'\u001b[39m: [{\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m'\u001b[39m: _FILTER_OPERATIONS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEQUALS\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m     }]\n\u001b[1;32m    588\u001b[0m })\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_experiment_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_api\u001b[38;5;241m.\u001b[39mlist_experiments(\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mexperiment_filter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api/experiment_service_api.py:565\u001b[0m, in \u001b[0;36mExperimentServiceApi.list_experiments\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finds all experiments. Supports pagination, and sorting on certain fields.  # noqa: E501\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m \u001b[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;124;03m:rtype: V2beta1ListExperimentsResponse\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    564\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_experiments_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api/experiment_service_api.py:661\u001b[0m, in \u001b[0;36mExperimentServiceApi.list_experiments_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# Authentication setting\u001b[39;00m\n\u001b[1;32m    659\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/apis/v2beta1/experiments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV2beta1ListExperimentsResponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:364\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    372\u001b[0m                                                method, path_params,\n\u001b[1;32m    373\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m                                                _request_timeout,\n\u001b[1;32m    382\u001b[0m                                                _host))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:188\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    190\u001b[0m content_type \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mgetheader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:181\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/api_client.py:389\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mHEAD(url,\n\u001b[1;32m    396\u001b[0m                                  query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    397\u001b[0m                                  _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    398\u001b[0m                                  _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    399\u001b[0m                                  headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/rest.py:230\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[0;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGET\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m         _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kfp_server_api/rest.py:224\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m299\u001b[39m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mApiException\u001b[0m: (401)\nReason: Unauthorized\nHTTP response headers: HTTPHeaderDict({'x-powered-by': 'Express', 'content-type': 'application/json', 'date': 'Mon, 12 Aug 2024 18:37:38 GMT', 'content-length': '811', 'x-envoy-upstream-service-time': '43', 'server': 'envoy'})\nHTTP response body: {\"error\":\"List experiments failed: Failed to authorize with API: Failed to authorize with API: Unauthenticated: Failed to check authorization. User identity is empty in the request header: Unauthenticated: Request header error: there is no user identity header.: Request header error: there is no user identity header.\",\"code\":16,\"message\":\"List experiments failed: Failed to authorize with API: Failed to authorize with API: Unauthenticated: Failed to check authorization. User identity is empty in the request header: Unauthenticated: Request header error: there is no user identity header.: Request header error: there is no user identity header.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.Status\",\"code\":16,\"message\":\"Failed to check authorization. User identity is empty in the request header\"}]}\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, ClassificationMetrics, OutputPath\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GitHub username and token from environment variables\n",
    "GITHUB_USERNAME = os.getenv(\"GITHUB_USERNAME\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Define configuration variables\n",
    "REPO_URL = \"https://github.com/danilonicioka/mlops-workflow.git\"\n",
    "CLONED_DIR = \"mlops-workflow\"\n",
    "BRANCH_NAME = \"tests\"\n",
    "PIPELINE_ID = \"my-pipeline-id\"\n",
    "PIPELINE_NAME = \"mlops\"\n",
    "KFP_HOST = \"http://ml-pipeline-ui.kubeflow:80\"  # KFP host URL\n",
    "\n",
    "# Define DVC remote configuration variables\n",
    "REMOTE_NAME = \"minio_remote\"\n",
    "REMOTE_URL = \"s3://dvc-data\"\n",
    "MINIO_URL = \"http://minio-svc.minio:9000\"\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "DVC_FILE_DIR = 'data/external'\n",
    "DVC_FILE_NAME = 'dataset.csv'\n",
    "\n",
    "# Model serve config vars\n",
    "MODEL_NAME = \"youtubegoes5g\"\n",
    "FRAMEWORK = \"pytorch\"\n",
    "NAMESPACE = \"kubeflow\"\n",
    "SVC_ACCOUNT = \"pipeline-runner\"\n",
    "\n",
    "# Define a KFP component factory function for data ingestion\n",
    "@component(base_image=\"python:3.11.9\",packages_to_install=['gitpython', 'dvc==3.51.1', 'dvc-s3==3.2.0', 'numpy==1.25.2', 'pandas==2.0.3'])\n",
    "def data_ingestion(\n",
    "    repo_url: str,\n",
    "    cloned_dir: str,\n",
    "    branch_name: str,\n",
    "    github_username: str,\n",
    "    github_token: str,\n",
    "    remote_name: str,\n",
    "    remote_url: str,\n",
    "    minio_url: str,\n",
    "    access_key: str,\n",
    "    secret_key: str,\n",
    "    dvc_file_dir: str,\n",
    "    dvc_file_name: str,\n",
    "    dataset_artifact: Output[Dataset]\n",
    "    ):\n",
    "    from git import Repo\n",
    "    from subprocess import run, CalledProcessError\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    def clone_repository_with_token(repo_url, cloned_dir, branch_name, github_username, github_token):\n",
    "        \"\"\"Clone a Git repository using a GitHub token in the URL and specifying the branch.\"\"\"\n",
    "        try:\n",
    "            # Construct the URL with the GitHub username and token\n",
    "            url_with_token = f\"https://{github_username}:{github_token}@{repo_url.split('//')[1]}\"\n",
    "            \n",
    "            # Clone the repository from the specified branch\n",
    "            repo = Repo.clone_from(url_with_token, cloned_dir, branch=branch_name)\n",
    "            return \"Repository cloned successfully\"\n",
    "        except Exception as e:\n",
    "            return f\"Error occurred during repository cloning: {e}\"\n",
    "\n",
    "    def configure_dvc_remote(cloned_dir, remote_name, remote_url, minio_url, access_key, secret_key):\n",
    "        \"\"\"Configure the Minio bucket as the DVC remote repository using the `dvc remote` commands.\"\"\"\n",
    "        try:\n",
    "            # Add the remote\n",
    "            run(\n",
    "                ['dvc', 'remote', 'add', '-d', remote_name, remote_url],\n",
    "                cwd=cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Configure the endpoint URL\n",
    "            run(\n",
    "                ['dvc', 'remote', 'modify', remote_name, 'endpointurl', minio_url],\n",
    "                cwd=cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Configure access key ID\n",
    "            run(\n",
    "                ['dvc', 'remote', 'modify', remote_name, 'access_key_id', access_key],\n",
    "                cwd=cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Configure secret access key\n",
    "            run(\n",
    "                ['dvc', 'remote', 'modify', remote_name, 'secret_access_key', secret_key],\n",
    "                cwd=cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            return f'Successfully configured Minio bucket as DVC remote repository: {remote_name}'\n",
    "        except CalledProcessError as e:\n",
    "            # Log and raise any errors\n",
    "            return f'Failed to configure DVC remote: {e.stderr}'\n",
    "\n",
    "    def perform_dvc_pull(cloned_dir, remote_name):\n",
    "        \"\"\"Perform a DVC pull to synchronize local data with the remote repository.\"\"\"\n",
    "        try:\n",
    "            # Run the `dvc pull` command\n",
    "            result = run(['dvc', 'pull', '-r', remote_name], cwd=cloned_dir, capture_output=True, text=True)\n",
    "            \n",
    "            # Check if the command executed successfully\n",
    "            if result.returncode != 0:\n",
    "                # Log and raise an error if the command failed\n",
    "                error_message = f\"dvc pull failed with error: {result.stderr}\"\n",
    "                raise Exception(error_message)\n",
    "            \n",
    "            # Log successful operation\n",
    "            return \"Successfully pulled data from remote DVC repository\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log and handle the error\n",
    "            return f\"Error occurred during dvc pull: {e}\"\n",
    "\n",
    "    # Call the functions\n",
    "    clone_result = clone_repository_with_token(repo_url, cloned_dir, branch_name, github_username, github_token)\n",
    "    configure_result = configure_dvc_remote(cloned_dir, remote_name, remote_url, minio_url, access_key, secret_key)\n",
    "    dvc_pull_result = perform_dvc_pull(cloned_dir, remote_name)\n",
    "\n",
    "    # Save dataset with pandas in Dataset artifact\n",
    "    pulled_dataset_path = os.path.join(cloned_dir, dvc_file_dir, dvc_file_name)\n",
    "    tmp_dataset_path = \"/tmp/\" + dvc_file_name\n",
    "    dataset = pd.read_csv(pulled_dataset_path)\n",
    "    dataset.to_pickle(tmp_dataset_path)\n",
    "    os.rename(tmp_dataset_path, dataset_artifact.path)\n",
    "    \n",
    "# Component for data preparation\n",
    "@component(base_image=\"python:3.11.9\", packages_to_install=['pandas==2.0.3', 'numpy==1.25.2', 'torch==2.3.0', 'scikit-learn==1.2.2', 'imblearn'])\n",
    "def data_preparation(\n",
    "    dataset_artifact: Input[Dataset],\n",
    "    X_train_artifact: Output[Dataset], \n",
    "    X_test_artifact: Output[Dataset],\n",
    "    y_train_artifact: Output[Dataset],\n",
    "    y_test_artifact: Output[Dataset],\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Load dataset from Dataset artifact\n",
    "    df = pd.read_pickle(dataset_artifact.path)\n",
    "\n",
    "    # Handle null values and replace specific characters\n",
    "    #df = df.replace([' ', '-',np.nan], 0) # There are null values\n",
    "    df = df.replace([' ', '-', np.nan], np.nan)\n",
    "\n",
    "    # Selective columns for mean calculation\n",
    "    columns_to_convert = [\n",
    "        'CQI1', 'CQI2', 'CQI3', 'cSTD CQI', 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n",
    "        'RSRP1', 'RSRP2', 'RSRP3', 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n",
    "        'RSRQ1', 'RSRQ2', 'RSRQ3', 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n",
    "        'SNR1', 'SNR2', 'SNR3', 'sMajority', 's25 P', 's50 P', 's75 P'\n",
    "    ]\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype(float)\n",
    "\n",
    "    # Replace np.nan with mean values for selective columns\n",
    "    df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n",
    "\n",
    "    # Convert 'Stall' column to numerical values\n",
    "    df['Stall'].replace({'Yes': 1, 'No': 0}, inplace=True)\n",
    "\n",
    "    X = df[columns_to_convert].values\n",
    "    y = df['Stall'].values\n",
    "\n",
    "    # Apply SMOTE for balancing the dataset\n",
    "    # oversample = SMOTE(random_state=random_state)\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    X_train_path = \"/tmp/X_train.pt\"\n",
    "    X_test_path = \"/tmp/X_test.pt\"\n",
    "    y_train_path = \"/tmp/y_train.pt\"\n",
    "    y_test_path = \"/tmp/y_test.pt\"\n",
    "    torch.save(X_train, X_train_path)\n",
    "    os.rename(X_train_path, X_train_artifact.path)\n",
    "\n",
    "    torch.save(X_test, X_test_path)\n",
    "    os.rename(X_test_path, X_test_artifact.path)\n",
    "\n",
    "    torch.save(y_train, y_train_path)\n",
    "    os.rename(y_train_path, y_train_artifact.path)\n",
    "\n",
    "    torch.save(y_test, y_test_path)\n",
    "    os.rename(y_test_path, y_test_artifact.path)\n",
    "\n",
    "# Component for model training\n",
    "@component(base_image=\"python:3.11.9\", packages_to_install=['torch==2.3.0', 'scikit-learn==1.2.2', 'numpy==1.25.2'])\n",
    "def model_training(\n",
    "    X_train_artifact: Input[Dataset], \n",
    "    X_test_artifact: Input[Dataset],\n",
    "    y_train_artifact: Input[Dataset],\n",
    "    y_test_artifact: Input[Dataset],\n",
    "    metrics: Output[Metrics], \n",
    "    classification_metrics: Output[ClassificationMetrics], \n",
    "    model_trained_artifact: Output[Model],\n",
    "    model_trained_artifact_path: OutputPath(str),\n",
    "    lr: float = 0.0001,\n",
    "    epochs: int = 3500,\n",
    "    print_every: int = 500\n",
    "    ):\n",
    "    import os\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "    # Build model with non-linear activation function\n",
    "    class InterruptionModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer_1 = nn.Linear(in_features=29, out_features=200)\n",
    "            self.layer_2 = nn.Linear(in_features=200, out_features=100)\n",
    "            self.layer_3 = nn.Linear(in_features=100, out_features=1)\n",
    "            self.relu = nn.ReLU() # <- add in ReLU activation function\n",
    "            # Can also put sigmoid in the model\n",
    "            # This would mean you don't need to use it on the predictions\n",
    "            # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Intersperse the ReLU activation function between layers\n",
    "            return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = InterruptionModel().to(device)\n",
    "\n",
    "    # Setup loss and optimizer\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def accuracy_fn(y_true, y_pred):\n",
    "        correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "        acc = (correct / len(y_pred)) * 100\n",
    "        return acc\n",
    "    \n",
    "    # Fit the model\n",
    "    torch.manual_seed(42)\n",
    "    epochs = epochs\n",
    "\n",
    "    # Put all data on target device\n",
    "    X_train = torch.load(X_train_artifact.path)\n",
    "    X_test = torch.load(X_test_artifact.path)\n",
    "    y_train = torch.load(y_train_artifact.path)\n",
    "    y_test = torch.load(y_test_artifact.path)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1. Forward pass\n",
    "        y_logits = model(X_train).squeeze()\n",
    "\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "        # 2. Calculate loss and accuracy\n",
    "        loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
    "        acc = accuracy_fn(y_true=y_train,\n",
    "                        y_pred=y_pred)\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "        # 1. Forward pass\n",
    "            test_logits = model(X_test).squeeze()\n",
    "            #print(test_logits.shape)\n",
    "            test_pred = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "            # 2. Calcuate loss and accuracy\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_true=y_test,\n",
    "                                y_pred=test_pred)\n",
    "\n",
    "\n",
    "        # Print out what's happening\n",
    "        #if epoch % print_every == 0:\n",
    "        #    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_preds = torch.round(torch.sigmoid(model(X_test))).squeeze()\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            predictions = y_preds.cpu().numpy() #if it is cuda, then this, otherwise y_pred.numpy()\n",
    "            true_labels = y_test.cpu().numpy()\n",
    "        else:\n",
    "            predictions = y_preds.numpy()\n",
    "            true_labels = y_test.numpy()\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cmatrix = confusion_matrix(true_labels, predictions)\n",
    "        #print(\"Confusion Matrix:\", cmatrix)\n",
    "\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        metrics.log_metric(\"Accuracy\", accuracy)\n",
    "        #print('Accuracy: %f' % accuracy)\n",
    "\n",
    "        precision = precision_score(true_labels,  predictions, average='weighted')\n",
    "        metrics.log_metric(\"Precision\", precision)\n",
    "        #print('Precision: %f' % precision)\n",
    "\n",
    "        recall = recall_score(true_labels, predictions, average='weighted')\n",
    "        metrics.log_metric(\"Recall\", recall)\n",
    "        #print('Recall: %f' % recall)\n",
    "\n",
    "        microf1 = f1_score(true_labels, predictions, average='micro')\n",
    "        metrics.log_metric(\"Micro F1 score\", microf1)\n",
    "        #print('Micro F1 score: %f' % microf1)\n",
    "\n",
    "        macrof1 = f1_score(true_labels, predictions, average='macro')\n",
    "        metrics.log_metric(\"Macro F1 score\", macrof1)\n",
    "        #print('Macro F1 score: %f' % macrof1)\n",
    "\n",
    "        target_names = ['No-Stall', 'Stall']\n",
    "        # Print precision-recall report\n",
    "        #print(classification_report(true_labels, predictions, target_names=target_names))\n",
    "\n",
    "        # Classification Metrics artifact\n",
    "        cmatrix = cmatrix.tolist()\n",
    "        target_names = ['No-Stall', 'Stall']\n",
    "        classification_metrics.log_confusion_matrix(target_names, cmatrix)\n",
    "\n",
    "        # Save model\n",
    "        model_path = \"/tmp/model.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        os.rename(model_path, model_trained_artifact.path)\n",
    "\n",
    "        with open(model_trained_artifact_path, 'w') as f:\n",
    "            f.write(model_trained_artifact.path)\n",
    "        \n",
    "kserve_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml\"\n",
    ")\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline(\n",
    "    repo_url: str,\n",
    "    cloned_dir: str,\n",
    "    branch_name: str,\n",
    "    github_username: str,\n",
    "    github_token: str,\n",
    "    remote_name: str,\n",
    "    remote_url: str,\n",
    "    minio_url: str,\n",
    "    access_key: str,\n",
    "    secret_key: str,\n",
    "    dvc_file_dir: str,\n",
    "    dvc_file_name: str,\n",
    "    model_name: str,\n",
    "    framework: str,\n",
    "    namespace: str,\n",
    "    svc_account: str,\n",
    "    action: str = 'apply'\n",
    "):\n",
    "    data_ingestion_task = data_ingestion(\n",
    "        repo_url=repo_url,\n",
    "        cloned_dir=cloned_dir,\n",
    "        branch_name=branch_name,\n",
    "        github_username=github_username,\n",
    "        github_token=github_token,\n",
    "        remote_name=remote_name,\n",
    "        remote_url=remote_url,\n",
    "        minio_url=minio_url,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        dvc_file_dir=dvc_file_dir,\n",
    "        dvc_file_name=dvc_file_name)\n",
    "    dataset_artifact = data_ingestion_task.output\n",
    "    data_preparation_task = data_preparation(dataset_artifact=dataset_artifact)\n",
    "    X_train_artifact = data_preparation_task.outputs[\"X_train_artifact\"]\n",
    "    X_test_artifact = data_preparation_task.outputs[\"X_test_artifact\"]\n",
    "    y_train_artifact = data_preparation_task.outputs[\"y_train_artifact\"]\n",
    "    y_test_artifact = data_preparation_task.outputs[\"y_test_artifact\"]\n",
    "    model_training_task = model_training(X_train_artifact=X_train_artifact, \n",
    "                                         X_test_artifact=X_test_artifact, \n",
    "                                         y_train_artifact=y_train_artifact, \n",
    "                                         y_test_artifact=y_test_artifact)\n",
    "    model_trained_artifact_path = model_training_task.outputs[\"model_trained_artifact_path\"]\n",
    "\n",
    "    # Replace 'http://' with 's3://' for kserve model uri\n",
    "    model_trained_artifact_s3_path = str(model_trained_artifact_path).replace(\"http://\", \"s3://\")\n",
    "\n",
    "    kserve_task = kserve_op(\n",
    "        action=action,\n",
    "        model_name=model_name,\n",
    "        model_uri=model_trained_artifact_s3_path,\n",
    "        namespace=namespace,\n",
    "        framework=framework,\n",
    "        service_account=svc_account\n",
    "    )\n",
    "    #model_serving_task = model_serving(model_trained_artifact=model_trained_artifact)\n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_filename = f\"{PIPELINE_NAME}.yaml\"\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path=pipeline_filename)\n",
    "\n",
    "# Submit the pipeline to the KFP cluster\n",
    "client = kfp.Client(host=KFP_HOST)  # Use the configured KFP host\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    my_pipeline, \n",
    "    enable_caching=False,\n",
    "    arguments={\n",
    "        'repo_url': REPO_URL,\n",
    "        'cloned_dir': CLONED_DIR,\n",
    "        'branch_name': BRANCH_NAME,\n",
    "        'github_username': GITHUB_USERNAME,\n",
    "        'github_token': GITHUB_TOKEN,\n",
    "        'remote_name': REMOTE_NAME,\n",
    "        'remote_url': REMOTE_URL,\n",
    "        'minio_url': MINIO_URL,\n",
    "        'access_key': ACCESS_KEY,\n",
    "        'secret_key': SECRET_KEY,\n",
    "        'dvc_file_dir': DVC_FILE_DIR,\n",
    "        'dvc_file_name': DVC_FILE_NAME,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'framework': FRAMEWORK,\n",
    "        'namespace': NAMESPACE,\n",
    "        'svc_account': SVC_ACCOUNT\n",
    "    })\n",
    "\n",
    "#upload to Kubeflow \n",
    "# client.upload_pipeline(pipeline_package_path=pipeline_filename,\n",
    "#                        pipeline_name=\"mlops\",\n",
    "#                        namespace = \"kubeflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95208a6e-fab2-402b-af2d-db1b8c3d31f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
