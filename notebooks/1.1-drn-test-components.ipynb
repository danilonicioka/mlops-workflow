{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fdb664-7216-4d08-aba0-afb5986d6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements-components-test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f9d15-afb1-45d8-816f-3cedd089f1c3",
   "metadata": {},
   "source": [
    "# Model archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b6ca209-9f06-40e0-92a3-75b8d3c3a2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod created. status='{'conditions': None,\n",
      " 'container_statuses': None,\n",
      " 'ephemeral_container_statuses': None,\n",
      " 'host_ip': None,\n",
      " 'init_container_statuses': None,\n",
      " 'message': None,\n",
      " 'nominated_node_name': None,\n",
      " 'phase': 'Pending',\n",
      " 'pod_i_ps': None,\n",
      " 'pod_ip': None,\n",
      " 'qos_class': 'Burstable',\n",
      " 'reason': None,\n",
      " 'start_time': None}'\n",
      "Error from server (Conflict): {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"persistentvolumeclaims \\\"model-pv-claim\\\" already exists\",\"reason\":\"AlreadyExists\",\"details\":{\"name\":\"model-pv-claim\",\"kind\":\"persistentvolumeclaims\"},\"code\":409}\n",
      "\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(0)\nReason: Handshake status 500 Internal Server Error -+-+- {'content-length': '35', 'content-type': 'text/plain; charset=utf-8', 'date': 'Thu, 26 Sep 2024 17:30:16 GMT'} -+-+- b'container not found (\"model-store\")'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebSocketBadStatusException\u001b[0m               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/stream/ws_client.py:521\u001b[0m, in \u001b[0;36mwebsocket_call\u001b[0;34m(configuration, _method, url, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mWSClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _preload_content:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/stream/ws_client.py:65\u001b[0m, in \u001b[0;36mWSClient.__init__\u001b[0;34m(self, configuration, url, headers, capture_all)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all \u001b[38;5;241m=\u001b[39m _IgnoredIO()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_websocket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/stream/ws_client.py:487\u001b[0m, in \u001b[0;36mcreate_websocket\u001b[0;34m(configuration, url, headers)\u001b[0m\n\u001b[1;32m    485\u001b[0m     connect_opt \u001b[38;5;241m=\u001b[39m websocket_proxycare(connect_opt, configuration, url, headers)\n\u001b[0;32m--> 487\u001b[0m \u001b[43mwebsocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconnect_opt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m websocket\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/websocket/_core.py:255\u001b[0m, in \u001b[0;36mWebSocket.connect\u001b[0;34m(self, url, **options)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandshake_response \u001b[38;5;241m=\u001b[39m \u001b[43mhandshake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maddrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredirect_limit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/websocket/_handshake.py:57\u001b[0m, in \u001b[0;36mhandshake\u001b[0;34m(sock, url, hostname, port, resource, **options)\u001b[0m\n\u001b[1;32m     55\u001b[0m dump(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest header\u001b[39m\u001b[38;5;124m\"\u001b[39m, header_str)\n\u001b[0;32m---> 57\u001b[0m status, resp \u001b[38;5;241m=\u001b[39m \u001b[43m_get_resp_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_REDIRECT_STATUSES:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/websocket/_handshake.py:150\u001b[0m, in \u001b[0;36m_get_resp_headers\u001b[0;34m(sock, success_statuses)\u001b[0m\n\u001b[1;32m    149\u001b[0m         response_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WebSocketBadStatusException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHandshake status \u001b[39m\u001b[38;5;132;01m{status}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{message}\u001b[39;00m\u001b[38;5;124m -+-+- \u001b[39m\u001b[38;5;132;01m{headers}\u001b[39;00m\u001b[38;5;124m -+-+- \u001b[39m\u001b[38;5;132;01m{body}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(status\u001b[38;5;241m=\u001b[39mstatus, message\u001b[38;5;241m=\u001b[39mstatus_message, headers\u001b[38;5;241m=\u001b[39mresp_headers, body\u001b[38;5;241m=\u001b[39mresponse_body), status, status_message, resp_headers, response_body)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, resp_headers\n",
      "\u001b[0;31mWebSocketBadStatusException\u001b[0m: Handshake status 500 Internal Server Error -+-+- {'content-length': '35', 'content-type': 'text/plain; charset=utf-8', 'date': 'Thu, 26 Sep 2024 17:30:16 GMT'} -+-+- b'container not found (\"model-store\")'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m wait_pod(core_v1, kserve_namespace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice.istio.io/canonical-name=model-store-pod\u001b[39m\u001b[38;5;124m\"\u001b[39m, mar_pod_name)\n\u001b[1;32m     94\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmkdir /pv/model-store/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 95\u001b[0m mk_ms_result \u001b[38;5;241m=\u001b[39m \u001b[43mexec_commands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore_v1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkserve_namespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmar_pod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmar_pod_container_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmkdir /pv/config/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m mk_conf_result \u001b[38;5;241m=\u001b[39m exec_commands(core_v1, kserve_namespace, mar_pod_name, mar_pod_container_name, command)\n",
      "Cell \u001b[0;32mIn[34], line 69\u001b[0m, in \u001b[0;36mexec_commands\u001b[0;34m(api_instance, namespace, pod_name, pod_container_name, command)\u001b[0m\n\u001b[1;32m     63\u001b[0m exec_command \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/bin/sh\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     66\u001b[0m     command]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# When calling a pod with multiple containers running the target container\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# has to be specified with a keyword argument container=<name>.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_get_namespaced_pod_exec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpod_container_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexec_command\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m resp)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/stream/stream.py:35\u001b[0m, in \u001b[0;36m_websocket_request\u001b[0;34m(websocket_request, force_kwargs, api_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     api_client\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(websocket_request, configuration)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     api_client\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m prev_request\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/client/api/core_v1_api.py:994\u001b[0m, in \u001b[0;36mCoreV1Api.connect_get_namespaced_pod_exec\u001b[0;34m(self, name, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"connect_get_namespaced_pod_exec  # noqa: E501\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03mconnect GET requests to exec of Pod  # noqa: E501\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m         returns the request thread.\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    993\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_get_namespaced_pod_exec_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/client/api/core_v1_api.py:1101\u001b[0m, in \u001b[0;36mCoreV1Api.connect_get_namespaced_pod_exec_with_http_info\u001b[0;34m(self, name, namespace, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# Authentication setting\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearerToken\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/v1/namespaces/\u001b[39;49m\u001b[38;5;132;43;01m{namespace}\u001b[39;49;00m\u001b[38;5;124;43m/pods/\u001b[39;49m\u001b[38;5;132;43;01m{name}\u001b[39;49;00m\u001b[38;5;124;43m/exec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/client/api_client.py:348\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    356\u001b[0m                                                method, path_params,\n\u001b[1;32m    357\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                                _request_timeout,\n\u001b[1;32m    366\u001b[0m                                                _host))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/client/api_client.py:180\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[1;32m    188\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/kubernetes/stream/ws_client.py:527\u001b[0m, in \u001b[0;36mwebsocket_call\u001b[0;34m(configuration, _method, url, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WSResponse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(client\u001b[38;5;241m.\u001b[39mread_all()))\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mSystemExit\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(status\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[0;31mApiException\u001b[0m: (0)\nReason: Handshake status 500 Internal Server Error -+-+- {'content-length': '35', 'content-type': 'text/plain; charset=utf-8', 'date': 'Thu, 26 Sep 2024 17:30:16 GMT'} -+-+- b'container not found (\"model-store\")'\n"
     ]
    }
   ],
   "source": [
    "from kubernetes import client, config, utils, watch\n",
    "import time\n",
    "from kubernetes.client import Configuration\n",
    "from kubernetes.client.api import core_v1_api\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.stream import stream\n",
    "import io\n",
    "import tarfile\n",
    "import pathlib\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "KSERVE_NAMESPACE = \"kubeflow-user-example-com\"\n",
    "MODEL_STORE_POD_NAME = \"model-store-pod\"\n",
    "MODEL_STORE_POD_CONTAINER_NAME = \"model-store\"\n",
    "MAR_POD_NAME = \"margen-pod\"\n",
    "MAR_POD_CONTAINER_NAME = \"margen-container\"\n",
    "\n",
    "config.load_kube_config(\"kubeconfig\")\n",
    "k8s_client = client.ApiClient()\n",
    "model_store_yaml_dir = '../model-archiver/model-store-manifests/'\n",
    "\n",
    "try:\n",
    "    pv_result = utils.create_from_directory(k8s_client, model_store_yaml_dir, verbose=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "def wait_pod(core_v1, namespace, label, pod_name):\n",
    "    w = watch.Watch()\n",
    "    for event in w.stream(func=core_v1.list_namespaced_pod,\n",
    "                              namespace=namespace,\n",
    "                              label_selector=label,\n",
    "                              timeout_seconds=60):\n",
    "        if event[\"object\"].status.phase == \"Running\":\n",
    "            w.stop()\n",
    "            end_time = time.time()\n",
    "            print(f\"{pod_name} running \")\n",
    "            return\n",
    "        # event.type: ADDED, MODIFIED, DELETED\n",
    "        if event[\"type\"] == \"DELETED\":\n",
    "            # Pod was deleted while we were waiting for it to start.\n",
    "            print(f\"{pod_name} deleted before it started\")\n",
    "            w.stop()\n",
    "            return\n",
    "    \n",
    "def exec_commands(api_instance, namespace, pod_name, pod_container_name, command):\n",
    "    name = mar_pod_name\n",
    "    resp = None\n",
    "    try:\n",
    "        resp = api_instance.read_namespaced_pod(name=name,\n",
    "                                                namespace=namespace)\n",
    "    except ApiException as e:\n",
    "        if e.status != 404:\n",
    "            print(f\"Unknown error: {e}\")\n",
    "            exit(1)\n",
    "\n",
    "    if not resp:\n",
    "        print(f\"Pod {name} does not exist.\")\n",
    "        \n",
    "    # Calling exec and waiting for response\n",
    "    exec_command = [\n",
    "        '/bin/sh',\n",
    "        '-c',\n",
    "        command]\n",
    "    # When calling a pod with multiple containers running the target container\n",
    "    # has to be specified with a keyword argument container=<name>.\n",
    "    resp = stream(api_instance.connect_get_namespaced_pod_exec,\n",
    "          name=pod_name,\n",
    "          container=pod_container_name,\n",
    "          namespace=namespace,\n",
    "          command=exec_command,\n",
    "          stderr=True, stdin=False,\n",
    "          stdout=True, tty=False)\n",
    "    print(\"Response: \" + resp)\n",
    "\n",
    "try:\n",
    "    c = Configuration().get_default_copy()\n",
    "except AttributeError:\n",
    "    c = Configuration()\n",
    "    c.assert_hostname = False\n",
    "Configuration.set_default(c)\n",
    "core_v1 = core_v1_api.CoreV1Api()\n",
    "\n",
    "# Create folders for model-store and config in PV\n",
    "kserve_namespace = KSERVE_NAMESPACE\n",
    "model_store_pod_name = MODEL_STORE_POD_NAME\n",
    "model_store_pod_container_name = MODEL_STORE_POD_CONTAINER_NAME\n",
    "model_store_container_label = \"service.istio.io/canonical-name=model-store-pod\"\n",
    "\n",
    "# Wait for pods to run before exec\n",
    "wait_pod(core_v1, kserve_namespace, model_store_container_label, model_store_pod_name)\n",
    "\n",
    "command = \"mkdir /pv/model-store/\"\n",
    "mk_ms_result = exec_commands(core_v1, kserve_namespace, model_store_pod_name, model_store_pod_container_name, command)\n",
    "command = \"mkdir /pv/config/\"\n",
    "mk_conf_result = exec_commands(core_v1, kserve_namespace, model_store_pod_name, model_store_pod_container_name, command)\n",
    "\n",
    "# Copy files to pv\n",
    "def copy_to_tar(source_path, dest_path, tar):\n",
    "    \"\"\"\n",
    "    Adds a file or directory to a tar archive.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_path: Path to the source file or directory on the local machine.\n",
    "    - dest_path: Destination directory inside the pod where files should be copied.\n",
    "    - tar: The tarfile object to which files and directories will be added.\n",
    "    \"\"\"\n",
    "    source_path = pathlib.Path(source_path)\n",
    "\n",
    "    if source_path.is_file():\n",
    "        # If it's a file, add to the tarfile with the destination path\n",
    "        tar.add(source_path, arcname=pathlib.Path(dest_path).joinpath(source_path.name))\n",
    "    elif source_path.is_dir():\n",
    "        # If it's a directory, recursively add all its content\n",
    "        for root, dirs, files in os.walk(source_path):\n",
    "            root_path = pathlib.Path(root)\n",
    "            # Compute the relative path within the tar and add to destination path\n",
    "            for file in files:\n",
    "                file_path = root_path / file\n",
    "                tar.add(file_path, arcname=pathlib.Path(dest_path).joinpath(file_path.relative_to(source_path)))\n",
    "\n",
    "def copy_file_or_dir(api_instance, namespace, pod_name, pod_container_name, source_path, dest_path):\n",
    "    \"\"\"\n",
    "    Copies a file or directory to a Kubernetes pod by tarring the content and extracting it within the pod.\n",
    "\n",
    "    Parameters:\n",
    "    - api_instance: Kubernetes API client instance.\n",
    "    - namespace: Namespace of the pod.\n",
    "    - pod_name: Name of the pod.\n",
    "    - pod_container_name: Name of the container within the pod.\n",
    "    - source_path: Path to the source file or directory on the local machine.\n",
    "    - dest_path: Destination directory inside the pod.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a buffer to store the tar archive\n",
    "        buf = io.BytesIO()\n",
    "        with tarfile.open(fileobj=buf, mode='w:tar') as tar:\n",
    "            copy_to_tar(source_path, dest_path, tar)\n",
    "\n",
    "        buf.seek(0)  # Reset buffer position after writing tar\n",
    "\n",
    "        # Command to extract the tar archive in the pod\n",
    "        exec_command = ['tar', 'xvf', '-', '-C', '/']\n",
    "\n",
    "        # Establish connection to the pod\n",
    "        resp = stream(api_instance.connect_get_namespaced_pod_exec,\n",
    "                      pod_name,\n",
    "                      namespace,\n",
    "                      container=pod_container_name,  # Specify container name if necessary\n",
    "                      command=exec_command,\n",
    "                      stderr=True, stdin=True, stdout=True, tty=False,\n",
    "                      _preload_content=False)\n",
    "\n",
    "        # Stream the tar file content to the pod\n",
    "        while resp.is_open():\n",
    "            resp.update(timeout=1)\n",
    "            if resp.peek_stdout():\n",
    "                print(f\"STDOUT: {resp.read_stdout()}\")\n",
    "            if resp.peek_stderr():\n",
    "                print(f\"STDERR: {resp.read_stderr()}\")\n",
    "            if buf.getvalue():\n",
    "                resp.write_stdin(buf.read())  # Write the tar file content to stdin\n",
    "            else:\n",
    "                resp.write_stdin('\\n')  # Send newline to signal end of input\n",
    "                break\n",
    "        resp.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying file or directory to pod: {e}\")\n",
    "\n",
    "model_store_source_path = \"../model-archiver/model-store/\"\n",
    "model_store_dest_path = \"/pv/model-store/\"\n",
    "\n",
    "cp_ms_result = copy_file_or_dir(core_v1, kserve_namespace, model_store_pod_name, model_store_pod_container_name, model_store_source_path, model_store_dest_path)\n",
    "\n",
    "config_source_path = \"../model-archiver/config/config.properties\"\n",
    "config_dest_path = \"/pv/config/\"\n",
    "\n",
    "cp_conf_result = copy_file_or_dir(core_v1, kserve_namespace, model_store_pod_name, model_store_pod_container_name, config_source_path, config_dest_path)\n",
    "\n",
    "# Delete model_store_pod\n",
    "try:\n",
    "    api_response = core_v1.delete_namespaced_pod(model_store_pod_name, kserve_namespace)\n",
    "    print(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling CoreV1Api->delete_namespaced_pod: %s\\n\" % e)\n",
    "    \n",
    "# Create model archiver pod\n",
    "mar_yaml_dir = '../model-archiver/manifests/'\n",
    "mar_pod_name = MAR_POD_NAME\n",
    "mar_pod_container_name = MAR_POD_CONTAINER_NAME\n",
    "\n",
    "try:\n",
    "    margen_result = utils.create_from_directory(k8s_client, mar_yaml_dir, verbose=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# Delete margen pod\n",
    "try:\n",
    "    api_response = core_v1.delete_namespaced_pod(mar_pod_name, kserve_namespace)\n",
    "    print(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling CoreV1Api->delete_namespaced_pod: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f2b06e-4971-46eb-bd54-4473990ebf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, ClassificationMetrics, Artifact\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from env file\n",
    "load_dotenv('env')\n",
    "\n",
    "# Github variables\n",
    "GITHUB_USERNAME = os.getenv(\"GITHUB_USERNAME\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "GITHUB_REPO_URL = \"https://github.com/danilonicioka/mlops-workflow.git\"\n",
    "GITHUB_CLONED_DIR = \"mlops-workflow\"\n",
    "GITHUB_DVC_BRANCH = \"dvc\"\n",
    "\n",
    "# Kubeflow variables\n",
    "KUBEFLOW_PIPELINE_NAME = \"mlops\"\n",
    "KUBEFLOW_HOST_URL = \"http://ml-pipeline.kubeflow:8888\"  # KFP host URL\n",
    "KUBEFLOW_PIPELINE_ID=\"7451916e-eee8-4c14-ad5f-8dee5aa61e3b\"\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    KUBEFLOW_TOKEN = f.read()\n",
    "\n",
    "# DVC variables\n",
    "DVC_REMOTE_DB = \"minio_remote\"\n",
    "DVC_REMOTE_DB_URL = \"s3://dvc-data\"\n",
    "DVC_FILE_DIR = 'data/external'\n",
    "DVC_FILE_NAME = 'dataset.csv'\n",
    "\n",
    "# MinIO variables\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "MINIO_MODEL_BUCKET_NAME = \"model-files\"\n",
    "MINIO_MODEL_OBJECT_NAME = \"model-store/youtubegoes5g/model.pt\"\n",
    "\n",
    "# Triggers variables\n",
    "TRIGGER_TYPE = '1'\n",
    "PERFORMANCE_FACTOR = 0.05\n",
    "# Temp dir and files to save accuracy for trigger 3\n",
    "TEMP_DIR = \"tmp\"\n",
    "TEMP_FILE_ACC_IN_LAST_RUN = \"accuracy_in_last_run.txt\"\n",
    "LAST_ACC_OBJECT_NAME = \"accuracy-score/last_acc.txt\"\n",
    "\n",
    "# Model variables\n",
    "MODEL_LR = 0.0001\n",
    "MODEL_EPOCHS = 3500\n",
    "MODEL_PRINT_FREQUENCY_PER_N_EPOCHS = 500\n",
    "MODEL_NAME = \"youtubegoes5g\"\n",
    "\n",
    "# Kserve variables\n",
    "#MODEL_FRAMEWORK = \"pytorch\"\n",
    "KSERVE_NAMESPACE = \"kubeflow-user-example-com\"\n",
    "KSERVE_SVC_ACC = \"sa-minio-kserve\"\n",
    "#MODEL_URI = \"pvc://model-store-claim\"\n",
    "#MODEL_URI = \"minio-service.kubeflow:9000/model-files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf08b0-8d7b-4b91-919b-6b433388bf45",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91437e7-ae9e-4c96-b691-83e49f048def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from subprocess import run, CalledProcessError\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "github_repo_url = GITHUB_REPO_URL\n",
    "github_cloned_dir = GITHUB_CLONED_DIR\n",
    "github_dvc_branch = GITHUB_DVC_BRANCH\n",
    "github_username = GITHUB_USERNAME\n",
    "github_token = GITHUB_TOKEN\n",
    "dvc_remote_name = DVC_REMOTE_DB\n",
    "dvc_remote_db_url = DVC_REMOTE_DB_URL\n",
    "minio_url = MINIO_URL\n",
    "minio_access_key = MINIO_ACCESS_KEY\n",
    "minio_secret_key = MINIO_SECRET_KEY\n",
    "dvc_file_dir = DVC_FILE_DIR\n",
    "dvc_file_name = DVC_FILE_NAME\n",
    "\n",
    "def clone_repository_with_token(github_repo_url, github_cloned_dir, github_dvc_branch, github_username, github_token):\n",
    "    \"\"\"Clone a Git repository using a GitHub token in the URL and specifying the branch.\"\"\"\n",
    "    try:\n",
    "        # Construct the URL with the GitHub username and token\n",
    "        url_with_token = f\"https://{github_username}:{github_token}@{github_repo_url.split('//')[1]}\"\n",
    "\n",
    "        # Clone the repository from the specified branch\n",
    "        repo = Repo.clone_from(url_with_token, github_cloned_dir, branch=github_dvc_branch)\n",
    "        return \"Repository cloned successfully\"\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred during repository cloning: {e}\"\n",
    "\n",
    "def configure_dvc_remote(github_cloned_dir, dvc_remote_name, dvc_remote_db_url, minio_url, minio_access_key, minio_secret_key):\n",
    "    http_minio = f'http://{minio_url}'\n",
    "    \"\"\"Configure the Minio bucket as the DVC remote repository using the `dvc remote` commands.\"\"\"\n",
    "    try:\n",
    "        # Add the remote\n",
    "        run(\n",
    "            ['dvc', 'remote', 'add', '-d', dvc_remote_name, dvc_remote_db_url],\n",
    "            cwd=github_cloned_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        # Configure the endpoint URL\n",
    "        run(\n",
    "            ['dvc', 'remote', 'modify', dvc_remote_name, 'endpointurl', http_minio],\n",
    "            cwd=github_cloned_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        # Configure access key ID\n",
    "        run(\n",
    "            ['dvc', 'remote', 'modify', dvc_remote_name, 'access_key_id', minio_access_key],\n",
    "            cwd=github_cloned_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        # Configure secret access key\n",
    "        run(\n",
    "            ['dvc', 'remote', 'modify', dvc_remote_name, 'secret_access_key', minio_secret_key],\n",
    "            cwd=github_cloned_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        return f'Successfully configured Minio bucket as DVC remote repository: {dvc_remote_name}'\n",
    "    except CalledProcessError as e:\n",
    "        # Log and raise any errors\n",
    "        return f'Failed to configure DVC remote: {e.stderr}'\n",
    "\n",
    "def perform_dvc_pull(github_cloned_dir, dvc_remote_name):\n",
    "    \"\"\"Perform a DVC pull to synchronize local data with the remote repository.\"\"\"\n",
    "    try:\n",
    "        # Run the `dvc pull` command\n",
    "        result = run(['dvc', 'pull', '-r', dvc_remote_name], cwd=github_cloned_dir, capture_output=True, text=True)\n",
    "\n",
    "        # Check if the command executed successfully\n",
    "        if result.returncode != 0:\n",
    "            # Log and raise an error if the command failed\n",
    "            error_message = f\"dvc pull failed with error: {result.stderr}\"\n",
    "            raise Exception(error_message)\n",
    "\n",
    "        # Log successful operation\n",
    "        return \"Successfully pulled data from remote DVC repository\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log and handle the error\n",
    "        return f\"Error occurred during dvc pull: {e}\"\n",
    "\n",
    "# Call the functions\n",
    "clone_result = clone_repository_with_token(github_repo_url, github_cloned_dir, github_dvc_branch, github_username, github_token)\n",
    "configure_result = configure_dvc_remote(github_cloned_dir, dvc_remote_name, dvc_remote_db_url, minio_url, minio_access_key, minio_secret_key)\n",
    "dvc_pull_result = perform_dvc_pull(github_cloned_dir, dvc_remote_name)\n",
    "\n",
    "# Save dataset with pandas in Dataset artifact\n",
    "pulled_dataset_path = os.path.join(github_cloned_dir, dvc_file_dir, dvc_file_name)\n",
    "tmp_dataset_path = \"/tmp/\" + dvc_file_name\n",
    "dataset = pd.read_csv(pulled_dataset_path)\n",
    "dataset.to_pickle(tmp_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46215ee-6416-4bfd-b1b5-82359506d9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size 2693\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size\", dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a6c0f4-4da4-432f-8e4f-aaf49a90e939",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5ebab9-6074-4cdc-b2df-5ffd12ae0e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load dataset from Dataset artifact\n",
    "df = pd.read_pickle(tmp_dataset_path)\n",
    "\n",
    "# Handle null values and replace specific characters\n",
    "#df = df.replace([' ', '-',np.nan], 0) # There are null values\n",
    "df = df.replace([' ', '-', np.nan], np.nan)\n",
    "\n",
    "# Selective columns for mean calculation\n",
    "columns_to_convert = [\n",
    "    'CQI1', 'CQI2', 'CQI3', 'cSTD CQI', 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n",
    "    'RSRP1', 'RSRP2', 'RSRP3', 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n",
    "    'RSRQ1', 'RSRQ2', 'RSRQ3', 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n",
    "    'SNR1', 'SNR2', 'SNR3', 'sMajority', 's25 P', 's50 P', 's75 P'\n",
    "]\n",
    "df[columns_to_convert] = df[columns_to_convert].astype(float)\n",
    "\n",
    "# Replace np.nan with mean values for selective columns\n",
    "df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n",
    "\n",
    "# Convert 'Stall' column to numerical values\n",
    "df['Stall'].replace({'Yes': 1, 'No': 0}, inplace=True)\n",
    "\n",
    "X = df[columns_to_convert].values\n",
    "y = df['Stall'].values\n",
    "\n",
    "# Apply SMOTE for balancing the dataset\n",
    "# oversample = SMOTE(random_state=random_state)\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee319604",
   "metadata": {},
   "source": [
    "# Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbcf5608-25a5-4fd6-b663-4e2552c17e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.69019, Accuracy: 51.72% | Test Loss: 0.68449, Test Accuracy: 55.18%\n",
      "Epoch: 500 | Loss: 0.47132, Accuracy: 78.29% | Test Loss: 0.48062, Test Accuracy: 77.87%\n",
      "Epoch: 1000 | Loss: 0.36650, Accuracy: 84.04% | Test Loss: 0.42489, Test Accuracy: 81.99%\n",
      "Epoch: 1500 | Loss: 0.26057, Accuracy: 89.68% | Test Loss: 0.39684, Test Accuracy: 82.41%\n",
      "Epoch: 2000 | Loss: 0.17468, Accuracy: 94.36% | Test Loss: 0.40738, Test Accuracy: 84.96%\n",
      "Epoch: 2500 | Loss: 0.11994, Accuracy: 96.70% | Test Loss: 0.43905, Test Accuracy: 86.24%\n",
      "Epoch: 3000 | Loss: 0.08511, Accuracy: 97.91% | Test Loss: 0.48346, Test Accuracy: 86.38%\n",
      "<class 'numpy.float64'>\n",
      "Precision: 0.859740\n",
      "Recall: 0.856738\n",
      "Micro F1 score: 0.856738\n",
      "Macro F1 score: 0.856727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No-Stall       0.89      0.82      0.86       370\n",
      "       Stall       0.82      0.89      0.86       335\n",
      "\n",
      "    accuracy                           0.86       705\n",
      "   macro avg       0.86      0.86      0.86       705\n",
      "weighted avg       0.86      0.86      0.86       705\n",
      "\n",
      "Error occurred: S3 operation failed; code: NoSuchKey, message: The specified key does not exist., resource: /model-files/accuracy-score/last_acc.txt, request_id: 17F7F5C0EB978A81, host_id: 04364467-0bee-474e-8dba-2559e675bfee, bucket_name: model-files, object_name: accuracy-score/last_acc.txt\n",
      "0.8567375886524823\n",
      "0.0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from minio import Minio\n",
    "\n",
    "model_lr = MODEL_LR\n",
    "model_epochs = MODEL_EPOCHS\n",
    "model_print_frequency_per_n_epochs = MODEL_PRINT_FREQUENCY_PER_N_EPOCHS\n",
    "bucket_name = MINIO_MODEL_BUCKET_NAME\n",
    "minio_model_object_name = MINIO_MODEL_OBJECT_NAME\n",
    "trigger_type = TRIGGER_TYPE\n",
    "performance_factor = PERFORMANCE_FACTOR\n",
    "last_accuracy_object_name = LAST_ACC_OBJECT_NAME\n",
    "tmp_dir = TEMP_DIR\n",
    "tmp_file_last_acc = TEMP_FILE_ACC_IN_LAST_RUN\n",
    "\n",
    "# Build model with non-linear activation function\n",
    "class InterruptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=29, out_features=200)\n",
    "        self.layer_2 = nn.Linear(in_features=200, out_features=100)\n",
    "        self.layer_3 = nn.Linear(in_features=100, out_features=1)\n",
    "        self.relu = nn.ReLU() # <- add in ReLU activation function\n",
    "        # Can also put sigmoid in the model\n",
    "        # This would mean you don't need to use it on the predictions\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Intersperse the ReLU activation function between layers\n",
    "        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "# Helper functions\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "def minio_setup(minio_url, minio_access_key, minio_secret_key):\n",
    "    # Initialize Minio client with just the base URL (without path)\n",
    "    client = Minio(\n",
    "        minio_url,  # Ensure minio_url does not include a path, only the base URL (e.g., http://localhost:9000)\n",
    "        access_key=minio_access_key,\n",
    "        secret_key=minio_secret_key,\n",
    "        secure=False  # Minio is using HTTP on localhost:9000\n",
    "    )\n",
    "    return client\n",
    "\n",
    "def upload_file(client, bucket_name, object_name, filepath):\n",
    "    # Create the bucket if it does not exist\n",
    "    if not client.bucket_exists(bucket_name):\n",
    "        client.make_bucket(bucket_name)\n",
    "        create_bucket_result = f\"Successfully created bucket: {bucket_name}\"\n",
    "    else:\n",
    "        create_bucket_result = f\"Bucket {bucket_name} already exists\"\n",
    "\n",
    "    try:\n",
    "        # Upload the file to the specified path in the bucket\n",
    "        client.fput_object(bucket_name, object_name, filepath)\n",
    "        return (f'Successfully uploaded {filepath} to {bucket_name}/{object_name}')\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log and raise any upload errors\n",
    "        raise Exception(f'Failed to upload model to Minio: {e}')\n",
    "\n",
    "def read_from_minio(client, bucket_name, object_name):\n",
    "    \"\"\"\n",
    "    Function to read a file from a MinIO bucket and convert its single content to a float.\n",
    "    If the file is not found or is empty, it returns 0.0.\n",
    "\n",
    "    Args:\n",
    "        client: minio client\n",
    "        bucket_name (str): The name of the bucket in MinIO.\n",
    "        object_name (str): The name of the object (file) in the bucket.\n",
    "\n",
    "    Returns:\n",
    "        float: The float value converted from the file content, or 0.0 if the file is not found or empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the file from the MinIO bucket\n",
    "        response = client.get_object(bucket_name, object_name)\n",
    "\n",
    "        # Read the file content into a buffer\n",
    "        file_data = response.read()\n",
    "\n",
    "        # Decode file content and strip whitespace\n",
    "        content = file_data.decode('utf-8').strip()\n",
    "\n",
    "        # If the content is empty, return 0.0\n",
    "        if not content:\n",
    "            print(f\"File {object_name} is empty.\")\n",
    "            return 0.0\n",
    "\n",
    "        # Convert the content to a float\n",
    "        float_value = float(content)\n",
    "        return float_value\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle file not found or any other errors\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def save_float_to_tempfile(float_value, dir_name, file_name):\n",
    "    \"\"\"\n",
    "    Saves a float value to a specified directory and file name.\n",
    "\n",
    "    Args:\n",
    "        float_value (float): The float value to save.\n",
    "        dir_name (str): The name of the directory to save the file in.\n",
    "        file_name (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the file.\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    temp_file_path = os.path.join(dir_name, file_name)\n",
    "\n",
    "    with open(temp_file_path, 'w') as temp_file:\n",
    "        # Convert the float to a string, then write to file\n",
    "        temp_file.write(str(float_value))\n",
    "\n",
    "    return temp_file_path\n",
    "\n",
    "def get_accuracy_in_last_run(client, bucket_name, object_name):\n",
    "    accuracy_in_last_run = read_from_minio(client, bucket_name, object_name)\n",
    "    return accuracy_in_last_run\n",
    "\n",
    "def update_accuracy_in_last_run(client, bucket_name, object_name, new_value, tmp_dir, tmp_file):\n",
    "    filepath = save_float_to_tempfile(new_value, tmp_dir, tmp_file)\n",
    "    upload_file(client, bucket_name, object_name, filepath)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = InterruptionModel().to(device)\n",
    "\n",
    "# Setup loss and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_lr)\n",
    "\n",
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Put all data on target device\n",
    "# X_train = torch.load(X_train_artifact.path)\n",
    "# X_test = torch.load(X_test_artifact.path)\n",
    "# y_train = torch.load(y_train_artifact.path)\n",
    "# y_test = torch.load(y_test_artifact.path)\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(model_epochs):\n",
    "    # 1. Forward pass\n",
    "    y_logits = model(X_train).squeeze()\n",
    "\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
    "    acc = accuracy_fn(y_true=y_train,\n",
    "                    y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    # 1. Forward pass\n",
    "        test_logits = model(X_test).squeeze()\n",
    "        #print(test_logits.shape)\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "        # 2. Calcuate loss and accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                            y_pred=test_pred)\n",
    "\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % model_print_frequency_per_n_epochs == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_preds = torch.round(torch.sigmoid(model(X_test))).squeeze()\n",
    "\n",
    "if device == \"cuda\":\n",
    "    predictions = y_preds.cpu().numpy() #if it is cuda, then this, otherwise y_pred.numpy()\n",
    "    true_labels = y_test.cpu().numpy()\n",
    "else:\n",
    "    predictions = y_preds.numpy()\n",
    "    true_labels = y_test.numpy()\n",
    "\n",
    "# Confusion Matrix\n",
    "cmatrix = confusion_matrix(true_labels, predictions)\n",
    "#print(\"Confusion Matrix:\", cmatrix)\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "# metrics.log_metric(\"Accuracy\", accuracy)\n",
    "#print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# test accuracy\n",
    "print(type(accuracy))\n",
    "\n",
    "precision = precision_score(true_labels,  predictions, average='weighted')\n",
    "# metrics.log_metric(\"Precision\", precision)\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "# metrics.log_metric(\"Recall\", recall)\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "microf1 = f1_score(true_labels, predictions, average='micro')\n",
    "# metrics.log_metric(\"Micro F1 score\", microf1)\n",
    "print('Micro F1 score: %f' % microf1)\n",
    "\n",
    "macrof1 = f1_score(true_labels, predictions, average='macro')\n",
    "# metrics.log_metric(\"Macro F1 score\", macrof1)\n",
    "print('Macro F1 score: %f' % macrof1)\n",
    "\n",
    "target_names = ['No-Stall', 'Stall']\n",
    "# Print precision-recall report\n",
    "print(classification_report(true_labels, predictions, target_names=target_names))\n",
    "\n",
    "# Classification Metrics artifact\n",
    "cmatrix = cmatrix.tolist()\n",
    "target_names = ['No-Stall', 'Stall']\n",
    "# classification_metrics.log_confusion_matrix(target_names, cmatrix)\n",
    "\n",
    "# Save model\n",
    "# model_path = \"/tmp/model.pt\"\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "# os.rename(model_path, model_trained_artifact.path)\n",
    "\n",
    "# Setup minio client to upload and read files\n",
    "client = minio_setup(minio_url, minio_access_key, minio_secret_key)\n",
    "\n",
    "previous_accuracy = get_accuracy_in_last_run(client, bucket_name, last_accuracy_object_name)\n",
    "\n",
    "#metrics.log_metric(\"current-previous accuracy\", accuracy-previous_accuracy)\n",
    "# metrics.log_metric(\"current accuracy\", accuracy)\n",
    "# metrics.log_metric(\"previous accuracy\", previous_accuracy)\n",
    "print(accuracy)\n",
    "print(previous_accuracy)\n",
    "\n",
    "if trigger_type == '1' or trigger_type == '2':\n",
    "    up_model = True\n",
    "elif trigger_type == '3':\n",
    "    if accuracy - previous_accuracy > performance_factor:\n",
    "        up_model = True\n",
    "        # update_accuracy_in_last_run(client, bucket_name, last_accuracy_object_name, accuracy, tmp_dir, tmp_file_last_acc)\n",
    "else:\n",
    "    up_model = False\n",
    "    print('0')\n",
    "    # metrics.log_metric(\"up model\", '0')\n",
    "    # with open(up_model_cond.path, 'w') as f:\n",
    "        # f.write('0')\n",
    "\n",
    "if up_model:\n",
    "    print('1')\n",
    "    # metrics.log_metric(\"up model\", '1')\n",
    "    # with open(up_model_cond.path, 'w') as f:\n",
    "        # f.write('1')\n",
    "    # upload_model_result = upload_file(client, bucket_name, minio_model_object_name, model_trained_artifact.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc32d0-2995-476b-bfae-c84d1474c205",
   "metadata": {},
   "source": [
    "# Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3f4eb-6022-4040-96f0-6006aa0c61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @component(base_image=\"python:3.11.9\", packages_to_install=['kserve==0.13.0','kubernetes==30.1.0'])\n",
    "# def model_serving(\n",
    "#     up_model_cond: Input[Artifact],\n",
    "#     cond_info: Output[Metrics],\n",
    "#     bucket_name: str,\n",
    "#     model_name: str,\n",
    "#     kserve_namespae: str,\n",
    "#     kserve_svc_acc: str\n",
    "# ):\n",
    "#     # Create kserve instance\n",
    "#     from kubernetes import client \n",
    "#     from kserve import KServeClient, constants, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1TorchServeSpec\n",
    "#     from datetime import datetime\n",
    "#     import time\n",
    "    \n",
    "#     # exec if a new model was uploaded\n",
    "#     with open(up_model_cond.path) as f:\n",
    "#         up_model = f.read()\n",
    "        \n",
    "#     if up_model == '1':\n",
    "#         cond_info.log_metric(\"Up model\", up_model)\n",
    "        \n",
    "#         #Inference server config\n",
    "#         now = datetime.now()\n",
    "#         kserve_version='v1beta1'\n",
    "#         api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "#         # with open(model_uri.path) as f:\n",
    "#         #     uri = f.read()\n",
    "#         uri = f's3://{bucket_name}'\n",
    "\n",
    "#         isvc = V1beta1InferenceService(api_version=api_version,\n",
    "#                                        kind=constants.KSERVE_KIND,\n",
    "#                                        metadata=client.V1ObjectMeta(\n",
    "#                                            name=model_name, namespace=kserve_namespae, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "#                                        spec=V1beta1InferenceServiceSpec(\n",
    "#                                        predictor=V1beta1PredictorSpec(\n",
    "#                                            service_account_name=kserve_svc_acc,\n",
    "#                                            pytorch=(V1beta1TorchServeSpec(\n",
    "#                                                storage_uri=uri))))\n",
    "#         )\n",
    "\n",
    "#         KServe = KServeClient()\n",
    "\n",
    "#         #replace old inference service with a new one\n",
    "#         try:\n",
    "#             KServe.delete(name=model_name, namespace=kserve_namespae)\n",
    "#             print(\"Old model deleted\")\n",
    "#         except:\n",
    "#             print(\"Couldn't delete old model\")\n",
    "#         time.sleep(10)\n",
    "\n",
    "#         KServe.create(isvc)\n",
    "#     else:\n",
    "#         cond_info.log_metric(\"Up model\", '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea7145-99c8-4234-b80d-cfa1c20961f8",
   "metadata": {},
   "source": [
    "# Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba3585-0b4f-4e8d-9b1c-6d8872d59b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pipeline\n",
    "# def my_pipeline(\n",
    "#     github_repo_url: str,\n",
    "#     github_cloned_dir: str,\n",
    "#     github_dvc_branch: str,\n",
    "#     github_username: str,\n",
    "#     github_token: str,\n",
    "#     dvc_remote_name: str,\n",
    "#     dvc_remote_db_url: str,\n",
    "#     minio_url: str,\n",
    "#     minio_access_key: str,\n",
    "#     minio_secret_key: str,\n",
    "#     dvc_file_dir: str,\n",
    "#     dvc_file_name: str,\n",
    "#     model_name: str,\n",
    "#     kserve_namespae: str,\n",
    "#     model_lr: float,\n",
    "#     model_epochs: int,\n",
    "#     model_print_frequency_per_n_epochs: int,\n",
    "#     bucket_name: str,\n",
    "#     minio_model_object_name: str,\n",
    "#     kserve_svc_acc: str,\n",
    "#     trigger_type: str,\n",
    "#     performance_factor: float,\n",
    "#     last_accuracy_object_name: str,\n",
    "#     tmp_dir: str,\n",
    "#     tmp_file_last_acc: str\n",
    "# ):\n",
    "#     data_ingestion_task = data_ingestion(\n",
    "#         github_repo_url=github_repo_url,\n",
    "#         github_cloned_dir=github_cloned_dir,\n",
    "#         github_dvc_branch=github_dvc_branch,\n",
    "#         github_username=github_username,\n",
    "#         github_token=github_token,\n",
    "#         dvc_remote_name=dvc_remote_name,\n",
    "#         dvc_remote_db_url=dvc_remote_db_url,\n",
    "#         minio_url=minio_url,\n",
    "#         minio_access_key=minio_access_key,\n",
    "#         minio_secret_key=minio_secret_key,\n",
    "#         dvc_file_dir=dvc_file_dir,\n",
    "#         dvc_file_name=dvc_file_name)\n",
    "#     dataset_artifact = data_ingestion_task.outputs[\"dataset_artifact\"]\n",
    "#     data_preparation_task = data_preparation(dataset_artifact=dataset_artifact)\n",
    "#     X_train_artifact = data_preparation_task.outputs[\"X_train_artifact\"]\n",
    "#     X_test_artifact = data_preparation_task.outputs[\"X_test_artifact\"]\n",
    "#     y_train_artifact = data_preparation_task.outputs[\"y_train_artifact\"]\n",
    "#     y_test_artifact = data_preparation_task.outputs[\"y_test_artifact\"]\n",
    "#     model_training_task = model_training(X_train_artifact=X_train_artifact, \n",
    "#                                          X_test_artifact=X_test_artifact, \n",
    "#                                          y_train_artifact=y_train_artifact, \n",
    "#                                          y_test_artifact=y_test_artifact,\n",
    "#                                          model_lr=model_lr,\n",
    "#                                          model_epochs=model_epochs,\n",
    "#                                          model_print_frequency_per_n_epochs=model_print_frequency_per_n_epochs,\n",
    "#                                          minio_url=minio_url,\n",
    "#                                          minio_access_key=minio_access_key,\n",
    "#                                          minio_secret_key=minio_secret_key,\n",
    "#                                          bucket_name=bucket_name,\n",
    "#                                          minio_model_object_name=minio_model_object_name,\n",
    "#                                          trigger_type=trigger_type,\n",
    "#                                          performance_factor=performance_factor,\n",
    "#                                          last_accuracy_object_name=last_accuracy_object_name,\n",
    "#                                          tmp_dir=tmp_dir,\n",
    "#                                          tmp_file_last_acc=tmp_file_last_acc)\n",
    "#     up_model_cond = model_training_task.outputs[\"up_model_cond\"]\n",
    "#     model_serving_task = model_serving(up_model_cond=up_model_cond,\n",
    "#                                        bucket_name=bucket_name,\n",
    "#                                        model_name=model_name, \n",
    "#                                        kserve_namespae=kserve_namespae,\n",
    "#                                        kserve_svc_acc=kserve_svc_acc)\n",
    "\n",
    "# # Compile the pipeline\n",
    "# pipeline_filename = f\"{KUBEFLOW_PIPELINE_NAME}.yaml\"\n",
    "# kfp.compiler.Compiler().compile(\n",
    "#     pipeline_func=my_pipeline,\n",
    "#     package_path=pipeline_filename)\n",
    "\n",
    "# # Submit the pipeline to the KFP cluster\n",
    "# client = kfp.Client(\n",
    "#     host=KUBEFLOW_HOST_URL,\n",
    "#     existing_token=KUBEFLOW_TOKEN)  \n",
    "\n",
    "# client.create_run_from_pipeline_func(\n",
    "#     my_pipeline,\n",
    "#     enable_caching=False,\n",
    "#     arguments={\n",
    "#         'github_repo_url': GITHUB_REPO_URL,\n",
    "#         'github_cloned_dir': GITHUB_CLONED_DIR,\n",
    "#         'github_dvc_branch': GITHUB_DVC_BRANCH,\n",
    "#         'github_username': GITHUB_USERNAME,\n",
    "#         'github_token': GITHUB_TOKEN,\n",
    "#         'dvc_remote_name': DVC_REMOTE_DB,\n",
    "#         'dvc_remote_db_url': DVC_REMOTE_DB_URL,\n",
    "#         'minio_url': MINIO_URL,\n",
    "#         'minio_access_key': MINIO_ACCESS_KEY,\n",
    "#         'minio_secret_key': MINIO_SECRET_KEY,\n",
    "#         'dvc_file_dir': DVC_FILE_DIR,\n",
    "#         'dvc_file_name': DVC_FILE_NAME,\n",
    "#         'model_name': MODEL_NAME,\n",
    "#         'kserve_namespae': KSERVE_NAMESPACE,\n",
    "#         'model_lr': MODEL_LR,\n",
    "#         'model_epochs': MODEL_EPOCHS,\n",
    "#         'model_print_frequency_per_n_epochs': MODEL_PRINT_FREQUENCY_PER_N_EPOCHS,\n",
    "#         'bucket_name': MINIO_MODEL_BUCKET_NAME,\n",
    "#         'minio_model_object_name': MINIO_MODEL_OBJECT_NAME,\n",
    "#         'kserve_svc_acc': KSERVE_SVC_ACC,\n",
    "#         'trigger_type': TRIGGER_TYPE,\n",
    "#         'performance_factor': PERFORMANCE_FACTOR,\n",
    "#         'last_accuracy_object_name': LAST_ACC_OBJECT_NAME,\n",
    "#         'tmp_dir': TEMP_DIR,\n",
    "#         'tmp_file_last_acc': TEMP_FILE_ACC_IN_LAST_RUN\n",
    "#     })\n",
    "\n",
    "# #upload to Kubeflow \n",
    "# client.upload_pipeline(pipeline_package_path=pipeline_filename,\n",
    "#                        pipeline_name=KUBEFLOW_PIPELINE_NAME,\n",
    "#                        namespace = KSERVE_NAMESPACE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
