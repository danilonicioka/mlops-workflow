{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e70959e6-4ef3-4eeb-a56c-ce8ed6e4b526",
   "metadata": {},
   "source": [
    "# Using K8s and Kserve python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95208a6e-fab2-402b-af2d-db1b8c3d31f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/experiments/details/23d52751-4aeb-4e71-a47e-01c1ced25793\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/runs/details/a6b49222-05dd-427c-9826-1d42118355f2\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=a6b49222-05dd-427c-9826-1d42118355f2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, ClassificationMetrics, OutputPath\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GitHub username and token from environment variables\n",
    "GITHUB_USERNAME = os.getenv(\"GITHUB_USERNAME\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Define configuration variables\n",
    "REPO_URL = \"https://github.com/danilonicioka/mlops-workflow.git\"\n",
    "CLONED_DIR = \"mlops-workflow\"\n",
    "BRANCH_NAME = \"tests\"\n",
    "PIPELINE_ID = \"my-pipeline-id\"\n",
    "PIPELINE_NAME = \"mlops\"\n",
    "KFP_HOST = \"http://ml-pipeline.kubeflow:8887\"  # KFP host URL\n",
    "\n",
    "# Define DVC remote configuration variables\n",
    "REMOTE_NAME = \"minio_remote\"\n",
    "REMOTE_URL = \"s3://dvc-data\"\n",
    "MINIO_URL = \"http://minio-service.kubeflow:9000\"\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "DVC_FILE_DIR = 'data/external'\n",
    "DVC_FILE_NAME = 'dataset.csv'\n",
    "\n",
    "# Model serve config vars\n",
    "MODEL_NAME = \"youtubegoes5g\"\n",
    "FRAMEWORK = \"pytorch\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "SVC_ACCOUNT = \"default-editor\"\n",
    "MODEL_URI = \"pvc://model-store-claim\"\n",
    "\n",
    "@component(base_image=\"python:3.11.9\", packages_to_install=['kserve==0.13.0','kubernetes==30.1.0'])\n",
    "def model_serving(\n",
    "    model_name: str,\n",
    "    namespace: str,\n",
    "    model_uri: str\n",
    "):\n",
    "    # Create kserve instance\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient, constants, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1TorchServeSpec\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from subprocess import run\n",
    "    \n",
    "    #TFServing wants this type of structure ./models/1/model\n",
    "    # the number represent the model version\n",
    "    # in this example we use only 1 version\n",
    "    \n",
    "    #Inference server config\n",
    "    now = datetime.now()\n",
    "    kserve_version='v1beta1'\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=model_name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=\"default-editor\",\n",
    "                                       pytorch=(V1beta1TorchServeSpec(\n",
    "                                           storage_uri=model_uri))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    \n",
    "    #replace old inference service with a new one\n",
    "    try:\n",
    "        KServe.delete(name=model_name, namespace=namespace)\n",
    "        print(\"Old model deleted\")\n",
    "    except:\n",
    "        print(\"Couldn't delete old model\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "    KServe.create(isvc)\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline(\n",
    "    model_name: str,\n",
    "    namespace: str,\n",
    "    model_uri: str\n",
    "):\n",
    "    model_serving_task = model_serving(namespace=namespace, \n",
    "                                       model_name=model_name,\n",
    "                                       model_uri=model_uri)\n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_filename = f\"{PIPELINE_NAME}.yaml\"\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path=pipeline_filename)\n",
    "\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    TOKEN = f.read()\n",
    "\n",
    "# Submit the pipeline to the KFP cluster\n",
    "client = kfp.Client(host=KFP_HOST, # Use the configured KFP host\n",
    "                    existing_token=TOKEN)  \n",
    "\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    enable_caching=False,\n",
    "    arguments={\n",
    "        'model_name': MODEL_NAME,\n",
    "        'namespace': NAMESPACE,\n",
    "        'model_uri': MODEL_URI\n",
    "        \n",
    "    })\n",
    "\n",
    "#upload to Kubeflow \n",
    "# client.upload_pipeline(pipeline_package_path=pipeline_filename,\n",
    "#                        pipeline_name=\"mlops\",\n",
    "#                        namespace = \"kubeflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256109d-2fcd-40df-a18b-f70318625721",
   "metadata": {},
   "source": [
    "# Using Kserve component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f536db67-005b-47c5-9897-eff692b93932",
   "metadata": {},
   "source": [
    "## Manifest in var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7849e-ccda-4c5d-8a3b-25acc3283b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, ClassificationMetrics, OutputPath\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get GitHub username and token from environment variables\n",
    "GITHUB_USERNAME = os.getenv(\"GITHUB_USERNAME\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Define configuration variables\n",
    "REPO_URL = \"https://github.com/danilonicioka/mlops-workflow.git\"\n",
    "CLONED_DIR = \"mlops-workflow\"\n",
    "BRANCH_NAME = \"tests\"\n",
    "PIPELINE_ID = \"my-pipeline-id\"\n",
    "PIPELINE_NAME = \"mlops\"\n",
    "KFP_HOST = \"http://ml-pipeline.kubeflow.svc.cluster.local:8888\"  # KFP host URL\n",
    "\n",
    "# Define DVC remote configuration variables\n",
    "REMOTE_NAME = \"minio_remote\"\n",
    "REMOTE_URL = \"s3://dvc-data\"\n",
    "MINIO_URL = \"http://minio-svc.minio:9000\"\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "DVC_FILE_DIR = 'data/external'\n",
    "DVC_FILE_NAME = 'dataset.csv'\n",
    "\n",
    "# Model serve config vars\n",
    "MODEL_NAME = \"youtubegoes5g\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "SVC_ACCOUNT = \"pipeline-runner\"\n",
    "MODEL_URI = \"pvc://model-store-claim\"\n",
    "\n",
    "def create_serving_task(\n",
    "    model_name, \n",
    "    namespace,\n",
    "    model_uri\n",
    "):\n",
    "    api_version = \"serving.kserve.io/v1beta1\"\n",
    "    serving_component_url = 'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml'\n",
    "\n",
    "    inference_service = '''\n",
    "apiVersion: {}\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: {}\n",
    "  namespace: {}\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: pytorch\n",
    "      storageUri: {}\n",
    "'''.format(api_version, model_name, namespace, model_uri)\n",
    "\n",
    "    serving_launcher_op = components.load_component_from_url(serving_component_url)\n",
    "    serving_launcher_op(action=\"apply\", inferenceservice_yaml=inference_service)\n",
    "    \n",
    "@pipeline\n",
    "def my_pipeline(\n",
    "    model_name: str,\n",
    "    namespace: str,\n",
    "    model_uri: str\n",
    "):\n",
    "    create_serving_task(model_name, namespace, model_uri)\n",
    "    \n",
    "# Compile the pipeline\n",
    "pipeline_filename = f\"{PIPELINE_NAME}.yaml\"\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path=pipeline_filename)\n",
    "\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    TOKEN = f.read()\n",
    "\n",
    "# Submit the pipeline to the KFP cluster\n",
    "client = kfp.Client(host=KFP_HOST, # Use the configured KFP host\n",
    "                    existing_token=TOKEN)  \n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    enable_caching=False,\n",
    "    arguments={\n",
    "        'model_name': MODEL_NAME,\n",
    "        'namespace': NAMESPACE,\n",
    "        'model_uri': MODEL_URI\n",
    "    })\n",
    "\n",
    "#upload to Kubeflow \n",
    "# client.upload_pipeline(pipeline_package_path=pipeline_filename,\n",
    "#                        pipeline_name=\"mlops\",\n",
    "#                        namespace = \"kubeflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c8bc2f-6bd1-4a72-a991-a0ce979d5eaf",
   "metadata": {},
   "source": [
    "## Setting vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a95d5a9-7144-4ddc-9f12-d7fcf943f964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/experiments/details/23d52751-4aeb-4e71-a47e-01c1ced25793\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/runs/details/1cf4b585-e78c-4a53-b28b-334b7579aff1\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=1cf4b585-e78c-4a53-b28b-334b7579aff1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kserve_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kserve/component.yaml\"\n",
    ")\n",
    "\n",
    "\n",
    "@pipeline(name=\"KServe pipeline\", description=\"A pipeline for KServe.\")\n",
    "def kservePipeline(\n",
    "    action: str,\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    namespace: str,\n",
    "    framework: str,\n",
    "):\n",
    "    kserve = kserve_op(\n",
    "        action=action,\n",
    "        model_name=model_name,\n",
    "        model_uri=model_uri,\n",
    "        namespace=namespace,\n",
    "        framework=framework,\n",
    "    )\n",
    "\n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_filename = f\"{PIPELINE_NAME}.yaml\"\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path=pipeline_filename)\n",
    "\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    TOKEN = f.read()\n",
    "\n",
    "# Submit the pipeline to the KFP cluster\n",
    "client = kfp.Client(host=KFP_HOST, # Use the configured KFP host\n",
    "                    existing_token=TOKEN)  \n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    kservePipeline,\n",
    "    enable_caching=False,\n",
    "    arguments={\n",
    "        'action': \"apply\",\n",
    "        'model_name': \"youtubegoes5g\",\n",
    "        'model_uri': \"pvc://model-store-claim\",\n",
    "        'namespace': \"kubeflow-user-example-com\",\n",
    "        'framework': \"pytorch\"\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
