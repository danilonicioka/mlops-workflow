{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f35f373-8b9a-4f0f-9439-210b77138e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../model-archiver/model-store/youtubegoes5g/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "627d25e2-a892-4111-9b21-6f575680f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Build model with non-linear activation function\n",
    "class InterruptionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=29, out_features=200)\n",
    "        self.layer_2 = nn.Linear(in_features=200, out_features=100)\n",
    "        self.layer_3 = nn.Linear(in_features=100, out_features=1)\n",
    "        self.relu = nn.ReLU() # <- add in ReLU activation function\n",
    "        # Can also put sigmoid in the model\n",
    "        # This would mean you don't need to use it on the predictions\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Intersperse the ReLU activation function between layers\n",
    "        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f9f54b-ab26-4f48-97a7-b15e44cd4bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pickle import load\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = None\n",
    "\n",
    "class MyHandler():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load scaler\n",
    "            scaler = StandardScaler()\n",
    "            scaler = load(open('../model-archiver/model-store/youtubegoes5g/scaler.pkl', 'rb'))\n",
    "\n",
    "            tensor_list = []\n",
    "            for item in data:\n",
    "                item = scaler.transform([item['data']])\n",
    "                tensor_data = torch.tensor(item, dtype=torch.float32)  # Each instance as a tensor\n",
    "                tensor_list.append(tensor_data)\n",
    "            # Stack all tensors along a new dimension to create a single tensor\n",
    "            combined_tensor = torch.cat(tensor_list, dim=0)\n",
    "            return combined_tensor\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Failed to preprocess input data: \", e, \"data received: \", data)\n",
    "\n",
    "    def inference(self, model_input):\n",
    "        \"\"\"\n",
    "        Perform model inference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inference_list = []\n",
    "            for tensor_data in model_input:\n",
    "                with torch.no_grad():\n",
    "                    output = torch.round(torch.sigmoid(self.model(tensor_data))).squeeze()\n",
    "                #inference = output.cpu().numpy().tolist()\n",
    "                inference_list.append(output)\n",
    "            return inference_list\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Inference failed:\", e)\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        \"\"\"\n",
    "        Convert model output to a list of predictions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Process each item in the batch\n",
    "            result_list = []\n",
    "            for result in inference_output:\n",
    "                if result > 0:\n",
    "                    result_list.append(\"Stall\")\n",
    "                else:\n",
    "                    result_list.append(\"No Stall\")\n",
    "            return result_list\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Failed to postprocess output data: \", e)\n",
    "\n",
    "    def handle(self, data):\n",
    "        \"\"\"\n",
    "        Handle a prediction request.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_input = self.preprocess(data)\n",
    "            model_output = self.inference(model_input)\n",
    "            return self.postprocess(model_output)\n",
    "        except Exception as e:\n",
    "            return [str(e)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "989a17c6-c900-4130-819f-7b58d3cd78a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and handler\n",
    "serialized_file = \"../model-archiver/model-store/youtubegoes5g/model.pt\"\n",
    "    \n",
    "model = InterruptionModel()\n",
    "model.load_state_dict(torch.load(serialized_file, weights_only=True))\n",
    "model.eval()\n",
    "handler = MyHandler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6eea02-e6a6-474a-a2aa-ae864cad11dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  ['No Stall']\n",
      "2.64 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "# Single input test\n",
    "data = [{\"data\": [13,13,13,0,13,13,13,13,-76,-76,-81,-76,-78.5,-76,-76,-7,-7,-12,-7,-9.5,-7,-7,12,12,7,12,9.5,12,12]}]\n",
    "\n",
    "result = handler.handle(data)\n",
    "\n",
    "print(\"result: \" ,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c318feb1-3092-4d66-aa79-47d68c66e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  ['No Stall', 'No Stall', 'Stall', 'Stall']\n",
      "3.46 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "data = [\n",
    "      {\"data\": [13,13,13,0,13,13,13,13,-76,-76,-81,-76,-78.5,-76,-76,-7,-7,-12,-7,-9.5,-7,-7,12,12,7,12,9.5,12,12]},\n",
    "      {\"data\": [13,13,12,0.4714045208,13,12.5,13,13,-81,-81,-82,-81,-81.5,-81,-81,-12,-12,-11,-12,-12,-12,-11.5,7,7,2,7,4.5,7,7]},\n",
    "      {\"data\": [6,7,7,0.4714045208,7,6.5,7,7,-107,-106,-106,-106,-106.5,-106,-106,-13,-14,-14,-14,-14,-14,-13.5,2,2,2,2,2,2,2]},\n",
    "      {\"data\": [8,8,8,0,8,8,8,8,-108,-108,-108,-108,-108,-108,-108,-13,-13,-13,-13,-13,-13,-13,2,2,2,2,2,2,2]}\n",
    "]\n",
    "\n",
    "result = handler.handle(data)\n",
    "\n",
    "print(\"result: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca2dcb54-1304-4200-85cc-4f5544710cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result 1: ['No Stall']\n",
      "result 2: ['No Stall']\n",
      "result 3: ['Stall']\n",
      "result 4: ['Stall']\n",
      "4.38 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "\n",
    "data1 = [{\"data\": [13,13,13,0,13,13,13,13,-76,-76,-81,-76,-78.5,-76,-76,-7,-7,-12,-7,-9.5,-7,-7,12,12,7,12,9.5,12,12]}]\n",
    "data2 = [{\"data\": [13,13,12,0.4714045208,13,12.5,13,13,-81,-81,-82,-81,-81.5,-81,-81,-12,-12,-11,-12,-12,-12,-11.5,7,7,2,7,4.5,7,7]}]\n",
    "data3 = [{\"data\": [6,7,7,0.4714045208,7,6.5,7,7,-107,-106,-106,-106,-106.5,-106,-106,-13,-14,-14,-14,-14,-14,-13.5,2,2,2,2,2,2,2]}]\n",
    "data4 = [{\"data\": [8,8,8,0,8,8,8,8,-108,-108,-108,-108,-108,-108,-108,-13,-13,-13,-13,-13,-13,-13,2,2,2,2,2,2,2]}]\n",
    "\n",
    "result1 = handler.handle(data1)\n",
    "print(\"result 1:\", result1)\n",
    "\n",
    "result2 = handler.handle(data2)\n",
    "print(\"result 2:\", result2)\n",
    "\n",
    "result3 = handler.handle(data3)\n",
    "print(\"result 3:\", result3)\n",
    "\n",
    "result4 = handler.handle(data4)\n",
    "print(\"result 4:\", result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4635fa2-e140-4c0b-995a-b6fc0a343b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Data1 finished with result: ['No Stall']\n",
      "Task Data2 finished with result: ['No Stall']\n",
      "Task Data3 finished with result: ['Stall']\n",
      "Task Data4 finished with result: ['Stall']\n",
      "14.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 1 -n 1\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Define a task function to handle data\n",
    "def handle_data(data, identifier):\n",
    "    handler = MyHandler(model)\n",
    "    result = handler.handle(data)\n",
    "    return f\"Task {identifier} finished with result: {result}\"\n",
    "\n",
    "\n",
    "# Example datasets\n",
    "datasets = [\n",
    "    ([{\"data\": [13, 13, 13, 0, 13, 13, 13, 13, -76, -76, -81, -76, -78.5, -76, -76, -7, -7, -12, -7, -9.5, -7, -7, 12, 12, 7, 12, 9.5, 12, 12]}], \"Data1\"),\n",
    "    ([{\"data\": [13, 13, 12, 0.4714045208, 13, 12.5, 13, 13, -81, -81, -82, -81, -81.5, -81, -81, -12, -12, -11, -12, -12, -12, -11.5, 7, 7, 2, 7, 4.5, 7, 7]}], \"Data2\"),\n",
    "    ([{\"data\": [6, 7, 7, 0.4714045208, 7, 6.5, 7, 7, -107, -106, -106, -106, -106.5, -106, -106, -13, -14, -14, -14, -14, -14, -13.5, 2, 2, 2, 2, 2, 2, 2]}], \"Data3\"),\n",
    "    ([{\"data\": [8, 8, 8, 0, 8, 8, 8, 8, -108, -108, -108, -108, -108, -108, -108, -13, -13, -13, -13, -13, -13, -13, 2, 2, 2, 2, 2, 2, 2]}], \"Data4\"),\n",
    "]\n",
    "\n",
    "# Run tasks concurrently\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(handle_data, dataset[0], dataset[1]) for dataset in datasets]\n",
    "\n",
    "    # Collect and print results as they complete\n",
    "    for future in futures:\n",
    "        print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1a9ee-2cd0-42ef-9c3b-ac51cffc6ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
