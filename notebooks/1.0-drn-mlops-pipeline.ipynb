{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7fdb664-7216-4d08-aba0-afb5986d6adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f2b06e-4971-46eb-bd54-4473990ebf7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow:8888/#/experiments/details/23d52751-4aeb-4e71-a47e-01c1ced25793\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow:8888/#/runs/details/4003b8ca-9730-48ce-bb8e-87855684496c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow:8888/#/pipelines/details/9f242c9c-412f-434f-b20b-566fe36a85bd\" target=\"_blank\" >Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2024, 9, 17, 19, 34, 47, tzinfo=tzlocal()),\n",
       " 'description': None,\n",
       " 'display_name': 'mlops',\n",
       " 'error': None,\n",
       " 'namespace': 'kubeflow-user-example-com',\n",
       " 'pipeline_id': '9f242c9c-412f-434f-b20b-566fe36a85bd'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.dsl import component, pipeline, Input, Output, Dataset, Model, Metrics, ClassificationMetrics, Artifact\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from env file\n",
    "load_dotenv('env')\n",
    "\n",
    "# Github variables\n",
    "GITHUB_USERNAME = os.getenv(\"GITHUB_USERNAME\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "GITHUB_REPO_URL = \"https://github.com/danilonicioka/mlops-workflow.git\"\n",
    "GITHUB_CLONED_DIR = \"mlops-workflow\"\n",
    "GITHUB_TEST_BRANCH = \"tests\"\n",
    "\n",
    "# Kubeflow variables\n",
    "KUBEFLOW_PIPELINE_NAME = \"mlops\"\n",
    "KUBEFLOW_HOST_URL = \"http://ml-pipeline.kubeflow:8888\"  # KFP host URL\n",
    "KUBEFLOW_PIPELINE_ID=\"7451916e-eee8-4c14-ad5f-8dee5aa61e3b\"\n",
    "with open(os.environ['KF_PIPELINES_SA_TOKEN_PATH'], \"r\") as f:\n",
    "    KUBEFLOW_TOKEN = f.read()\n",
    "\n",
    "# DVC variables\n",
    "REMOTE_NAME = \"minio_remote\"\n",
    "REMOTE_URL = \"s3://dvc-data\"\n",
    "MINIO_URL = \"minio-service.kubeflow:9000\"\n",
    "ACCESS_KEY = os.getenv(\"ACCESS_KEY\")\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "DVC_FILE_DIR = 'data/external'\n",
    "DVC_FILE_NAME = 'dataset.csv'\n",
    "BUCKET_NAME = \"model-files\"\n",
    "MODEL_OBJECT_NAME = \"model-store/youtubegoes5g/model.pt\"\n",
    "\n",
    "TRIGGER_TYPE = '1'\n",
    "PERFORMANCE_FACTOR = 5.0\n",
    "\n",
    "# Model config\n",
    "LR = 0.0001\n",
    "EPOCHS = 3500\n",
    "PRINT_FREQUENCY = 500\n",
    "\n",
    "# Model serve config vars\n",
    "MODEL_NAME = \"youtubegoes5g\"\n",
    "FRAMEWORK = \"pytorch\"\n",
    "NAMESPACE = \"kubeflow-user-example-com\"\n",
    "SVC_ACC = \"sa-minio-kserve\"\n",
    "#MODEL_URI = \"pvc://model-store-claim\"\n",
    "#MODEL_URI = \"minio-service.kubeflow:9000/model-files\"\n",
    "\n",
    "# Temp dir and files to save accuracy for trigger 3\n",
    "TEMP_DIR = \"tmp\"\n",
    "TEMP_FILE_ACC_IN_LAST_RUN = \"accuracy_in_last_run.txt\"\n",
    "LAST_ACC_OBJECT_NAME = \"accuracy-score/last_acc.txt\"\n",
    "\n",
    "# Define a KFP component factory function for data ingestion\n",
    "@component(base_image=\"python:3.11.9\",packages_to_install=['gitpython', 'dvc==3.54.1', 'dvc-s3==3.2.0', 'numpy==1.25.2', 'pandas==2.0.3'])\n",
    "def data_ingestion(\n",
    "    github_repo_url: str,\n",
    "    github_cloned_dir: str,\n",
    "    github_test_branch: str,\n",
    "    github_username: str,\n",
    "    github_token: str,\n",
    "    remote_name: str,\n",
    "    remote_url: str,\n",
    "    minio_url: str,\n",
    "    access_key: str,\n",
    "    secret_key: str,\n",
    "    dvc_file_dir: str,\n",
    "    dvc_file_name: str,\n",
    "    dataset_artifact: Output[Dataset],\n",
    "    dataset_info: Output[Metrics]\n",
    "    ):\n",
    "    from git import Repo\n",
    "    from subprocess import run, CalledProcessError\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    def clone_repository_with_token(github_repo_url, github_cloned_dir, github_test_branch, github_username, github_token):\n",
    "        \"\"\"Clone a Git repository using a GitHub token in the URL and specifying the branch.\"\"\"\n",
    "        try:\n",
    "            # Construct the URL with the GitHub username and token\n",
    "            url_with_token = f\"https://{github_username}:{github_token}@{github_repo_url.split('//')[1]}\"\n",
    "            \n",
    "            # Clone the repository from the specified branch\n",
    "            repo = Repo.clone_from(url_with_token, github_cloned_dir, branch=github_test_branch)\n",
    "            return \"Repository cloned successfully\"\n",
    "        except Exception as e:\n",
    "            return f\"Error occurred during repository cloning: {e}\"\n",
    "\n",
    "    def configure_dvc_remote(github_cloned_dir, remote_name, remote_url, minio_url, access_key, secret_key):\n",
    "        http_minio = f'http://{minio_url}'\n",
    "        \"\"\"Configure the Minio bucket as the DVC remote repository using the `dvc remote` commands.\"\"\"\n",
    "        try:\n",
    "            # Add the remote\n",
    "            run(\n",
    "                ['dvc', 'remote', 'add', '-d', remote_name, remote_url],\n",
    "                cwd=github_cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Configure the endpoint URL\n",
    "            run(\n",
    "                ['dvc', 'remote', 'modify', remote_name, 'endpointurl', http_minio],\n",
    "                cwd=github_cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Configure access key ID\n",
    "            run(\n",
    "                ['dvc', 'remote', 'modify', remote_name, 'access_key_id', access_key],\n",
    "                cwd=github_cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            # Configure secret access key\n",
    "            run(\n",
    "                ['dvc', 'remote', 'modify', remote_name, 'secret_access_key', secret_key],\n",
    "                cwd=github_cloned_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                check=True\n",
    "            )\n",
    "            \n",
    "            return f'Successfully configured Minio bucket as DVC remote repository: {remote_name}'\n",
    "        except CalledProcessError as e:\n",
    "            # Log and raise any errors\n",
    "            return f'Failed to configure DVC remote: {e.stderr}'\n",
    "\n",
    "    def perform_dvc_pull(github_cloned_dir, remote_name):\n",
    "        \"\"\"Perform a DVC pull to synchronize local data with the remote repository.\"\"\"\n",
    "        try:\n",
    "            # Run the `dvc pull` command\n",
    "            result = run(['dvc', 'pull', '-r', remote_name], cwd=github_cloned_dir, capture_output=True, text=True)\n",
    "            \n",
    "            # Check if the command executed successfully\n",
    "            if result.returncode != 0:\n",
    "                # Log and raise an error if the command failed\n",
    "                error_message = f\"dvc pull failed with error: {result.stderr}\"\n",
    "                raise Exception(error_message)\n",
    "            \n",
    "            # Log successful operation\n",
    "            return \"Successfully pulled data from remote DVC repository\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log and handle the error\n",
    "            return f\"Error occurred during dvc pull: {e}\"\n",
    "\n",
    "    # Call the functions\n",
    "    clone_result = clone_repository_with_token(github_repo_url, github_cloned_dir, github_test_branch, github_username, github_token)\n",
    "    configure_result = configure_dvc_remote(github_cloned_dir, remote_name, remote_url, minio_url, access_key, secret_key)\n",
    "    dvc_pull_result = perform_dvc_pull(github_cloned_dir, remote_name)\n",
    "\n",
    "    # Save dataset with pandas in Dataset artifact\n",
    "    pulled_dataset_path = os.path.join(github_cloned_dir, dvc_file_dir, dvc_file_name)\n",
    "    tmp_dataset_path = \"/tmp/\" + dvc_file_name\n",
    "    dataset = pd.read_csv(pulled_dataset_path)\n",
    "    dataset.to_pickle(tmp_dataset_path)\n",
    "    os.rename(tmp_dataset_path, dataset_artifact.path)\n",
    "    \n",
    "    # save dataset info to see on kubeflow graph\n",
    "    dataset_info.log_metric(\"Dataset size\", dataset.shape[0])\n",
    "    \n",
    "# Component for data preparation\n",
    "@component(base_image=\"python:3.11.9\", packages_to_install=['pandas==2.0.3', 'numpy==1.25.2', 'torch==2.3.0', 'scikit-learn==1.2.2', 'imblearn'])\n",
    "def data_preparation(\n",
    "    dataset_artifact: Input[Dataset],\n",
    "    X_train_artifact: Output[Dataset], \n",
    "    X_test_artifact: Output[Dataset],\n",
    "    y_train_artifact: Output[Dataset],\n",
    "    y_test_artifact: Output[Dataset],\n",
    "    test_size: float = 0.2, \n",
    "    random_state: int = 42\n",
    "    ):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    # Load dataset from Dataset artifact\n",
    "    df = pd.read_pickle(dataset_artifact.path)\n",
    "\n",
    "    # Handle null values and replace specific characters\n",
    "    #df = df.replace([' ', '-',np.nan], 0) # There are null values\n",
    "    df = df.replace([' ', '-', np.nan], np.nan)\n",
    "\n",
    "    # Selective columns for mean calculation\n",
    "    columns_to_convert = [\n",
    "        'CQI1', 'CQI2', 'CQI3', 'cSTD CQI', 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n",
    "        'RSRP1', 'RSRP2', 'RSRP3', 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n",
    "        'RSRQ1', 'RSRQ2', 'RSRQ3', 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n",
    "        'SNR1', 'SNR2', 'SNR3', 'sMajority', 's25 P', 's50 P', 's75 P'\n",
    "    ]\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype(float)\n",
    "\n",
    "    # Replace np.nan with mean values for selective columns\n",
    "    df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n",
    "\n",
    "    # Convert 'Stall' column to numerical values\n",
    "    df['Stall'].replace({'Yes': 1, 'No': 0}, inplace=True)\n",
    "\n",
    "    X = df[columns_to_convert].values\n",
    "    y = df['Stall'].values\n",
    "\n",
    "    # Apply SMOTE for balancing the dataset\n",
    "    # oversample = SMOTE(random_state=random_state)\n",
    "    oversample = SMOTE()\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    X_train_path = \"/tmp/X_train.pt\"\n",
    "    X_test_path = \"/tmp/X_test.pt\"\n",
    "    y_train_path = \"/tmp/y_train.pt\"\n",
    "    y_test_path = \"/tmp/y_test.pt\"\n",
    "    torch.save(X_train, X_train_path)\n",
    "    os.rename(X_train_path, X_train_artifact.path)\n",
    "    \n",
    "    torch.save(X_test, X_test_path)\n",
    "    os.rename(X_test_path, X_test_artifact.path)\n",
    "\n",
    "    torch.save(y_train, y_train_path)\n",
    "    os.rename(y_train_path, y_train_artifact.path)\n",
    "\n",
    "    torch.save(y_test, y_test_path)\n",
    "    os.rename(y_test_path, y_test_artifact.path)\n",
    "\n",
    "# Component for model training\n",
    "@component(base_image=\"python:3.11.9\", packages_to_install=['torch==2.3.0', 'scikit-learn==1.2.2', 'numpy==1.25.2','Minio==7.2.5'])\n",
    "def model_training(\n",
    "    X_train_artifact: Input[Dataset], \n",
    "    X_test_artifact: Input[Dataset],\n",
    "    y_train_artifact: Input[Dataset],\n",
    "    y_test_artifact: Input[Dataset],\n",
    "    metrics: Output[Metrics], \n",
    "    classification_metrics: Output[ClassificationMetrics], \n",
    "    model_trained_artifact: Output[Model],\n",
    "    up_model_cond: Output[Artifact],\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    print_frequency: int,\n",
    "    minio_url: str,\n",
    "    access_key: str,\n",
    "    secret_key: str,\n",
    "    bucket_name: str,\n",
    "    model_object_name: str,\n",
    "    trigger_type: str,\n",
    "    performance_factor: float,\n",
    "    last_accuracy_object_name: str,\n",
    "    tmp_dir: str,\n",
    "    tmp_file_last_acc: str\n",
    "    ):\n",
    "    import os\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "    from minio import Minio\n",
    "    \n",
    "    # Build model with non-linear activation function\n",
    "    class InterruptionModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer_1 = nn.Linear(in_features=29, out_features=200)\n",
    "            self.layer_2 = nn.Linear(in_features=200, out_features=100)\n",
    "            self.layer_3 = nn.Linear(in_features=100, out_features=1)\n",
    "            self.relu = nn.ReLU() # <- add in ReLU activation function\n",
    "            # Can also put sigmoid in the model\n",
    "            # This would mean you don't need to use it on the predictions\n",
    "            # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Intersperse the ReLU activation function between layers\n",
    "            return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "    # Helper functions\n",
    "    def accuracy_fn(y_true, y_pred):\n",
    "        correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "        acc = (correct / len(y_pred)) * 100\n",
    "        return acc\n",
    "\n",
    "    def minio_setup(minio_url, access_key, secret_key):\n",
    "        # Initialize Minio client with just the base URL (without path)\n",
    "        client = Minio(\n",
    "            minio_url,  # Ensure minio_url does not include a path, only the base URL (e.g., http://localhost:9000)\n",
    "            access_key=access_key,\n",
    "            secret_key=secret_key,\n",
    "            secure=False  # Minio is using HTTP on localhost:9000\n",
    "        )\n",
    "        return client\n",
    "\n",
    "    def upload_file(client, bucket_name, object_name, filepath):\n",
    "        # Create the bucket if it does not exist\n",
    "        if not client.bucket_exists(bucket_name):\n",
    "            client.make_bucket(bucket_name)\n",
    "            create_bucket_result = f\"Successfully created bucket: {bucket_name}\"\n",
    "        else:\n",
    "            create_bucket_result = f\"Bucket {bucket_name} already exists\"\n",
    "    \n",
    "        try:\n",
    "            # Upload the file to the specified path in the bucket\n",
    "            client.fput_object(bucket_name, object_name, filepath)\n",
    "            return (f'Successfully uploaded {filepath} to {bucket_name}/{object_name}')\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and raise any upload errors\n",
    "            raise Exception(f'Failed to upload model to Minio: {e}')\n",
    "\n",
    "    def read_from_minio(client, bucket_name, object_name):\n",
    "        \"\"\"\n",
    "        Function to read a file from a MinIO bucket and convert its single content to a float.\n",
    "        If the file is not found or is empty, it returns 0.0.\n",
    "\n",
    "        Args:\n",
    "            client: minio client\n",
    "            bucket_name (str): The name of the bucket in MinIO.\n",
    "            object_name (str): The name of the object (file) in the bucket.\n",
    "\n",
    "        Returns:\n",
    "            float: The float value converted from the file content, or 0.0 if the file is not found or empty.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get the file from the MinIO bucket\n",
    "            response = client.get_object(bucket_name, object_name)\n",
    "\n",
    "            # Read the file content into a buffer\n",
    "            file_data = response.read()\n",
    "\n",
    "            # Decode file content and strip whitespace\n",
    "            content = file_data.decode('utf-8').strip()\n",
    "\n",
    "            # If the content is empty, return 0.0\n",
    "            if not content:\n",
    "                print(f\"File {object_name} is empty.\")\n",
    "                return 0.0\n",
    "\n",
    "            # Convert the content to a float\n",
    "            float_value = float(content)\n",
    "            return float_value\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle file not found or any other errors\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def save_float_to_tempfile(float_value, dir_name, file_name):\n",
    "        \"\"\"\n",
    "        Saves a float value to a specified directory and file name.\n",
    "\n",
    "        Args:\n",
    "            float_value (float): The float value to save.\n",
    "            dir_name (str): The name of the directory to save the file in.\n",
    "            file_name (str): The name of the file.\n",
    "        \n",
    "        Returns:\n",
    "            str: The path to the file.\n",
    "        \"\"\"\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        temp_file_path = os.path.join(dir_name, file_name)\n",
    "        \n",
    "        with open(temp_file_path, 'w') as temp_file:\n",
    "            # Convert the float to a string, then write to file\n",
    "            temp_file.write(str(float_value))\n",
    "        \n",
    "        return temp_file_path\n",
    "\n",
    "    def get_accuracy_in_last_run(client, bucket_name, object_name):\n",
    "        accuracy_in_last_run = read_from_minio(client, bucket_name, object_name)\n",
    "        return accuracy_in_last_run\n",
    "\n",
    "    def update_accuracy_in_last_run(client, bucket_name, object_name, new_value, tmp_dir, tmp_file):\n",
    "        filepath = save_float_to_tempfile(new_value, tmp_dir, tmp_file)\n",
    "        upload_file(client, bucket_name, object_name, filepath)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = InterruptionModel().to(device)\n",
    "\n",
    "    # Setup loss and optimizer\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Fit the model\n",
    "    torch.manual_seed(42)\n",
    "    epochs = epochs\n",
    "\n",
    "    # Put all data on target device\n",
    "    X_train = torch.load(X_train_artifact.path)\n",
    "    X_test = torch.load(X_test_artifact.path)\n",
    "    y_train = torch.load(y_train_artifact.path)\n",
    "    y_test = torch.load(y_test_artifact.path)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1. Forward pass\n",
    "        y_logits = model(X_train).squeeze()\n",
    "\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "        # 2. Calculate loss and accuracy\n",
    "        loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n",
    "        acc = accuracy_fn(y_true=y_train,\n",
    "                        y_pred=y_pred)\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "        # 1. Forward pass\n",
    "            test_logits = model(X_test).squeeze()\n",
    "            #print(test_logits.shape)\n",
    "            test_pred = torch.round(torch.sigmoid(test_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "            # 2. Calcuate loss and accuracy\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_true=y_test,\n",
    "                                y_pred=test_pred)\n",
    "\n",
    "\n",
    "        # Print out what's happening\n",
    "        if epoch % print_frequency == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_preds = torch.round(torch.sigmoid(model(X_test))).squeeze()\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        predictions = y_preds.cpu().numpy() #if it is cuda, then this, otherwise y_pred.numpy()\n",
    "        true_labels = y_test.cpu().numpy()\n",
    "    else:\n",
    "        predictions = y_preds.numpy()\n",
    "        true_labels = y_test.numpy()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cmatrix = confusion_matrix(true_labels, predictions)\n",
    "    #print(\"Confusion Matrix:\", cmatrix)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    metrics.log_metric(\"Accuracy\", accuracy)\n",
    "    #print('Accuracy: %f' % accuracy)\n",
    "\n",
    "    # test accuracy\n",
    "    print(type(accuracy))\n",
    "\n",
    "    precision = precision_score(true_labels,  predictions, average='weighted')\n",
    "    metrics.log_metric(\"Precision\", precision)\n",
    "    #print('Precision: %f' % precision)\n",
    "\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    metrics.log_metric(\"Recall\", recall)\n",
    "    #print('Recall: %f' % recall)\n",
    "\n",
    "    microf1 = f1_score(true_labels, predictions, average='micro')\n",
    "    metrics.log_metric(\"Micro F1 score\", microf1)\n",
    "    #print('Micro F1 score: %f' % microf1)\n",
    "\n",
    "    macrof1 = f1_score(true_labels, predictions, average='macro')\n",
    "    metrics.log_metric(\"Macro F1 score\", macrof1)\n",
    "    #print('Macro F1 score: %f' % macrof1)\n",
    "\n",
    "    target_names = ['No-Stall', 'Stall']\n",
    "    # Print precision-recall report\n",
    "    #print(classification_report(true_labels, predictions, target_names=target_names))\n",
    "\n",
    "    # Classification Metrics artifact\n",
    "    cmatrix = cmatrix.tolist()\n",
    "    target_names = ['No-Stall', 'Stall']\n",
    "    classification_metrics.log_confusion_matrix(target_names, cmatrix)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = \"/tmp/model.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    os.rename(model_path, model_trained_artifact.path)\n",
    "\n",
    "    # Setup minio client to upload and read files\n",
    "    client = minio_setup(minio_url, access_key, secret_key)\n",
    "\n",
    "    previous_accuracy = get_accuracy_in_last_run(client, bucket_name, last_accuracy_object_name)\n",
    "\n",
    "    print(accuracy-previous_accuracy)\n",
    "    metrics.log_metric(\"current-previous accuracy\", accuracy-previous_accuracy)\n",
    "\n",
    "    if trigger_type == '1' or trigger_type == '2':\n",
    "        up_model = True\n",
    "    elif trigger_type == '3':\n",
    "        if accuracy - previous_accuracy > performance_factor:\n",
    "            up_model = True\n",
    "            update_accuracy_in_last_run(client, bucket_name, last_accuracy_object_name, accuracy, tmp_dir, tmp_file_last_acc)\n",
    "    else:\n",
    "        up_model = False\n",
    "        metrics.log_metric(\"up model\", '0')\n",
    "        with open(up_model_cond.path, 'w') as f:\n",
    "            f.write('0')\n",
    "    \n",
    "    if up_model:\n",
    "        metrics.log_metric(\"up model\", '1')\n",
    "        with open(up_model_cond.path, 'w') as f:\n",
    "            f.write('1')\n",
    "        upload_model_result = upload_file(client, bucket_name, model_object_name, model_trained_artifact.path)\n",
    "\n",
    "@component(base_image=\"python:3.11.9\", packages_to_install=['kserve==0.13.0','kubernetes==30.1.0'])\n",
    "def model_serving(\n",
    "    up_model_cond: Input[Artifact],\n",
    "    cond_info: Output[Metrics],\n",
    "    bucket_name: str,\n",
    "    model_name: str,\n",
    "    namespace: str,\n",
    "    svc_acc: str\n",
    "):\n",
    "    # Create kserve instance\n",
    "    from kubernetes import client \n",
    "    from kserve import KServeClient, constants, V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1TorchServeSpec\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    \n",
    "    # exec if a new model was uploaded\n",
    "    with open(up_model_cond.path) as f:\n",
    "        up_model = f.read()\n",
    "        \n",
    "    if up_model == '1':\n",
    "        cond_info.log_metric(\"Up model\", up_model)\n",
    "        \n",
    "        #Inference server config\n",
    "        now = datetime.now()\n",
    "        kserve_version='v1beta1'\n",
    "        api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "        # with open(model_uri.path) as f:\n",
    "        #     uri = f.read()\n",
    "        uri = f's3://{bucket_name}'\n",
    "\n",
    "        isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                       kind=constants.KSERVE_KIND,\n",
    "                                       metadata=client.V1ObjectMeta(\n",
    "                                           name=model_name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                       spec=V1beta1InferenceServiceSpec(\n",
    "                                       predictor=V1beta1PredictorSpec(\n",
    "                                           service_account_name=svc_acc,\n",
    "                                           pytorch=(V1beta1TorchServeSpec(\n",
    "                                               storage_uri=uri))))\n",
    "        )\n",
    "\n",
    "        KServe = KServeClient()\n",
    "\n",
    "        #replace old inference service with a new one\n",
    "        try:\n",
    "            KServe.delete(name=model_name, namespace=namespace)\n",
    "            print(\"Old model deleted\")\n",
    "        except:\n",
    "            print(\"Couldn't delete old model\")\n",
    "        time.sleep(10)\n",
    "\n",
    "        KServe.create(isvc)\n",
    "    else:\n",
    "        cond_info.log_metric(\"Up model\", '0')\n",
    "\n",
    "@pipeline\n",
    "def my_pipeline(\n",
    "    github_repo_url: str,\n",
    "    github_cloned_dir: str,\n",
    "    github_test_branch: str,\n",
    "    github_username: str,\n",
    "    github_token: str,\n",
    "    remote_name: str,\n",
    "    remote_url: str,\n",
    "    minio_url: str,\n",
    "    access_key: str,\n",
    "    secret_key: str,\n",
    "    dvc_file_dir: str,\n",
    "    dvc_file_name: str,\n",
    "    model_name: str,\n",
    "    namespace: str,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    print_frequency: int,\n",
    "    bucket_name: str,\n",
    "    model_object_name: str,\n",
    "    svc_acc: str,\n",
    "    trigger_type: str,\n",
    "    performance_factor: float,\n",
    "    last_accuracy_object_name: str,\n",
    "    tmp_dir: str,\n",
    "    tmp_file_last_acc: str\n",
    "):\n",
    "    data_ingestion_task = data_ingestion(\n",
    "        github_repo_url=github_repo_url,\n",
    "        github_cloned_dir=github_cloned_dir,\n",
    "        github_test_branch=github_test_branch,\n",
    "        github_username=github_username,\n",
    "        github_token=github_token,\n",
    "        remote_name=remote_name,\n",
    "        remote_url=remote_url,\n",
    "        minio_url=minio_url,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key,\n",
    "        dvc_file_dir=dvc_file_dir,\n",
    "        dvc_file_name=dvc_file_name)\n",
    "    dataset_artifact = data_ingestion_task.outputs[\"dataset_artifact\"]\n",
    "    data_preparation_task = data_preparation(dataset_artifact=dataset_artifact)\n",
    "    X_train_artifact = data_preparation_task.outputs[\"X_train_artifact\"]\n",
    "    X_test_artifact = data_preparation_task.outputs[\"X_test_artifact\"]\n",
    "    y_train_artifact = data_preparation_task.outputs[\"y_train_artifact\"]\n",
    "    y_test_artifact = data_preparation_task.outputs[\"y_test_artifact\"]\n",
    "    model_training_task = model_training(X_train_artifact=X_train_artifact, \n",
    "                                         X_test_artifact=X_test_artifact, \n",
    "                                         y_train_artifact=y_train_artifact, \n",
    "                                         y_test_artifact=y_test_artifact,\n",
    "                                         lr=lr,\n",
    "                                         epochs=epochs,\n",
    "                                         print_frequency=print_frequency,\n",
    "                                         minio_url=minio_url,\n",
    "                                         access_key=access_key,\n",
    "                                         secret_key=secret_key,\n",
    "                                         bucket_name=bucket_name,\n",
    "                                         model_object_name=model_object_name,\n",
    "                                         trigger_type=trigger_type,\n",
    "                                         performance_factor=performance_factor,\n",
    "                                         last_accuracy_object_name=last_accuracy_object_name,\n",
    "                                         tmp_dir=tmp_dir,\n",
    "                                         tmp_file_last_acc=tmp_file_last_acc)\n",
    "    up_model_cond = model_training_task.outputs[\"up_model_cond\"]\n",
    "    model_serving_task = model_serving(up_model_cond=up_model_cond,\n",
    "                                       bucket_name=bucket_name,\n",
    "                                       model_name=model_name, \n",
    "                                       namespace=namespace,\n",
    "                                       svc_acc=svc_acc)\n",
    "\n",
    "# Compile the pipeline\n",
    "pipeline_filename = f\"{KUBEFLOW_PIPELINE_NAME}.yaml\"\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path=pipeline_filename)\n",
    "\n",
    "# Submit the pipeline to the KFP cluster\n",
    "client = kfp.Client(\n",
    "    host=KUBEFLOW_HOST_URL,\n",
    "    existing_token=KUBEFLOW_TOKEN)  \n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    my_pipeline,\n",
    "    enable_caching=False,\n",
    "    arguments={\n",
    "        'github_repo_url': GITHUB_REPO_URL,\n",
    "        'github_cloned_dir': GITHUB_CLONED_DIR,\n",
    "        'github_test_branch': GITHUB_TEST_BRANCH,\n",
    "        'github_username': GITHUB_USERNAME,\n",
    "        'github_token': GITHUB_TOKEN,\n",
    "        'remote_name': REMOTE_NAME,\n",
    "        'remote_url': REMOTE_URL,\n",
    "        'minio_url': MINIO_URL,\n",
    "        'access_key': ACCESS_KEY,\n",
    "        'secret_key': SECRET_KEY,\n",
    "        'dvc_file_dir': DVC_FILE_DIR,\n",
    "        'dvc_file_name': DVC_FILE_NAME,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'namespace': NAMESPACE,\n",
    "        'lr': LR,\n",
    "        'epochs': EPOCHS,\n",
    "        'print_frequency': PRINT_FREQUENCY,\n",
    "        'bucket_name': BUCKET_NAME,\n",
    "        'model_object_name': MODEL_OBJECT_NAME,\n",
    "        'svc_acc': SVC_ACC,\n",
    "        'trigger_type': TRIGGER_TYPE,\n",
    "        'performance_factor': PERFORMANCE_FACTOR,\n",
    "        'last_accuracy_object_name': LAST_ACC_OBJECT_NAME,\n",
    "        'tmp_dir': TEMP_DIR,\n",
    "        'tmp_file_last_acc': TEMP_FILE_ACC_IN_LAST_RUN\n",
    "    })\n",
    "\n",
    "#upload to Kubeflow \n",
    "client.upload_pipeline(pipeline_package_path=pipeline_filename,\n",
    "                       pipeline_name=KUBEFLOW_PIPELINE_NAME,\n",
    "                       namespace = NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63d36a-aade-43f9-abe1-b95581cbee20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
