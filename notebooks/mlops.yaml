# PIPELINE DEFINITION
# Name: my-pipeline
# Inputs:
#    bucket_name: str
#    dvc_file_dir: str
#    dvc_file_name: str
#    dvc_remote_db_url: str
#    dvc_remote_name: str
#    github_cloned_dir: str
#    github_dvc_branch: str
#    github_main_branch: str
#    github_repo_url: str
#    github_token: str
#    github_username: str
#    k8s_api_token: str
#    kserve_namespace: str
#    kserve_svc_acc: str
#    last_accuracy_object_name: str
#    minio_access_key: str
#    minio_model_object_name: str
#    minio_secret_key: str
#    minio_url: str
#    model_epochs: int
#    model_lr: float
#    model_name: str
#    model_print_frequency_per_n_epochs: int
#    performance_factor: float
#    tmp_dir: str
#    tmp_file_last_acc: str
#    trigger_type: str
# Outputs:
#    data-ingestion-dataset_info: system.Metrics
#    mar-gen-mar_gen_info: system.Metrics
#    model-serving-cond_info: system.Metrics
#    model-training-classification_metrics: system.ClassificationMetrics
#    model-training-metrics: system.Metrics
#    model-training-up_model_info: system.Metrics
components:
  comp-data-ingestion:
    executorLabel: exec-data-ingestion
    inputDefinitions:
      parameters:
        dvc_file_dir:
          parameterType: STRING
        dvc_file_name:
          parameterType: STRING
        dvc_remote_db_url:
          parameterType: STRING
        dvc_remote_name:
          parameterType: STRING
        github_cloned_dir:
          parameterType: STRING
        github_dvc_branch:
          parameterType: STRING
        github_repo_url:
          parameterType: STRING
        github_token:
          parameterType: STRING
        github_username:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        minio_url:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-data-preparation:
    executorLabel: exec-data-preparation
    inputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-mar-gen:
    executorLabel: exec-mar-gen
    inputDefinitions:
      artifacts:
        up_model_cond:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        bucket_name:
          parameterType: STRING
        github_cloned_dir:
          parameterType: STRING
        github_main_branch:
          parameterType: STRING
        github_repo_url:
          parameterType: STRING
        github_token:
          parameterType: STRING
        github_username:
          parameterType: STRING
        k8s_api_token:
          parameterType: STRING
        kserve_namespace:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        minio_url:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        mar_gen_cond:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        mar_gen_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-model-serving:
    executorLabel: exec-model-serving
    inputDefinitions:
      artifacts:
        mar_gen_cond:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        bucket_name:
          parameterType: STRING
        kserve_namespace:
          parameterType: STRING
        kserve_svc_acc:
          parameterType: STRING
        model_name:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        cond_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-model-training:
    executorLabel: exec-model-training
    inputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        bucket_name:
          parameterType: STRING
        last_accuracy_object_name:
          parameterType: STRING
        minio_access_key:
          parameterType: STRING
        minio_model_object_name:
          parameterType: STRING
        minio_secret_key:
          parameterType: STRING
        minio_url:
          parameterType: STRING
        model_epochs:
          parameterType: NUMBER_INTEGER
        model_lr:
          parameterType: NUMBER_DOUBLE
        model_print_frequency_per_n_epochs:
          parameterType: NUMBER_INTEGER
        performance_factor:
          parameterType: NUMBER_DOUBLE
        tmp_dir:
          parameterType: STRING
        tmp_file_last_acc:
          parameterType: STRING
        trigger_type:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        classification_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model_trained_artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        up_model_cond:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        up_model_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-data-ingestion:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_ingestion
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'gitpython'\
          \ 'dvc==3.54.1' 'dvc-s3==3.2.0' 'numpy==1.25.2' 'pandas==2.0.3' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_ingestion(\n    github_repo_url: str,\n    github_cloned_dir:\
          \ str,\n    github_dvc_branch: str,\n    github_username: str,\n    github_token:\
          \ str,\n    dvc_remote_name: str,\n    dvc_remote_db_url: str,\n    minio_url:\
          \ str,\n    minio_access_key: str,\n    minio_secret_key: str,\n    dvc_file_dir:\
          \ str,\n    dvc_file_name: str,\n    dataset_artifact: Output[Dataset],\n\
          \    dataset_info: Output[Metrics]\n    ):\n    from git import Repo\n \
          \   from subprocess import run, CalledProcessError\n    import os\n    import\
          \ pandas as pd\n\n    def clone_repository_with_token(github_repo_url, github_cloned_dir,\
          \ github_dvc_branch, github_username, github_token):\n        \"\"\"Clone\
          \ a Git repository using a GitHub token in the URL and specifying the branch.\"\
          \"\"\n        try:\n            # Construct the URL with the GitHub username\
          \ and token\n            url_with_token = f\"https://{github_username}:{github_token}@{github_repo_url.split('//')[1]}\"\
          \n\n            # Clone the repository from the specified branch\n     \
          \       repo = Repo.clone_from(url_with_token, github_cloned_dir, branch=github_dvc_branch)\n\
          \            return \"Repository cloned successfully\"\n        except Exception\
          \ as e:\n            return f\"Error occurred during repository cloning:\
          \ {e}\"\n\n    def configure_dvc_remote(github_cloned_dir, dvc_remote_name,\
          \ dvc_remote_db_url, minio_url, minio_access_key, minio_secret_key):\n \
          \       http_minio = f'http://{minio_url}'\n        \"\"\"Configure the\
          \ Minio bucket as the DVC remote repository using the `dvc remote` commands.\"\
          \"\"\n        try:\n            # Add the remote\n            run(\n   \
          \             ['dvc', 'remote', 'add', '-d', dvc_remote_name, dvc_remote_db_url],\n\
          \                cwd=github_cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure the endpoint URL\n            run(\n         \
          \       ['dvc', 'remote', 'modify', dvc_remote_name, 'endpointurl', http_minio],\n\
          \                cwd=github_cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure access key ID\n            run(\n            \
          \    ['dvc', 'remote', 'modify', dvc_remote_name, 'access_key_id', minio_access_key],\n\
          \                cwd=github_cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure secret access key\n            run(\n        \
          \        ['dvc', 'remote', 'modify', dvc_remote_name, 'secret_access_key',\
          \ minio_secret_key],\n                cwd=github_cloned_dir,\n         \
          \       capture_output=True,\n                text=True,\n             \
          \   check=True\n            )\n\n            return f'Successfully configured\
          \ Minio bucket as DVC remote repository: {dvc_remote_name}'\n        except\
          \ CalledProcessError as e:\n            # Log and raise any errors\n   \
          \         return f'Failed to configure DVC remote: {e.stderr}'\n\n    def\
          \ perform_dvc_pull(github_cloned_dir, dvc_remote_name):\n        \"\"\"\
          Perform a DVC pull to synchronize local data with the remote repository.\"\
          \"\"\n        try:\n            # Run the `dvc pull` command\n         \
          \   result = run(['dvc', 'pull', '-r', dvc_remote_name], cwd=github_cloned_dir,\
          \ capture_output=True, text=True)\n\n            # Check if the command\
          \ executed successfully\n            if result.returncode != 0:\n      \
          \          # Log and raise an error if the command failed\n            \
          \    error_message = f\"dvc pull failed with error: {result.stderr}\"\n\
          \                raise Exception(error_message)\n\n            # Log successful\
          \ operation\n            return \"Successfully pulled data from remote DVC\
          \ repository\"\n\n        except Exception as e:\n            # Log and\
          \ handle the error\n            return f\"Error occurred during dvc pull:\
          \ {e}\"\n\n    # Call the functions\n    clone_result = clone_repository_with_token(github_repo_url,\
          \ github_cloned_dir, github_dvc_branch, github_username, github_token)\n\
          \    configure_result = configure_dvc_remote(github_cloned_dir, dvc_remote_name,\
          \ dvc_remote_db_url, minio_url, minio_access_key, minio_secret_key)\n  \
          \  dvc_pull_result = perform_dvc_pull(github_cloned_dir, dvc_remote_name)\n\
          \n    # Save dataset with pandas in Dataset artifact\n    pulled_dataset_path\
          \ = os.path.join(github_cloned_dir, dvc_file_dir, dvc_file_name)\n    tmp_dataset_path\
          \ = \"/tmp/\" + dvc_file_name\n    dataset = pd.read_csv(pulled_dataset_path)\n\
          \    dataset.to_pickle(tmp_dataset_path)\n    os.rename(tmp_dataset_path,\
          \ dataset_artifact.path)\n\n    # save dataset info to see on kubeflow graph\n\
          \    dataset_info.log_metric(\"Dataset size\", dataset.shape[0])\n\n"
        image: python:3.11.9
    exec-data-preparation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preparation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.3'\
          \ 'numpy==1.25.2' 'torch==2.3.0' 'scikit-learn==1.2.2' 'imblearn' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preparation(\n    dataset_artifact: Input[Dataset],\n  \
          \  X_train_artifact: Output[Dataset], \n    X_test_artifact: Output[Dataset],\n\
          \    y_train_artifact: Output[Dataset],\n    y_test_artifact: Output[Dataset],\n\
          \    test_size: float = 0.2, \n    random_state: int = 42\n    ):\n    import\
          \ pandas as pd\n    import numpy as np\n    from sklearn.model_selection\
          \ import train_test_split\n    from imblearn.over_sampling import SMOTE\n\
          \    from sklearn.preprocessing import StandardScaler\n    import torch\n\
          \    import os\n\n    # Load dataset from Dataset artifact\n    df = pd.read_pickle(dataset_artifact.path)\n\
          \n    # Handle null values and replace specific characters\n    #df = df.replace(['\
          \ ', '-',np.nan], 0) # There are null values\n    df = df.replace([' ',\
          \ '-', np.nan], np.nan)\n\n    # Selective columns for mean calculation\n\
          \    columns_to_convert = [\n        'CQI1', 'CQI2', 'CQI3', 'cSTD CQI',\
          \ 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n        'RSRP1', 'RSRP2', 'RSRP3',\
          \ 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n        'RSRQ1', 'RSRQ2', 'RSRQ3',\
          \ 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n        'SNR1', 'SNR2', 'SNR3',\
          \ 'sMajority', 's25 P', 's50 P', 's75 P'\n    ]\n    df[columns_to_convert]\
          \ = df[columns_to_convert].astype(float)\n\n    # Replace np.nan with mean\
          \ values for selective columns\n    df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n\
          \n    # Convert 'Stall' column to numerical values\n    df['Stall'].replace({'Yes':\
          \ 1, 'No': 0}, inplace=True)\n\n    X = df[columns_to_convert].values\n\
          \    y = df['Stall'].values\n\n    # Apply SMOTE for balancing the dataset\n\
          \    # oversample = SMOTE(random_state=random_state)\n    oversample = SMOTE()\n\
          \    X, y = oversample.fit_resample(X, y)\n\n    # Standardize the features\n\
          \    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    #\
          \ Convert to torch tensors\n    X = torch.tensor(X, dtype=torch.float32)\n\
          \    y = torch.tensor(y, dtype=torch.float32)\n\n    # Split the dataset\
          \ into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=test_size, random_state=random_state)\n\n    X_train_path\
          \ = \"/tmp/X_train.pt\"\n    X_test_path = \"/tmp/X_test.pt\"\n    y_train_path\
          \ = \"/tmp/y_train.pt\"\n    y_test_path = \"/tmp/y_test.pt\"\n    torch.save(X_train,\
          \ X_train_path)\n    os.rename(X_train_path, X_train_artifact.path)\n\n\
          \    torch.save(X_test, X_test_path)\n    os.rename(X_test_path, X_test_artifact.path)\n\
          \n    torch.save(y_train, y_train_path)\n    os.rename(y_train_path, y_train_artifact.path)\n\
          \n    torch.save(y_test, y_test_path)\n    os.rename(y_test_path, y_test_artifact.path)\n\
          \n"
        image: python:3.11.9
    exec-mar-gen:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - mar_gen
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'gitpython'\
          \ 'kubernetes==30.1.0' 'Minio==7.2.5' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef mar_gen(\n    up_model_cond: Input[Artifact],\n    mar_gen_cond:\
          \ Output[Artifact],\n    mar_gen_info: Output[Metrics],\n    github_repo_url:\
          \ str,\n    github_cloned_dir: str,\n    github_main_branch: str,\n    github_username:\
          \ str,\n    github_token: str,\n    minio_url: str,\n    minio_access_key:\
          \ str,\n    minio_secret_key: str,\n    kserve_namespace: str,\n    bucket_name:\
          \ str,\n    k8s_api_token: str\n):\n    from kubernetes import client, config,\
          \ utils, watch\n    import time\n    from kubernetes.client import Configuration\n\
          \    from kubernetes.client.api import core_v1_api\n    from kubernetes.client.rest\
          \ import ApiException\n    from kubernetes.stream import stream\n    import\
          \ io\n    import tarfile\n    import pathlib\n    import os\n    from minio\
          \ import Minio\n    from minio.error import S3Error\n    from git import\
          \ Repo\n    from subprocess import run\n\n    # exec if a new model was\
          \ uploaded\n    with open(up_model_cond.path) as f:\n        up_model =\
          \ f.read()\n\n    if up_model == '1':\n        ## aux functions\n\n    \
          \    # clone repo to get custom files for model\n        def clone_repository_with_token(github_repo_url,\
          \ \n                                        github_cloned_dir, \n      \
          \                                  github_branch, \n                   \
          \                     github_username, \n                              \
          \          github_token):\n            \"\"\"Clone a Git repository using\
          \ a GitHub token in the URL and specifying the branch.\"\"\"\n         \
          \   try:\n                # Construct the URL with the GitHub username and\
          \ token\n                url_with_token = f\"https://{github_username}:{github_token}@{github_repo_url.split('//')[1]}\"\
          \n\n                # Clone the repository from the specified branch\n \
          \               repo = Repo.clone_from(url_with_token, github_cloned_dir,\
          \ branch=github_branch)\n                return \"Repository cloned successfully\"\
          \n            except Exception as e:\n                return f\"Error occurred\
          \ during repository cloning: {e}\"\n\n        # to wait some time while\
          \ pod isn't running yet\n        def wait_pod(core_v1, namespace, label,\
          \ pod_name, time_in_sec):\n            w = watch.Watch()\n            for\
          \ event in w.stream(func=core_v1.list_namespaced_pod,\n                \
          \                      namespace=namespace,\n                          \
          \            label_selector=label,\n                                   \
          \   timeout_seconds=time_in_sec):\n                if event[\"object\"].status.phase\
          \ == \"Running\":\n                    w.stop()\n                    end_time\
          \ = time.time()\n                    print(f\"{pod_name} running \")\n \
          \                   return\n                # event.type: ADDED, MODIFIED,\
          \ DELETED\n                if event[\"type\"] == \"DELETED\":\n        \
          \            # Pod was deleted while we were waiting for it to start.\n\
          \                    print(f\"{pod_name} deleted before it started\")\n\
          \                    w.stop()\n                    return\n\n        # kubectl\
          \ exec\n        def exec_commands(api_instance, namespace, pod_name, pod_container_name,\
          \ command):\n            name = pod_name\n            resp = None\n    \
          \        try:\n                resp = api_instance.read_namespaced_pod(name=name,\n\
          \                                                        namespace=namespace)\n\
          \            except ApiException as e:\n                if e.status != 404:\n\
          \                    print(f\"Unknown error: {e}\")\n                  \
          \  exit(1)\n\n            if not resp:\n                print(f\"Pod {name}\
          \ does not exist.\")\n\n            # Calling exec and waiting for response\n\
          \            exec_command = [\n                '/bin/sh',\n            \
          \    '-c',\n                command]\n            # When calling a pod with\
          \ multiple containers running the target container\n            # has to\
          \ be specified with a keyword argument container=<name>.\n            resp\
          \ = stream(api_instance.connect_get_namespaced_pod_exec,\n             \
          \     name=pod_name,\n                  container=pod_container_name,\n\
          \                  namespace=namespace,\n                  command=exec_command,\n\
          \                  stderr=True, stdin=False,\n                  stdout=True,\
          \ tty=False)\n            print(\"Response: \" + resp)\n\n        # Copy\
          \ from or to pod\n        def copy_to_tar(source_path, dest_path, tar):\n\
          \            \"\"\"\n            Adds a file or directory to a tar archive.\n\
          \n            Parameters:\n            - source_path: Path to the source\
          \ file or directory on the local machine.\n            - dest_path: Destination\
          \ directory inside the pod where files should be copied.\n            -\
          \ tar: The tarfile object to which files and directories will be added.\n\
          \            \"\"\"\n            source_path = pathlib.Path(source_path)\n\
          \n            if source_path.is_file():\n                # If it's a file,\
          \ add to the tarfile with the destination path\n                tar.add(source_path,\
          \ arcname=pathlib.Path(dest_path).joinpath(source_path.name))\n        \
          \    elif source_path.is_dir():\n                # If it's a directory,\
          \ recursively add all its content\n                for root, dirs, files\
          \ in os.walk(source_path):\n                    root_path = pathlib.Path(root)\n\
          \                    # Compute the relative path within the tar and add\
          \ to destination path\n                    for file in files:\n        \
          \                file_path = root_path / file\n                        tar.add(file_path,\
          \ arcname=pathlib.Path(dest_path).joinpath(file_path.relative_to(source_path)))\n\
          \n        def extract_tar_to_local(tar_stream, dest_path):\n           \
          \ \"\"\"\n            Extracts a tar archive stream to a local directory.\n\
          \n            Parameters:\n            - tar_stream: Tar stream from the\
          \ pod.\n            - dest_path: Local directory where the files will be\
          \ extracted.\n            \"\"\"\n            with tarfile.open(fileobj=tar_stream,\
          \ mode='r:') as tar:\n                tar.extractall(path=dest_path)\n\n\
          \        def copy_file_or_dir(api_instance, namespace, pod_name, pod_container_name,\
          \ source_path, dest_path, to_pod=True):\n            \"\"\"\n          \
          \  Copies a file or directory between a Kubernetes pod and the local machine.\n\
          \n            Parameters:\n            - api_instance: Kubernetes API client\
          \ instance.\n            - namespace: Namespace of the pod.\n          \
          \  - pod_name: Name of the pod.\n            - pod_container_name: Name\
          \ of the container within the pod.\n            - source_path: Path to the\
          \ source file or directory (local or in the pod).\n            - dest_path:\
          \ Destination directory (local or in the pod).\n            - to_pod: If\
          \ True, copy from local to pod; if False, copy from pod to local.\n    \
          \        \"\"\"\n            try:\n                if to_pod:\n        \
          \            # Copying from local to pod\n                    buf = io.BytesIO()\n\
          \                    with tarfile.open(fileobj=buf, mode='w:tar') as tar:\n\
          \                        copy_to_tar(source_path, dest_path, tar)\n\n  \
          \                  buf.seek(0)  # Reset buffer position after writing tar\n\
          \n                    exec_command = ['tar', 'xvf', '-', '-C', '/']\n  \
          \                  resp = stream(api_instance.connect_get_namespaced_pod_exec,\n\
          \                                  pod_name,\n                         \
          \         namespace,\n                                  container=pod_container_name,\n\
          \                                  command=exec_command,\n             \
          \                     stderr=True, stdin=True, stdout=True, tty=False,\n\
          \                                  _preload_content=False)\n\n         \
          \           # Send tar file to pod\n                    while resp.is_open():\n\
          \                        resp.update(timeout=1)\n                      \
          \  if resp.peek_stdout():\n                            print(f\"STDOUT:\
          \ {resp.read_stdout()}\")\n                        if resp.peek_stderr():\n\
          \                            print(f\"STDERR: {resp.read_stderr()}\")\n\
          \                        if buf.getvalue():\n                          \
          \  resp.write_stdin(buf.read())  # Write tar data to pod\n             \
          \           else:\n                            resp.write_stdin('\\n') \
          \ # Signal end of input\n                            break\n           \
          \         resp.close()\n\n                else:\n                    # Copying\
          \ from pod to local\n                    exec_command = ['tar', 'cvf', '-',\
          \ source_path]\n                    resp = stream(api_instance.connect_get_namespaced_pod_exec,\n\
          \                                  pod_name,\n                         \
          \         namespace,\n                                  container=pod_container_name,\n\
          \                                  command=exec_command,\n             \
          \                     stderr=True, stdin=False, stdout=True, tty=False,\n\
          \                                  _preload_content=False)\n\n         \
          \           tar_stream = io.BytesIO()\n                    while resp.is_open():\n\
          \                        resp.update(timeout=1)\n                      \
          \  if resp.peek_stdout():\n                            tar_stream.write(resp.read_stdout().encode('utf-8'))\
          \  # Write stdout (tar) to stream\n                        if resp.peek_stderr():\n\
          \                            print(f\"STDERR: {resp.read_stderr()}\")\n\n\
          \                    tar_stream.seek(0)  # Reset stream position for extraction\n\
          \                    extract_tar_to_local(tar_stream, dest_path)\n     \
          \               resp.close()\n\n            except Exception as e:\n   \
          \             print(f\"Error copying file or directory: {e}\")\n\n     \
          \   # init minio client\n        def minio_setup(minio_url, minio_access_key,\
          \ minio_secret_key):\n            # Initialize Minio client with just the\
          \ base URL (without path)\n            client = Minio(\n               \
          \ minio_url,  # Ensure minio_url does not include a path, only the base\
          \ URL (e.g., http://localhost:9000)\n                access_key=minio_access_key,\n\
          \                secret_key=minio_secret_key,\n                secure=False\
          \  # Minio is using HTTP on localhost:9000\n            )\n            return\
          \ client\n\n        # upload file to minio using a client\n        def upload_file(client,\
          \ bucket_name, object_name, filepath):\n            # Create the bucket\
          \ if it does not exist\n            if not client.bucket_exists(bucket_name):\n\
          \                client.make_bucket(bucket_name)\n                create_bucket_result\
          \ = f\"Successfully created bucket: {bucket_name}\"\n            else:\n\
          \                create_bucket_result = f\"Bucket {bucket_name} already\
          \ exists\"\n\n            try:\n                # Upload the file to the\
          \ specified path in the bucket\n                client.fput_object(bucket_name,\
          \ object_name, filepath)\n                return (f'Successfully uploaded\
          \ {filepath} to {bucket_name}/{object_name}')\n\n            except Exception\
          \ as e:\n                # Log and raise any upload errors\n           \
          \     raise Exception(f'Failed to upload model to Minio: {e}')\n\n     \
          \   # clone the repo\n        clone_result = clone_repository_with_token(github_repo_url,\
          \ \n                                                   github_cloned_dir,\
          \ \n                                                   github_main_branch,\
          \ \n                                                   github_username,\
          \ \n                                                   github_token)\n\n\
          \        ## init access to cluster when outside or inside the cluster\n\
          \        # config.load_kube_config(\"kubeconfig\")\n        config.load_incluster_config()\n\
          \        k8s_client = client.ApiClient()\n\n        # apply model-store\
          \ manifests \n        model_store_yaml_dir = f'{github_cloned_dir}/model-archiver/model-store-manifests/'\n\
          \        try:\n            pv_result = utils.create_from_directory(k8s_client,\
          \ model_store_yaml_dir, verbose=True)\n        except Exception as e:\n\
          \            print(e)\n\n        # init config to exec commands in pods\n\
          \        try:\n            c = Configuration().get_default_copy()\n    \
          \    except AttributeError:\n            c = Configuration()\n         \
          \   c.assert_hostname = False\n        Configuration.set_default(c)\n  \
          \      core_v1 = core_v1_api.CoreV1Api()\n\n        # Create folders for\
          \ model-store, config and scripts in PV\n        model_store_pod_name =\
          \ \"model-store-pod\"\n        model_store_pod_container_name = \"model-store\"\
          \n        model_store_pod_label = \"service.istio.io/canonical-name=model-store-pod\"\
          \n\n        # Wait for pods to run before exec\n        wait_pod(core_v1,\
          \ kserve_namespace, model_store_pod_label, model_store_pod_name, 120)\n\n\
          \        mkdir_ms_command = \"mkdir -p /pv/model-store/youtubegoes5g/\"\n\
          \        mkdir_ms_result = exec_commands(core_v1, \n                   \
          \                     kserve_namespace, \n                             \
          \           model_store_pod_name, \n                                   \
          \     model_store_pod_container_name, \n                               \
          \         mkdir_ms_command)\n        mkdir_conf_command = \"mkdir /pv/config/\"\
          \n        mkdir_conf_result = exec_commands(core_v1, \n                \
          \                          kserve_namespace, \n                        \
          \                  model_store_pod_name, \n                            \
          \              model_store_pod_container_name, \n                      \
          \                    mkdir_conf_command)\n        mkdir_scripts_command\
          \ = \"mkdir /pv/scripts/\"\n        mkdir_scripts_result = exec_commands(core_v1,\
          \ \n                                             kserve_namespace, \n  \
          \                                           model_store_pod_name, \n   \
          \                                          model_store_pod_container_name,\
          \ \n                                             mkdir_scripts_command)\n\
          \n        # download pt file from minio\n        pt_object_name = 'model-store/youtubegoes5g/model.pt'\n\
          \        pt_local_file_path = 'model.pt'\n\n        # Initialize the MinIO\
          \ client\n        client = minio_setup(minio_url, minio_access_key, minio_secret_key)\n\
          \n        try:\n            # Download the file from MinIO\n           \
          \ client.fget_object(bucket_name, pt_object_name, pt_local_file_path)\n\
          \            print(f\"File {pt_object_name} downloaded successfully to {pt_local_file_path}.\"\
          )\n        except S3Error as exc:\n            print(f\"Error occurred:\
          \ {exc}\")\n\n        # cp pt file to model store pod\n        model_pt_source_path\
          \ = 'model.pt'\n        model_store_dest_path = \"/pv/model-store/youtubegoes5g/\"\
          \n\n        cp_pt_result = copy_file_or_dir(core_v1, \n                \
          \                        kserve_namespace, \n                          \
          \              model_store_pod_name, \n                                \
          \        model_store_pod_container_name, \n                            \
          \            model_pt_source_path, \n                                  \
          \      model_store_dest_path, \n                                       \
          \ to_pod=True)\n\n        # cp handler file to model store pod\n       \
          \ model_handler_source_path = f'{github_cloned_dir}/model-archiver/model-store/youtubegoes5g/custom_handler.py'\n\
          \n        cp_handler_result = copy_file_or_dir(core_v1, \n             \
          \                                kserve_namespace, \n                  \
          \                           model_store_pod_name, \n                   \
          \                          model_store_pod_container_name, \n          \
          \                                   model_handler_source_path, \n      \
          \                                       model_store_dest_path, \n      \
          \                                       to_pod=True)\n\n        # cp model.py\
          \ file to model store pod\n        model_py_source_path =  f'{github_cloned_dir}/model-archiver/model-store/youtubegoes5g/model.py'\n\
          \n        cp_py_result = copy_file_or_dir(core_v1, \n                  \
          \                      kserve_namespace, \n                            \
          \            model_store_pod_name, \n                                  \
          \      model_store_pod_container_name, \n                              \
          \          model_py_source_path, \n                                    \
          \    model_store_dest_path, \n                                        to_pod=True)\n\
          \n        # cp properties.json file to model store pod\n        prop_source_path\
          \ = f'{github_cloned_dir}/model-archiver/model-store/properties.json'\n\
          \        model_prop_dest_path = \"/pv/model-store/\"\n\n        cp_prop_result\
          \ = copy_file_or_dir(core_v1, \n                                       \
          \   kserve_namespace, \n                                          model_store_pod_name,\
          \ \n                                          model_store_pod_container_name,\
          \ \n                                          prop_source_path, \n     \
          \                                     model_prop_dest_path, \n         \
          \                                 to_pod=True)\n\n        # cp config.properties\
          \ file to model store pod\n        config_source_path = f'{github_cloned_dir}/model-archiver/config/config.properties'\n\
          \    #         config_dest_path = \"/pv/config/\"\n\n    #         cp_conf_result\
          \ = copy_file_or_dir(core_v1, \n    #                                  \
          \         kserve_namespace, \n    #                                    \
          \       model_store_pod_name, \n    #                                  \
          \         model_store_pod_container_name, \n    #                      \
          \                     config_source_path, \n    #                      \
          \                     config_dest_path, \n    #                        \
          \                   to_pod=True)\n\n        # cp margen script file to model\
          \ store pod\n        scripts_source_path = f'{github_cloned_dir}/model-archiver/scripts/margen.sh'\n\
          \        scripts_dest_path = \"/pv/scripts/\"\n\n        cp_scripts_result\
          \ = copy_file_or_dir(core_v1, \n                                       \
          \   kserve_namespace, \n                                          model_store_pod_name,\
          \ \n                                          model_store_pod_container_name,\
          \ \n                                          scripts_source_path, \n  \
          \                                        scripts_dest_path, \n         \
          \                                 to_pod=True)\n\n        # Delete model_store_pod\n\
          \        try:\n            api_response = core_v1.delete_namespaced_pod(model_store_pod_name,\
          \ kserve_namespace)\n            print(api_response)\n        except ApiException\
          \ as e:\n            print(\"Exception when calling CoreV1Api->delete_namespaced_pod:\
          \ %s\\n\" % e)\n\n        # Upload config.properties to minio for IS\n \
          \       config_object_name = \"config/config.properties\"\n        up_config_result\
          \ = upload_file(client, bucket_name, config_object_name, config_source_path)\n\
          \n        # Create model archiver pod\n        mar_yaml_dir = f'{github_cloned_dir}/model-archiver/manifests/'\n\
          \        mar_pod_name = \"margen-pod\"\n        mar_pod_container_name =\
          \ \"margen-container\"\n        mar_pod_label = \"service.istio.io/canonical-name=margen-pod\"\
          \n\n        try:\n            margen_result = utils.create_from_directory(k8s_client,\
          \ mar_yaml_dir, verbose=True)\n        except Exception as e:\n        \
          \    print(e)\n\n        # Wait for pods to run before exec\n        wait_pod(core_v1,\
          \ kserve_namespace, mar_pod_label, mar_pod_name, 120)\n\n        # Exec\
          \ mar gen in a script\n        mar_gen_command = \"bash scripts/margen.sh\"\
          \n        mar_gen_result = exec_commands(core_v1, \n                   \
          \                    kserve_namespace, \n                              \
          \         mar_pod_name, \n                                       mar_pod_container_name,\
          \ \n                                       mar_gen_command)\n\n        #\
          \ Copy mar file to local\n        mar_source_path = \"youtubegoes5g.mar\"\
          \n        mar_dest_path = \"./\"\n\n        mar_cp_result = copy_file_or_dir(core_v1,\
          \ \n                                         kserve_namespace, \n      \
          \                                   mar_pod_name, \n                   \
          \                      mar_pod_container_name, \n                      \
          \                   mar_source_path, \n                                \
          \         mar_dest_path, \n                                         to_pod=False)\n\
          \n        # Upload mar file to minio\n        mar_object_name = \"model-store/youtubegoes5g.mar\"\
          \n        mar_filepath = './youtubegoes5g.mar'\n\n        up_mar_result\
          \ = upload_file(client, bucket_name, mar_object_name, mar_filepath)\n\n\
          \        # Delete margen pod\n        try:\n            api_response = core_v1.delete_namespaced_pod(mar_pod_name,\
          \ kserve_namespace)\n            print(api_response)\n        except ApiException\
          \ as e:\n            print(\"Exception when calling CoreV1Api->delete_namespaced_pod:\
          \ %s\\n\" % e)\n\n        with open(mar_gen_cond.path, 'w') as f:\n    \
          \        f.write('1')\n        mar_gen_info.log_metric(\"mar file\", \"\
          created\")\n    else:\n        with open(mar_gen_cond.path, 'w') as f:\n\
          \            f.write('0')\n        mar_gen_info.log_metric(\"mar file\"\
          , \"not created\")\n\n"
        image: python:3.11.9
    exec-model-serving:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_serving
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kserve==0.13.0'\
          \ 'kubernetes==30.1.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_serving(\n    mar_gen_cond: Input[Artifact],\n    cond_info:\
          \ Output[Metrics],\n    bucket_name: str,\n    model_name: str,\n    kserve_namespace:\
          \ str,\n    kserve_svc_acc: str\n):\n    # Create kserve instance\n    from\
          \ kubernetes import client \n    from kserve import KServeClient, constants,\
          \ V1beta1InferenceService, V1beta1InferenceServiceSpec, V1beta1PredictorSpec,\
          \ V1beta1TorchServeSpec\n    from datetime import datetime\n    import time\n\
          \n    # exec if a new model was uploaded\n    with open(mar_gen_cond.path)\
          \ as f:\n        up_model = f.read()\n\n    if up_model == '1':\n      \
          \  cond_info.log_metric(\"Inference Service\", \"Created/Updated\")\n\n\
          \        #Inference server config\n        now = datetime.now()\n      \
          \  kserve_version='v1beta1'\n        api_version = constants.KSERVE_GROUP\
          \ + '/' + kserve_version\n\n        # with open(model_uri.path) as f:\n\
          \        #     uri = f.read()\n        uri = f's3://{bucket_name}'\n\n \
          \       isvc = V1beta1InferenceService(api_version=api_version,\n      \
          \                                 kind=constants.KSERVE_KIND,\n        \
          \                               metadata=client.V1ObjectMeta(\n        \
          \                                   name=model_name, namespace=kserve_namespace,\
          \ annotations={'sidecar.istio.io/inject':'false'}),\n                  \
          \                     spec=V1beta1InferenceServiceSpec(\n              \
          \                         predictor=V1beta1PredictorSpec(\n            \
          \                               service_account_name=kserve_svc_acc,\n \
          \                                          pytorch=(V1beta1TorchServeSpec(\n\
          \                                               storage_uri=uri))))\n  \
          \      )\n\n        KServe = KServeClient()\n\n        #replace old inference\
          \ service with a new one\n        try:\n            KServe.delete(name=model_name,\
          \ namespace=kserve_namespace)\n            print(\"Old model deleted\")\n\
          \        except:\n            print(\"Couldn't delete old model\")\n   \
          \     time.sleep(10)\n\n        KServe.create(isvc)\n    else:\n       \
          \ cond_info.log_metric(\"Inference Service\", \"Not Created/Updated\")\n\
          \n"
        image: python:3.11.9
    exec-model-training:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_training
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'torch==2.3.0'\
          \ 'scikit-learn==1.2.2' 'numpy==1.25.2' 'Minio==7.2.5' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_training(\n    X_train_artifact: Input[Dataset], \n   \
          \ X_test_artifact: Input[Dataset],\n    y_train_artifact: Input[Dataset],\n\
          \    y_test_artifact: Input[Dataset],\n    metrics: Output[Metrics], \n\
          \    up_model_info: Output[Metrics],\n    classification_metrics: Output[ClassificationMetrics],\
          \ \n    model_trained_artifact: Output[Model],\n    up_model_cond: Output[Artifact],\n\
          \    model_lr: float,\n    model_epochs: int,\n    model_print_frequency_per_n_epochs:\
          \ int,\n    minio_url: str,\n    minio_access_key: str,\n    minio_secret_key:\
          \ str,\n    bucket_name: str,\n    minio_model_object_name: str,\n    trigger_type:\
          \ str,\n    performance_factor: float,\n    last_accuracy_object_name: str,\n\
          \    tmp_dir: str,\n    tmp_file_last_acc: str\n    ):\n    import os\n\
          \    import torch\n    from torch import nn\n    from sklearn.metrics import\
          \ accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\
          \    from minio import Minio\n\n    # Build model with non-linear activation\
          \ function\n    class InterruptionModel(nn.Module):\n        def __init__(self):\n\
          \            super().__init__()\n            self.layer_1 = nn.Linear(in_features=29,\
          \ out_features=200)\n            self.layer_2 = nn.Linear(in_features=200,\
          \ out_features=100)\n            self.layer_3 = nn.Linear(in_features=100,\
          \ out_features=1)\n            self.relu = nn.ReLU() # <- add in ReLU activation\
          \ function\n            # Can also put sigmoid in the model\n          \
          \  # This would mean you don't need to use it on the predictions\n     \
          \       # self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n\
          \            # Intersperse the ReLU activation function between layers\n\
          \            return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\
          \n    # Helper functions\n    def accuracy_fn(y_true, y_pred):\n       \
          \ correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates\
          \ where two tensors are equal\n        acc = (correct / len(y_pred)) * 100\n\
          \        return acc\n\n    def minio_setup(minio_url, minio_access_key,\
          \ minio_secret_key):\n        # Initialize Minio client with just the base\
          \ URL (without path)\n        client = Minio(\n            minio_url,  #\
          \ Ensure minio_url does not include a path, only the base URL (e.g., http://localhost:9000)\n\
          \            access_key=minio_access_key,\n            secret_key=minio_secret_key,\n\
          \            secure=False  # Minio is using HTTP on localhost:9000\n   \
          \     )\n        return client\n\n    def upload_file(client, bucket_name,\
          \ object_name, filepath):\n        # Create the bucket if it does not exist\n\
          \        if not client.bucket_exists(bucket_name):\n            client.make_bucket(bucket_name)\n\
          \            create_bucket_result = f\"Successfully created bucket: {bucket_name}\"\
          \n        else:\n            create_bucket_result = f\"Bucket {bucket_name}\
          \ already exists\"\n\n        try:\n            # Upload the file to the\
          \ specified path in the bucket\n            client.fput_object(bucket_name,\
          \ object_name, filepath)\n            return (f'Successfully uploaded {filepath}\
          \ to {bucket_name}/{object_name}')\n\n        except Exception as e:\n \
          \           # Log and raise any upload errors\n            raise Exception(f'Failed\
          \ to upload model to Minio: {e}')\n\n    def read_from_minio(client, bucket_name,\
          \ object_name):\n        \"\"\"\n        Function to read a file from a\
          \ MinIO bucket and convert its single content to a float.\n        If the\
          \ file is not found or is empty, it returns 0.0.\n\n        Args:\n    \
          \        client: minio client\n            bucket_name (str): The name of\
          \ the bucket in MinIO.\n            object_name (str): The name of the object\
          \ (file) in the bucket.\n\n        Returns:\n            float: The float\
          \ value converted from the file content, or 0.0 if the file is not found\
          \ or empty.\n        \"\"\"\n        try:\n            # Get the file from\
          \ the MinIO bucket\n            response = client.get_object(bucket_name,\
          \ object_name)\n\n            # Read the file content into a buffer\n  \
          \          file_data = response.read()\n\n            # Decode file content\
          \ and strip whitespace\n            content = file_data.decode('utf-8').strip()\n\
          \n            # If the content is empty, return 0.0\n            if not\
          \ content:\n                print(f\"File {object_name} is empty.\")\n \
          \               return 0.0\n\n            # Convert the content to a float\n\
          \            float_value = float(content)\n            return float_value\n\
          \n        except Exception as e:\n            # Handle file not found or\
          \ any other errors\n            print(f\"Error occurred: {e}\")\n      \
          \      return 0.0\n\n    def save_float_to_tempfile(float_value, dir_name,\
          \ file_name):\n        \"\"\"\n        Saves a float value to a specified\
          \ directory and file name.\n\n        Args:\n            float_value (float):\
          \ The float value to save.\n            dir_name (str): The name of the\
          \ directory to save the file in.\n            file_name (str): The name\
          \ of the file.\n\n        Returns:\n            str: The path to the file.\n\
          \        \"\"\"\n        # Ensure the directory exists\n        os.makedirs(dir_name,\
          \ exist_ok=True)\n        temp_file_path = os.path.join(dir_name, file_name)\n\
          \n        with open(temp_file_path, 'w') as temp_file:\n            # Convert\
          \ the float to a string, then write to file\n            temp_file.write(str(float_value))\n\
          \n        return temp_file_path\n\n    def get_accuracy_in_last_run(client,\
          \ bucket_name, object_name):\n        accuracy_in_last_run = read_from_minio(client,\
          \ bucket_name, object_name)\n        return accuracy_in_last_run\n\n   \
          \ def update_accuracy_in_last_run(client, bucket_name, object_name, new_value,\
          \ tmp_dir, tmp_file):\n        filepath = save_float_to_tempfile(new_value,\
          \ tmp_dir, tmp_file)\n        upload_file(client, bucket_name, object_name,\
          \ filepath)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"\
          cpu\"\n    model = InterruptionModel().to(device)\n\n    # Setup loss and\
          \ optimizer\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),\
          \ lr=model_lr)\n\n    # Fit the model\n    torch.manual_seed(42)\n\n   \
          \ # Put all data on target device\n    X_train = torch.load(X_train_artifact.path)\n\
          \    X_test = torch.load(X_test_artifact.path)\n    y_train = torch.load(y_train_artifact.path)\n\
          \    y_test = torch.load(y_test_artifact.path)\n    X_train, y_train = X_train.to(device),\
          \ y_train.to(device)\n    X_test, y_test = X_test.to(device), y_test.to(device)\n\
          \n    for epoch in range(model_epochs):\n        # 1. Forward pass\n   \
          \     y_logits = model(X_train).squeeze()\n\n        y_pred = torch.round(torch.sigmoid(y_logits))\
          \ # logits -> prediction probabilities -> prediction labels\n\n        #\
          \ 2. Calculate loss and accuracy\n        loss = loss_fn(y_logits, y_train)\
          \ # BCEWithLogitsLoss calculates loss using logits\n        acc = accuracy_fn(y_true=y_train,\n\
          \                        y_pred=y_pred)\n\n        # 3. Optimizer zero grad\n\
          \        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\
          \n        # 5. Optimizer step\n        optimizer.step()\n\n        ### Testing\n\
          \        model.eval()\n        with torch.no_grad():\n        # 1. Forward\
          \ pass\n            test_logits = model(X_test).squeeze()\n            #print(test_logits.shape)\n\
          \            test_pred = torch.round(torch.sigmoid(test_logits)) # logits\
          \ -> prediction probabilities -> prediction labels\n            # 2. Calcuate\
          \ loss and accuracy\n            test_loss = loss_fn(test_logits, y_test)\n\
          \            test_acc = accuracy_fn(y_true=y_test,\n                   \
          \             y_pred=test_pred)\n\n\n        # Print out what's happening\n\
          \        if epoch % model_print_frequency_per_n_epochs == 0:\n         \
          \   print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test\
          \ Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n\n    model.eval()\n\
          \    with torch.no_grad():\n        y_preds = torch.round(torch.sigmoid(model(X_test))).squeeze()\n\
          \n    if device == \"cuda\":\n        predictions = y_preds.cpu().numpy()\
          \ #if it is cuda, then this, otherwise y_pred.numpy()\n        true_labels\
          \ = y_test.cpu().numpy()\n    else:\n        predictions = y_preds.numpy()\n\
          \        true_labels = y_test.numpy()\n\n    # Confusion Matrix\n    cmatrix\
          \ = confusion_matrix(true_labels, predictions)\n    #print(\"Confusion Matrix:\"\
          , cmatrix)\n\n    # Metrics\n    accuracy = accuracy_score(true_labels,\
          \ predictions)\n    metrics.log_metric(\"Accuracy\", accuracy)\n    #print('Accuracy:\
          \ %f' % accuracy)\n\n    # test accuracy\n    # print(type(accuracy))\n\n\
          \    precision = precision_score(true_labels,  predictions, average='weighted')\n\
          \    metrics.log_metric(\"Precision\", precision)\n    #print('Precision:\
          \ %f' % precision)\n\n    recall = recall_score(true_labels, predictions,\
          \ average='weighted')\n    metrics.log_metric(\"Recall\", recall)\n    #print('Recall:\
          \ %f' % recall)\n\n    microf1 = f1_score(true_labels, predictions, average='micro')\n\
          \    metrics.log_metric(\"Micro F1 score\", microf1)\n    #print('Micro\
          \ F1 score: %f' % microf1)\n\n    macrof1 = f1_score(true_labels, predictions,\
          \ average='macro')\n    metrics.log_metric(\"Macro F1 score\", macrof1)\n\
          \    #print('Macro F1 score: %f' % macrof1)\n\n    target_names = ['No-Stall',\
          \ 'Stall']\n    # Print precision-recall report\n    #print(classification_report(true_labels,\
          \ predictions, target_names=target_names))\n\n    # Classification Metrics\
          \ artifact\n    cmatrix = cmatrix.tolist()\n    target_names = ['No-Stall',\
          \ 'Stall']\n    classification_metrics.log_confusion_matrix(target_names,\
          \ cmatrix)\n\n    # Save model\n    model_path = \"/tmp/model.pt\"\n   \
          \ torch.save(model.state_dict(), model_path)\n    os.rename(model_path,\
          \ model_trained_artifact.path)\n\n    # Setup minio client to upload and\
          \ read files\n    client = minio_setup(minio_url, minio_access_key, minio_secret_key)\n\
          \n    previous_accuracy = get_accuracy_in_last_run(client, bucket_name,\
          \ last_accuracy_object_name)\n\n    metrics.log_metric(\"Current-Previous\
          \ accuracy\", accuracy-previous_accuracy)\n    metrics.log_metric(\"Performance\
          \ factor\", performance_factor)\n\n    up_model = False\n\n    if trigger_type\
          \ == '1' or trigger_type == '2':\n        up_model = True\n    elif trigger_type\
          \ == '3':\n        if accuracy - previous_accuracy > performance_factor:\n\
          \            up_model = True\n            update_accuracy_in_last_run(client,\
          \ bucket_name, last_accuracy_object_name, accuracy, tmp_dir, tmp_file_last_acc)\n\
          \n    if up_model:\n        up_model_info.log_metric(\"up model\", \"Yes\"\
          )\n        with open(up_model_cond.path, 'w') as f:\n            f.write('1')\n\
          \        upload_model_result = upload_file(client, bucket_name, minio_model_object_name,\
          \ model_trained_artifact.path)\n    else:\n        up_model_info.log_metric(\"\
          up model\", \"No\")\n        with open(up_model_cond.path, 'w') as f:\n\
          \            f.write('0')\n\n"
        image: python:3.11.9
pipelineInfo:
  name: my-pipeline
root:
  dag:
    outputs:
      artifacts:
        data-ingestion-dataset_info:
          artifactSelectors:
          - outputArtifactKey: dataset_info
            producerSubtask: data-ingestion
        mar-gen-mar_gen_info:
          artifactSelectors:
          - outputArtifactKey: mar_gen_info
            producerSubtask: mar-gen
        model-serving-cond_info:
          artifactSelectors:
          - outputArtifactKey: cond_info
            producerSubtask: model-serving
        model-training-classification_metrics:
          artifactSelectors:
          - outputArtifactKey: classification_metrics
            producerSubtask: model-training
        model-training-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: model-training
        model-training-up_model_info:
          artifactSelectors:
          - outputArtifactKey: up_model_info
            producerSubtask: model-training
    tasks:
      data-ingestion:
        cachingOptions: {}
        componentRef:
          name: comp-data-ingestion
        inputs:
          parameters:
            dvc_file_dir:
              componentInputParameter: dvc_file_dir
            dvc_file_name:
              componentInputParameter: dvc_file_name
            dvc_remote_db_url:
              componentInputParameter: dvc_remote_db_url
            dvc_remote_name:
              componentInputParameter: dvc_remote_name
            github_cloned_dir:
              componentInputParameter: github_cloned_dir
            github_dvc_branch:
              componentInputParameter: github_dvc_branch
            github_repo_url:
              componentInputParameter: github_repo_url
            github_token:
              componentInputParameter: github_token
            github_username:
              componentInputParameter: github_username
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_secret_key:
              componentInputParameter: minio_secret_key
            minio_url:
              componentInputParameter: minio_url
        taskInfo:
          name: data-ingestion
      data-preparation:
        cachingOptions: {}
        componentRef:
          name: comp-data-preparation
        dependentTasks:
        - data-ingestion
        inputs:
          artifacts:
            dataset_artifact:
              taskOutputArtifact:
                outputArtifactKey: dataset_artifact
                producerTask: data-ingestion
        taskInfo:
          name: data-preparation
      mar-gen:
        cachingOptions: {}
        componentRef:
          name: comp-mar-gen
        dependentTasks:
        - model-training
        inputs:
          artifacts:
            up_model_cond:
              taskOutputArtifact:
                outputArtifactKey: up_model_cond
                producerTask: model-training
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            github_cloned_dir:
              componentInputParameter: github_cloned_dir
            github_main_branch:
              componentInputParameter: github_main_branch
            github_repo_url:
              componentInputParameter: github_repo_url
            github_token:
              componentInputParameter: github_token
            github_username:
              componentInputParameter: github_username
            k8s_api_token:
              componentInputParameter: k8s_api_token
            kserve_namespace:
              componentInputParameter: kserve_namespace
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_secret_key:
              componentInputParameter: minio_secret_key
            minio_url:
              componentInputParameter: minio_url
        taskInfo:
          name: mar-gen
      model-serving:
        cachingOptions: {}
        componentRef:
          name: comp-model-serving
        dependentTasks:
        - mar-gen
        inputs:
          artifacts:
            mar_gen_cond:
              taskOutputArtifact:
                outputArtifactKey: mar_gen_cond
                producerTask: mar-gen
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            kserve_namespace:
              componentInputParameter: kserve_namespace
            kserve_svc_acc:
              componentInputParameter: kserve_svc_acc
            model_name:
              componentInputParameter: model_name
        taskInfo:
          name: model-serving
      model-training:
        cachingOptions: {}
        componentRef:
          name: comp-model-training
        dependentTasks:
        - data-preparation
        inputs:
          artifacts:
            X_test_artifact:
              taskOutputArtifact:
                outputArtifactKey: X_test_artifact
                producerTask: data-preparation
            X_train_artifact:
              taskOutputArtifact:
                outputArtifactKey: X_train_artifact
                producerTask: data-preparation
            y_test_artifact:
              taskOutputArtifact:
                outputArtifactKey: y_test_artifact
                producerTask: data-preparation
            y_train_artifact:
              taskOutputArtifact:
                outputArtifactKey: y_train_artifact
                producerTask: data-preparation
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            last_accuracy_object_name:
              componentInputParameter: last_accuracy_object_name
            minio_access_key:
              componentInputParameter: minio_access_key
            minio_model_object_name:
              componentInputParameter: minio_model_object_name
            minio_secret_key:
              componentInputParameter: minio_secret_key
            minio_url:
              componentInputParameter: minio_url
            model_epochs:
              componentInputParameter: model_epochs
            model_lr:
              componentInputParameter: model_lr
            model_print_frequency_per_n_epochs:
              componentInputParameter: model_print_frequency_per_n_epochs
            performance_factor:
              componentInputParameter: performance_factor
            tmp_dir:
              componentInputParameter: tmp_dir
            tmp_file_last_acc:
              componentInputParameter: tmp_file_last_acc
            trigger_type:
              componentInputParameter: trigger_type
        taskInfo:
          name: model-training
  inputDefinitions:
    parameters:
      bucket_name:
        parameterType: STRING
      dvc_file_dir:
        parameterType: STRING
      dvc_file_name:
        parameterType: STRING
      dvc_remote_db_url:
        parameterType: STRING
      dvc_remote_name:
        parameterType: STRING
      github_cloned_dir:
        parameterType: STRING
      github_dvc_branch:
        parameterType: STRING
      github_main_branch:
        parameterType: STRING
      github_repo_url:
        parameterType: STRING
      github_token:
        parameterType: STRING
      github_username:
        parameterType: STRING
      k8s_api_token:
        parameterType: STRING
      kserve_namespace:
        parameterType: STRING
      kserve_svc_acc:
        parameterType: STRING
      last_accuracy_object_name:
        parameterType: STRING
      minio_access_key:
        parameterType: STRING
      minio_model_object_name:
        parameterType: STRING
      minio_secret_key:
        parameterType: STRING
      minio_url:
        parameterType: STRING
      model_epochs:
        parameterType: NUMBER_INTEGER
      model_lr:
        parameterType: NUMBER_DOUBLE
      model_name:
        parameterType: STRING
      model_print_frequency_per_n_epochs:
        parameterType: NUMBER_INTEGER
      performance_factor:
        parameterType: NUMBER_DOUBLE
      tmp_dir:
        parameterType: STRING
      tmp_file_last_acc:
        parameterType: STRING
      trigger_type:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      data-ingestion-dataset_info:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      mar-gen-mar_gen_info:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-serving-cond_info:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-training-classification_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      model-training-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-training-up_model_info:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
