# PIPELINE DEFINITION
# Name: my-pipeline
# Inputs:
#    access_key: str
#    branch_name: str
#    bucket_name: str
#    cloned_dir: str
#    dvc_file_dir: str
#    dvc_file_name: str
#    epochs: int
#    github_token: str
#    github_username: str
#    lr: float
#    minio_url: str
#    model_name: str
#    model_uri: str
#    namespace: str
#    object_name: str
#    print_frequency: int
#    remote_name: str
#    remote_url: str
#    repo_url: str
#    secret_key: str
# Outputs:
#    model-training-classification_metrics: system.ClassificationMetrics
#    model-training-metrics: system.Metrics
components:
  comp-data-ingestion:
    executorLabel: exec-data-ingestion
    inputDefinitions:
      parameters:
        access_key:
          parameterType: STRING
        branch_name:
          parameterType: STRING
        cloned_dir:
          parameterType: STRING
        dvc_file_dir:
          parameterType: STRING
        dvc_file_name:
          parameterType: STRING
        github_token:
          parameterType: STRING
        github_username:
          parameterType: STRING
        minio_url:
          parameterType: STRING
        remote_name:
          parameterType: STRING
        remote_url:
          parameterType: STRING
        repo_url:
          parameterType: STRING
        secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-data-preparation:
    executorLabel: exec-data-preparation
    inputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-model-training:
    executorLabel: exec-model-training
    inputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        access_key:
          parameterType: STRING
        bucket_name:
          parameterType: STRING
        epochs:
          parameterType: NUMBER_INTEGER
        lr:
          parameterType: NUMBER_DOUBLE
        minio_url:
          parameterType: STRING
        object_name:
          parameterType: STRING
        print_frequency:
          parameterType: NUMBER_INTEGER
        secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        classification_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model_trained_artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_trained_artifact_path:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-data-ingestion:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_ingestion
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'gitpython'\
          \ 'dvc==3.54.1' 'dvc-s3==3.2.0' 'numpy==1.25.2' 'pandas==2.0.3' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_ingestion(\n    repo_url: str,\n    cloned_dir: str,\n \
          \   branch_name: str,\n    github_username: str,\n    github_token: str,\n\
          \    remote_name: str,\n    remote_url: str,\n    minio_url: str,\n    access_key:\
          \ str,\n    secret_key: str,\n    dvc_file_dir: str,\n    dvc_file_name:\
          \ str,\n    dataset_artifact: Output[Dataset]\n    ):\n    from git import\
          \ Repo\n    from subprocess import run, CalledProcessError\n    import os\n\
          \    import pandas as pd\n\n    def clone_repository_with_token(repo_url,\
          \ cloned_dir, branch_name, github_username, github_token):\n        \"\"\
          \"Clone a Git repository using a GitHub token in the URL and specifying\
          \ the branch.\"\"\"\n        try:\n            # Construct the URL with\
          \ the GitHub username and token\n            url_with_token = f\"https://{github_username}:{github_token}@{repo_url.split('//')[1]}\"\
          \n\n            # Clone the repository from the specified branch\n     \
          \       repo = Repo.clone_from(url_with_token, cloned_dir, branch=branch_name)\n\
          \            return \"Repository cloned successfully\"\n        except Exception\
          \ as e:\n            return f\"Error occurred during repository cloning:\
          \ {e}\"\n\n    def configure_dvc_remote(cloned_dir, remote_name, remote_url,\
          \ minio_url, access_key, secret_key):\n        \"\"\"Configure the Minio\
          \ bucket as the DVC remote repository using the `dvc remote` commands.\"\
          \"\"\n        try:\n            # Add the remote\n            run(\n   \
          \             ['dvc', 'remote', 'add', '-d', remote_name, remote_url],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure the endpoint URL\n            run(\n         \
          \       ['dvc', 'remote', 'modify', remote_name, 'endpointurl', minio_url],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure access key ID\n            run(\n            \
          \    ['dvc', 'remote', 'modify', remote_name, 'access_key_id', access_key],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            # Configure secret access key\n            run(\n        \
          \        ['dvc', 'remote', 'modify', remote_name, 'secret_access_key', secret_key],\n\
          \                cwd=cloned_dir,\n                capture_output=True,\n\
          \                text=True,\n                check=True\n            )\n\
          \n            return f'Successfully configured Minio bucket as DVC remote\
          \ repository: {remote_name}'\n        except CalledProcessError as e:\n\
          \            # Log and raise any errors\n            return f'Failed to\
          \ configure DVC remote: {e.stderr}'\n\n    def perform_dvc_pull(cloned_dir,\
          \ remote_name):\n        \"\"\"Perform a DVC pull to synchronize local data\
          \ with the remote repository.\"\"\"\n        try:\n            # Run the\
          \ `dvc pull` command\n            result = run(['dvc', 'pull', '-r', remote_name],\
          \ cwd=cloned_dir, capture_output=True, text=True)\n\n            # Check\
          \ if the command executed successfully\n            if result.returncode\
          \ != 0:\n                # Log and raise an error if the command failed\n\
          \                error_message = f\"dvc pull failed with error: {result.stderr}\"\
          \n                raise Exception(error_message)\n\n            # Log successful\
          \ operation\n            return \"Successfully pulled data from remote DVC\
          \ repository\"\n\n        except Exception as e:\n            # Log and\
          \ handle the error\n            return f\"Error occurred during dvc pull:\
          \ {e}\"\n\n    # Call the functions\n    clone_result = clone_repository_with_token(repo_url,\
          \ cloned_dir, branch_name, github_username, github_token)\n    configure_result\
          \ = configure_dvc_remote(cloned_dir, remote_name, remote_url, minio_url,\
          \ access_key, secret_key)\n    dvc_pull_result = perform_dvc_pull(cloned_dir,\
          \ remote_name)\n\n    # Save dataset with pandas in Dataset artifact\n \
          \   pulled_dataset_path = os.path.join(cloned_dir, dvc_file_dir, dvc_file_name)\n\
          \    tmp_dataset_path = \"/tmp/\" + dvc_file_name\n    dataset = pd.read_csv(pulled_dataset_path)\n\
          \    dataset.to_pickle(tmp_dataset_path)\n    os.rename(tmp_dataset_path,\
          \ dataset_artifact.path)\n\n"
        image: python:3.11.9
    exec-data-preparation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preparation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.3'\
          \ 'numpy==1.25.2' 'torch==2.3.0' 'scikit-learn==1.2.2' 'imblearn' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preparation(\n    dataset_artifact: Input[Dataset],\n  \
          \  X_train_artifact: Output[Dataset], \n    X_test_artifact: Output[Dataset],\n\
          \    y_train_artifact: Output[Dataset],\n    y_test_artifact: Output[Dataset],\n\
          \    test_size: float = 0.2, \n    random_state: int = 42\n    ):\n    import\
          \ pandas as pd\n    import numpy as np\n    from sklearn.model_selection\
          \ import train_test_split\n    from imblearn.over_sampling import SMOTE\n\
          \    from sklearn.preprocessing import StandardScaler\n    import torch\n\
          \    import os\n\n    # Load dataset from Dataset artifact\n    df = pd.read_pickle(dataset_artifact.path)\n\
          \n    # Handle null values and replace specific characters\n    #df = df.replace(['\
          \ ', '-',np.nan], 0) # There are null values\n    df = df.replace([' ',\
          \ '-', np.nan], np.nan)\n\n    # Selective columns for mean calculation\n\
          \    columns_to_convert = [\n        'CQI1', 'CQI2', 'CQI3', 'cSTD CQI',\
          \ 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n        'RSRP1', 'RSRP2', 'RSRP3',\
          \ 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n        'RSRQ1', 'RSRQ2', 'RSRQ3',\
          \ 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n        'SNR1', 'SNR2', 'SNR3',\
          \ 'sMajority', 's25 P', 's50 P', 's75 P'\n    ]\n    df[columns_to_convert]\
          \ = df[columns_to_convert].astype(float)\n\n    # Replace np.nan with mean\
          \ values for selective columns\n    df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n\
          \n    # Convert 'Stall' column to numerical values\n    df['Stall'].replace({'Yes':\
          \ 1, 'No': 0}, inplace=True)\n\n    X = df[columns_to_convert].values\n\
          \    y = df['Stall'].values\n\n    # Apply SMOTE for balancing the dataset\n\
          \    # oversample = SMOTE(random_state=random_state)\n    oversample = SMOTE()\n\
          \    X, y = oversample.fit_resample(X, y)\n\n    # Standardize the features\n\
          \    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    #\
          \ Convert to torch tensors\n    X = torch.tensor(X, dtype=torch.float32)\n\
          \    y = torch.tensor(y, dtype=torch.float32)\n\n    # Split the dataset\
          \ into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=test_size, random_state=random_state)\n\n    X_train_path\
          \ = \"/tmp/X_train.pt\"\n    X_test_path = \"/tmp/X_test.pt\"\n    y_train_path\
          \ = \"/tmp/y_train.pt\"\n    y_test_path = \"/tmp/y_test.pt\"\n    torch.save(X_train,\
          \ X_train_path)\n    os.rename(X_train_path, X_train_artifact.path)\n\n\
          \    torch.save(X_test, X_test_path)\n    os.rename(X_test_path, X_test_artifact.path)\n\
          \n    torch.save(y_train, y_train_path)\n    os.rename(y_train_path, y_train_artifact.path)\n\
          \n    torch.save(y_test, y_test_path)\n    os.rename(y_test_path, y_test_artifact.path)\n\
          \n"
        image: python:3.11.9
    exec-model-training:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_training
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'torch==2.3.0'\
          \ 'scikit-learn==1.2.2' 'numpy==1.25.2' 'Minio==7.2.5' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_training(\n    X_train_artifact: Input[Dataset], \n   \
          \ X_test_artifact: Input[Dataset],\n    y_train_artifact: Input[Dataset],\n\
          \    y_test_artifact: Input[Dataset],\n    metrics: Output[Metrics], \n\
          \    classification_metrics: Output[ClassificationMetrics], \n    model_trained_artifact:\
          \ Output[Model],\n    model_trained_artifact_path: OutputPath(str),\n  \
          \  lr: float,\n    epochs: int,\n    print_frequency: int,\n    minio_url:\
          \ str,\n    access_key: str,\n    secret_key: str,\n    bucket_name: str,\n\
          \    object_name: str\n    ):\n    import os\n    import torch\n    from\
          \ torch import nn\n    from sklearn.metrics import accuracy_score, confusion_matrix,\
          \ classification_report, precision_score, recall_score, f1_score\n    from\
          \ minio import Minio\n\n    # Build model with non-linear activation function\n\
          \    class InterruptionModel(nn.Module):\n        def __init__(self):\n\
          \            super().__init__()\n            self.layer_1 = nn.Linear(in_features=29,\
          \ out_features=200)\n            self.layer_2 = nn.Linear(in_features=200,\
          \ out_features=100)\n            self.layer_3 = nn.Linear(in_features=100,\
          \ out_features=1)\n            self.relu = nn.ReLU() # <- add in ReLU activation\
          \ function\n            # Can also put sigmoid in the model\n          \
          \  # This would mean you don't need to use it on the predictions\n     \
          \       # self.sigmoid = nn.Sigmoid()\n\n        def forward(self, x):\n\
          \            # Intersperse the ReLU activation function between layers\n\
          \            return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\
          \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model\
          \ = InterruptionModel().to(device)\n\n    # Setup loss and optimizer\n \
          \   loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),\
          \ lr=lr)\n\n    def accuracy_fn(y_true, y_pred):\n        correct = torch.eq(y_true,\
          \ y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n\
          \        acc = (correct / len(y_pred)) * 100\n        return acc\n\n   \
          \ # Fit the model\n    torch.manual_seed(42)\n    epochs = epochs\n\n  \
          \  # Put all data on target device\n    X_train = torch.load(X_train_artifact.path)\n\
          \    X_test = torch.load(X_test_artifact.path)\n    y_train = torch.load(y_train_artifact.path)\n\
          \    y_test = torch.load(y_test_artifact.path)\n    X_train, y_train = X_train.to(device),\
          \ y_train.to(device)\n    X_test, y_test = X_test.to(device), y_test.to(device)\n\
          \n    for epoch in range(epochs):\n        # 1. Forward pass\n        y_logits\
          \ = model(X_train).squeeze()\n\n        y_pred = torch.round(torch.sigmoid(y_logits))\
          \ # logits -> prediction probabilities -> prediction labels\n\n        #\
          \ 2. Calculate loss and accuracy\n        loss = loss_fn(y_logits, y_train)\
          \ # BCEWithLogitsLoss calculates loss using logits\n        acc = accuracy_fn(y_true=y_train,\n\
          \                        y_pred=y_pred)\n\n        # 3. Optimizer zero grad\n\
          \        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\
          \n        # 5. Optimizer step\n        optimizer.step()\n\n        ### Testing\n\
          \        model.eval()\n        with torch.no_grad():\n        # 1. Forward\
          \ pass\n            test_logits = model(X_test).squeeze()\n            #print(test_logits.shape)\n\
          \            test_pred = torch.round(torch.sigmoid(test_logits)) # logits\
          \ -> prediction probabilities -> prediction labels\n            # 2. Calcuate\
          \ loss and accuracy\n            test_loss = loss_fn(test_logits, y_test)\n\
          \            test_acc = accuracy_fn(y_true=y_test,\n                   \
          \             y_pred=test_pred)\n\n\n        # Print out what's happening\n\
          \        #if epoch % print_every == 0:\n        #    print(f\"Epoch: {epoch}\
          \ | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f},\
          \ Test Accuracy: {test_acc:.2f}%\")\n\n        model.eval()\n        with\
          \ torch.no_grad():\n            y_preds = torch.round(torch.sigmoid(model(X_test))).squeeze()\n\
          \n        if device == \"cuda\":\n            predictions = y_preds.cpu().numpy()\
          \ #if it is cuda, then this, otherwise y_pred.numpy()\n            true_labels\
          \ = y_test.cpu().numpy()\n        else:\n            predictions = y_preds.numpy()\n\
          \            true_labels = y_test.numpy()\n\n        # Confusion Matrix\n\
          \        cmatrix = confusion_matrix(true_labels, predictions)\n        #print(\"\
          Confusion Matrix:\", cmatrix)\n\n        # Metrics\n        accuracy = accuracy_score(true_labels,\
          \ predictions)\n        metrics.log_metric(\"Accuracy\", accuracy)\n   \
          \     #print('Accuracy: %f' % accuracy)\n\n        precision = precision_score(true_labels,\
          \  predictions, average='weighted')\n        metrics.log_metric(\"Precision\"\
          , precision)\n        #print('Precision: %f' % precision)\n\n        recall\
          \ = recall_score(true_labels, predictions, average='weighted')\n       \
          \ metrics.log_metric(\"Recall\", recall)\n        #print('Recall: %f' %\
          \ recall)\n\n        microf1 = f1_score(true_labels, predictions, average='micro')\n\
          \        metrics.log_metric(\"Micro F1 score\", microf1)\n        #print('Micro\
          \ F1 score: %f' % microf1)\n\n        macrof1 = f1_score(true_labels, predictions,\
          \ average='macro')\n        metrics.log_metric(\"Macro F1 score\", macrof1)\n\
          \        #print('Macro F1 score: %f' % macrof1)\n\n        target_names\
          \ = ['No-Stall', 'Stall']\n        # Print precision-recall report\n   \
          \     #print(classification_report(true_labels, predictions, target_names=target_names))\n\
          \n        # Classification Metrics artifact\n        cmatrix = cmatrix.tolist()\n\
          \        target_names = ['No-Stall', 'Stall']\n        classification_metrics.log_confusion_matrix(target_names,\
          \ cmatrix)\n\n        # Save model\n        model_path = \"/tmp/model.pt\"\
          \n        torch.save(model.state_dict(), model_path)\n        os.rename(model_path,\
          \ model_trained_artifact.path)\n\n        with open(model_trained_artifact_path,\
          \ 'w') as f:\n            f.write(model_trained_artifact.path)\n\n     \
          \   # Initialize Minio client with just the base URL (without path)\n  \
          \      client = Minio(\n            \"minio-service.kubeflow:9000\",  #\
          \ Ensure minio_url does not include a path, only the base URL (e.g., http://localhost:9000)\n\
          \            access_key=access_key,\n            secret_key=secret_key,\n\
          \            secure=False  # Minio is using HTTP on localhost:9000\n   \
          \     )\n\n        # Create the bucket if it does not exist\n        if\
          \ not client.bucket_exists(bucket_name):\n            client.make_bucket(bucket_name)\n\
          \            logger.info(f\"Successfully created bucket: {bucket_name}\"\
          )\n        else:\n            logger.info(f\"Bucket {bucket_name} already\
          \ exists\")\n\n        try:\n            # Upload the model file to the\
          \ specified path in the bucket\n            client.fput_object(bucket_name,\
          \ object_name, model_trained_artifact_path)\n            logger.info(f'Successfully\
          \ uploaded {model_file_path} to {bucket_name}/{object_name}')\n\n      \
          \  except Exception as e:\n            # Log and raise any upload errors\n\
          \            logger.error(f'Failed to upload model to Minio: {e}')\n   \
          \         raise Exception(f'Failed to upload model to Minio: {e}')\n\n"
        image: python:3.11.9
pipelineInfo:
  name: my-pipeline
root:
  dag:
    outputs:
      artifacts:
        model-training-classification_metrics:
          artifactSelectors:
          - outputArtifactKey: classification_metrics
            producerSubtask: model-training
        model-training-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: model-training
    tasks:
      data-ingestion:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-ingestion
        inputs:
          parameters:
            access_key:
              componentInputParameter: access_key
            branch_name:
              componentInputParameter: branch_name
            cloned_dir:
              componentInputParameter: cloned_dir
            dvc_file_dir:
              componentInputParameter: dvc_file_dir
            dvc_file_name:
              componentInputParameter: dvc_file_name
            github_token:
              componentInputParameter: github_token
            github_username:
              componentInputParameter: github_username
            minio_url:
              componentInputParameter: minio_url
            remote_name:
              componentInputParameter: remote_name
            remote_url:
              componentInputParameter: remote_url
            repo_url:
              componentInputParameter: repo_url
            secret_key:
              componentInputParameter: secret_key
        taskInfo:
          name: data-ingestion
      data-preparation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preparation
        dependentTasks:
        - data-ingestion
        inputs:
          artifacts:
            dataset_artifact:
              taskOutputArtifact:
                outputArtifactKey: dataset_artifact
                producerTask: data-ingestion
        taskInfo:
          name: data-preparation
      model-training:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-training
        dependentTasks:
        - data-preparation
        inputs:
          artifacts:
            X_test_artifact:
              taskOutputArtifact:
                outputArtifactKey: X_test_artifact
                producerTask: data-preparation
            X_train_artifact:
              taskOutputArtifact:
                outputArtifactKey: X_train_artifact
                producerTask: data-preparation
            y_test_artifact:
              taskOutputArtifact:
                outputArtifactKey: y_test_artifact
                producerTask: data-preparation
            y_train_artifact:
              taskOutputArtifact:
                outputArtifactKey: y_train_artifact
                producerTask: data-preparation
          parameters:
            access_key:
              componentInputParameter: access_key
            bucket_name:
              componentInputParameter: bucket_name
            epochs:
              componentInputParameter: epochs
            lr:
              componentInputParameter: lr
            minio_url:
              componentInputParameter: minio_url
            object_name:
              componentInputParameter: object_name
            print_frequency:
              componentInputParameter: print_frequency
            secret_key:
              componentInputParameter: secret_key
        taskInfo:
          name: model-training
  inputDefinitions:
    parameters:
      access_key:
        parameterType: STRING
      branch_name:
        parameterType: STRING
      bucket_name:
        parameterType: STRING
      cloned_dir:
        parameterType: STRING
      dvc_file_dir:
        parameterType: STRING
      dvc_file_name:
        parameterType: STRING
      epochs:
        parameterType: NUMBER_INTEGER
      github_token:
        parameterType: STRING
      github_username:
        parameterType: STRING
      lr:
        parameterType: NUMBER_DOUBLE
      minio_url:
        parameterType: STRING
      model_name:
        parameterType: STRING
      model_uri:
        parameterType: STRING
      namespace:
        parameterType: STRING
      object_name:
        parameterType: STRING
      print_frequency:
        parameterType: NUMBER_INTEGER
      remote_name:
        parameterType: STRING
      remote_url:
        parameterType: STRING
      repo_url:
        parameterType: STRING
      secret_key:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      model-training-classification_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      model-training-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
