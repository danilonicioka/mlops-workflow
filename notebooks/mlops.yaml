# PIPELINE DEFINITION
# Name: my-pipeline
# Inputs:
#    access_key: str
#    branch_name: str
#    bucket_name: str
#    cloned_dir: str
#    dvc_file_dir: str
#    dvc_file_name: str
#    epochs: int
#    github_repo_url: str
#    github_token: str
#    github_username: str
#    last_accuracy_object_name: str
#    lr: float
#    minio_url: str
#    model_name: str
#    model_object_name: str
#    namespace: str
#    performance_factor: float
#    print_frequency: int
#    remote_name: str
#    remote_url: str
#    secret_key: str
#    svc_acc: str
#    tmp_dir: str
#    tmp_file_last_acc: str
#    trigger_type: str
# Outputs:
#    data-ingestion-dataset_info: system.Metrics
#    model-serving-cond_info: system.Metrics
#    model-training-classification_metrics: system.ClassificationMetrics
#    model-training-metrics: system.Metrics
components:
  comp-data-ingestion:
    executorLabel: exec-data-ingestion
    inputDefinitions:
      parameters:
        access_key:
          parameterType: STRING
        branch_name:
          parameterType: STRING
        cloned_dir:
          parameterType: STRING
        dvc_file_dir:
          parameterType: STRING
        dvc_file_name:
          parameterType: STRING
        github_repo_url:
          parameterType: STRING
        github_token:
          parameterType: STRING
        github_username:
          parameterType: STRING
        minio_url:
          parameterType: STRING
        remote_name:
          parameterType: STRING
        remote_url:
          parameterType: STRING
        secret_key:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-data-preparation:
    executorLabel: exec-data-preparation
    inputDefinitions:
      artifacts:
        dataset_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        random_state:
          defaultValue: 42.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        test_size:
          defaultValue: 0.2
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-model-serving:
    executorLabel: exec-model-serving
    inputDefinitions:
      artifacts:
        up_model_cond:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
      parameters:
        bucket_name:
          parameterType: STRING
        model_name:
          parameterType: STRING
        namespace:
          parameterType: STRING
        svc_acc:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        cond_info:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
  comp-model-training:
    executorLabel: exec-model-training
    inputDefinitions:
      artifacts:
        X_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        X_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_test_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        y_train_artifact:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        access_key:
          parameterType: STRING
        bucket_name:
          parameterType: STRING
        epochs:
          parameterType: NUMBER_INTEGER
        last_accuracy_object_name:
          parameterType: STRING
        lr:
          parameterType: NUMBER_DOUBLE
        minio_url:
          parameterType: STRING
        model_object_name:
          parameterType: STRING
        performance_factor:
          parameterType: NUMBER_DOUBLE
        print_frequency:
          parameterType: NUMBER_INTEGER
        secret_key:
          parameterType: STRING
        tmp_dir:
          parameterType: STRING
        tmp_file_last_acc:
          parameterType: STRING
        trigger_type:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        classification_metrics:
          artifactType:
            schemaTitle: system.ClassificationMetrics
            schemaVersion: 0.0.1
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model_trained_artifact:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
        up_model_cond:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-data-ingestion:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_ingestion
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'gitpython'\
          \ 'dvc==3.54.1' 'dvc-s3==3.2.0' 'numpy==1.25.2' 'pandas==2.0.3' && \"$0\"\
          \ \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_ingestion(\n    github_repo_url: str,\n    cloned_dir: str,\n\
          \    branch_name: str,\n    github_username: str,\n    github_token: str,\n\
          \    remote_name: str,\n    remote_url: str,\n    minio_url: str,\n    access_key:\
          \ str,\n    secret_key: str,\n    dvc_file_dir: str,\n    dvc_file_name:\
          \ str,\n    dataset_artifact: Output[Dataset],\n    dataset_info: Output[Metrics]\n\
          \    ):\n    from git import Repo\n    from subprocess import run, CalledProcessError\n\
          \    import os\n    import pandas as pd\n\n    def clone_repository_with_token(github_repo_url,\
          \ cloned_dir, branch_name, github_username, github_token):\n        \"\"\
          \"Clone a Git repository using a GitHub token in the URL and specifying\
          \ the branch.\"\"\"\n        try:\n            # Construct the URL with\
          \ the GitHub username and token\n            url_with_token = f\"https://{github_username}:{github_token}@{github_repo_url.split('//')[1]}\"\
          \n\n            # Clone the repository from the specified branch\n     \
          \       repo = Repo.clone_from(url_with_token, cloned_dir, branch=branch_name)\n\
          \            return \"Repository cloned successfully\"\n        except Exception\
          \ as e:\n            return f\"Error occurred during repository cloning:\
          \ {e}\"\n\n    def configure_dvc_remote(cloned_dir, remote_name, remote_url,\
          \ minio_url, access_key, secret_key):\n        http_minio = f'http://{minio_url}'\n\
          \        \"\"\"Configure the Minio bucket as the DVC remote repository using\
          \ the `dvc remote` commands.\"\"\"\n        try:\n            # Add the\
          \ remote\n            run(\n                ['dvc', 'remote', 'add', '-d',\
          \ remote_name, remote_url],\n                cwd=cloned_dir,\n         \
          \       capture_output=True,\n                text=True,\n             \
          \   check=True\n            )\n\n            # Configure the endpoint URL\n\
          \            run(\n                ['dvc', 'remote', 'modify', remote_name,\
          \ 'endpointurl', http_minio],\n                cwd=cloned_dir,\n       \
          \         capture_output=True,\n                text=True,\n           \
          \     check=True\n            )\n\n            # Configure access key ID\n\
          \            run(\n                ['dvc', 'remote', 'modify', remote_name,\
          \ 'access_key_id', access_key],\n                cwd=cloned_dir,\n     \
          \           capture_output=True,\n                text=True,\n         \
          \       check=True\n            )\n\n            # Configure secret access\
          \ key\n            run(\n                ['dvc', 'remote', 'modify', remote_name,\
          \ 'secret_access_key', secret_key],\n                cwd=cloned_dir,\n \
          \               capture_output=True,\n                text=True,\n     \
          \           check=True\n            )\n\n            return f'Successfully\
          \ configured Minio bucket as DVC remote repository: {remote_name}'\n   \
          \     except CalledProcessError as e:\n            # Log and raise any errors\n\
          \            return f'Failed to configure DVC remote: {e.stderr}'\n\n  \
          \  def perform_dvc_pull(cloned_dir, remote_name):\n        \"\"\"Perform\
          \ a DVC pull to synchronize local data with the remote repository.\"\"\"\
          \n        try:\n            # Run the `dvc pull` command\n            result\
          \ = run(['dvc', 'pull', '-r', remote_name], cwd=cloned_dir, capture_output=True,\
          \ text=True)\n\n            # Check if the command executed successfully\n\
          \            if result.returncode != 0:\n                # Log and raise\
          \ an error if the command failed\n                error_message = f\"dvc\
          \ pull failed with error: {result.stderr}\"\n                raise Exception(error_message)\n\
          \n            # Log successful operation\n            return \"Successfully\
          \ pulled data from remote DVC repository\"\n\n        except Exception as\
          \ e:\n            # Log and handle the error\n            return f\"Error\
          \ occurred during dvc pull: {e}\"\n\n    # Call the functions\n    clone_result\
          \ = clone_repository_with_token(github_repo_url, cloned_dir, branch_name,\
          \ github_username, github_token)\n    configure_result = configure_dvc_remote(cloned_dir,\
          \ remote_name, remote_url, minio_url, access_key, secret_key)\n    dvc_pull_result\
          \ = perform_dvc_pull(cloned_dir, remote_name)\n\n    # Save dataset with\
          \ pandas in Dataset artifact\n    pulled_dataset_path = os.path.join(cloned_dir,\
          \ dvc_file_dir, dvc_file_name)\n    tmp_dataset_path = \"/tmp/\" + dvc_file_name\n\
          \    dataset = pd.read_csv(pulled_dataset_path)\n    dataset.to_pickle(tmp_dataset_path)\n\
          \    os.rename(tmp_dataset_path, dataset_artifact.path)\n\n    # save dataset\
          \ info to see on kubeflow graph\n    dataset_info.log_metric(\"Dataset size\"\
          , dataset.shape[0])\n\n"
        image: python:3.11.9
    exec-data-preparation:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - data_preparation
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.3'\
          \ 'numpy==1.25.2' 'torch==2.3.0' 'scikit-learn==1.2.2' 'imblearn' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef data_preparation(\n    dataset_artifact: Input[Dataset],\n  \
          \  X_train_artifact: Output[Dataset], \n    X_test_artifact: Output[Dataset],\n\
          \    y_train_artifact: Output[Dataset],\n    y_test_artifact: Output[Dataset],\n\
          \    test_size: float = 0.2, \n    random_state: int = 42\n    ):\n    import\
          \ pandas as pd\n    import numpy as np\n    from sklearn.model_selection\
          \ import train_test_split\n    from imblearn.over_sampling import SMOTE\n\
          \    from sklearn.preprocessing import StandardScaler\n    import torch\n\
          \    import os\n\n    # Load dataset from Dataset artifact\n    df = pd.read_pickle(dataset_artifact.path)\n\
          \n    # Handle null values and replace specific characters\n    #df = df.replace(['\
          \ ', '-',np.nan], 0) # There are null values\n    df = df.replace([' ',\
          \ '-', np.nan], np.nan)\n\n    # Selective columns for mean calculation\n\
          \    columns_to_convert = [\n        'CQI1', 'CQI2', 'CQI3', 'cSTD CQI',\
          \ 'cMajority', 'c25 P', 'c50 P', 'c75 P', \n        'RSRP1', 'RSRP2', 'RSRP3',\
          \ 'pMajority', 'p25 P', 'p50 P', 'p75 P', \n        'RSRQ1', 'RSRQ2', 'RSRQ3',\
          \ 'qMajority', 'q25 P', 'q50 P', 'q75 P', \n        'SNR1', 'SNR2', 'SNR3',\
          \ 'sMajority', 's25 P', 's50 P', 's75 P'\n    ]\n    df[columns_to_convert]\
          \ = df[columns_to_convert].astype(float)\n\n    # Replace np.nan with mean\
          \ values for selective columns\n    df[columns_to_convert] = df[columns_to_convert].fillna(df[columns_to_convert].mean())\n\
          \n    # Convert 'Stall' column to numerical values\n    df['Stall'].replace({'Yes':\
          \ 1, 'No': 0}, inplace=True)\n\n    X = df[columns_to_convert].values\n\
          \    y = df['Stall'].values\n\n    # Apply SMOTE for balancing the dataset\n\
          \    # oversample = SMOTE(random_state=random_state)\n    oversample = SMOTE()\n\
          \    X, y = oversample.fit_resample(X, y)\n\n    # Standardize the features\n\
          \    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    #\
          \ Convert to torch tensors\n    X = torch.tensor(X, dtype=torch.float32)\n\
          \    y = torch.tensor(y, dtype=torch.float32)\n\n    # Split the dataset\
          \ into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X,\
          \ y, test_size=test_size, random_state=random_state)\n\n    X_train_path\
          \ = \"/tmp/X_train.pt\"\n    X_test_path = \"/tmp/X_test.pt\"\n    y_train_path\
          \ = \"/tmp/y_train.pt\"\n    y_test_path = \"/tmp/y_test.pt\"\n    torch.save(X_train,\
          \ X_train_path)\n    os.rename(X_train_path, X_train_artifact.path)\n\n\
          \    torch.save(X_test, X_test_path)\n    os.rename(X_test_path, X_test_artifact.path)\n\
          \n    torch.save(y_train, y_train_path)\n    os.rename(y_train_path, y_train_artifact.path)\n\
          \n    torch.save(y_test, y_test_path)\n    os.rename(y_test_path, y_test_artifact.path)\n\
          \n"
        image: python:3.11.9
    exec-model-serving:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_serving
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'kserve==0.13.0'\
          \ 'kubernetes==30.1.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_serving(\n    up_model_cond: Input[Artifact],\n    cond_info:\
          \ Output[Metrics],\n    bucket_name: str,\n    model_name: str,\n    namespace:\
          \ str,\n    svc_acc: str\n):\n    # Create kserve instance\n    from kubernetes\
          \ import client \n    from kserve import KServeClient, constants, V1beta1InferenceService,\
          \ V1beta1InferenceServiceSpec, V1beta1PredictorSpec, V1beta1TorchServeSpec\n\
          \    from datetime import datetime\n    import time\n\n    # exec if a new\
          \ model was uploaded\n    with open(up_model_cond.path) as f:\n        up_model\
          \ = f.read()\n\n    if up_model == '1':\n        cond_info.log_metric(\"\
          Up model\", up_model)\n\n        #Inference server config\n        now =\
          \ datetime.now()\n        kserve_version='v1beta1'\n        api_version\
          \ = constants.KSERVE_GROUP + '/' + kserve_version\n\n        # with open(model_uri.path)\
          \ as f:\n        #     uri = f.read()\n        uri = f's3://{bucket_name}'\n\
          \n        isvc = V1beta1InferenceService(api_version=api_version,\n    \
          \                                   kind=constants.KSERVE_KIND,\n      \
          \                                 metadata=client.V1ObjectMeta(\n      \
          \                                     name=model_name, namespace=namespace,\
          \ annotations={'sidecar.istio.io/inject':'false'}),\n                  \
          \                     spec=V1beta1InferenceServiceSpec(\n              \
          \                         predictor=V1beta1PredictorSpec(\n            \
          \                               service_account_name=svc_acc,\n        \
          \                                   pytorch=(V1beta1TorchServeSpec(\n  \
          \                                             storage_uri=uri))))\n    \
          \    )\n\n        KServe = KServeClient()\n\n        #replace old inference\
          \ service with a new one\n        try:\n            KServe.delete(name=model_name,\
          \ namespace=namespace)\n            print(\"Old model deleted\")\n     \
          \   except:\n            print(\"Couldn't delete old model\")\n        time.sleep(10)\n\
          \n        KServe.create(isvc)\n    else:\n        cond_info.log_metric(\"\
          Up model\", '0')\n\n"
        image: python:3.11.9
    exec-model-training:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - model_training
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'torch==2.3.0'\
          \ 'scikit-learn==1.2.2' 'numpy==1.25.2' 'Minio==7.2.5' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef model_training(\n    X_train_artifact: Input[Dataset], \n   \
          \ X_test_artifact: Input[Dataset],\n    y_train_artifact: Input[Dataset],\n\
          \    y_test_artifact: Input[Dataset],\n    metrics: Output[Metrics], \n\
          \    classification_metrics: Output[ClassificationMetrics], \n    model_trained_artifact:\
          \ Output[Model],\n    up_model_cond: Output[Artifact],\n    lr: float,\n\
          \    epochs: int,\n    print_frequency: int,\n    minio_url: str,\n    access_key:\
          \ str,\n    secret_key: str,\n    bucket_name: str,\n    model_object_name:\
          \ str,\n    trigger_type: str,\n    performance_factor: float,\n    last_accuracy_object_name:\
          \ str,\n    tmp_dir: str,\n    tmp_file_last_acc: str\n    ):\n    import\
          \ os\n    import torch\n    from torch import nn\n    from sklearn.metrics\
          \ import accuracy_score, confusion_matrix, precision_score, recall_score,\
          \ f1_score\n    from minio import Minio\n\n    # Build model with non-linear\
          \ activation function\n    class InterruptionModel(nn.Module):\n       \
          \ def __init__(self):\n            super().__init__()\n            self.layer_1\
          \ = nn.Linear(in_features=29, out_features=200)\n            self.layer_2\
          \ = nn.Linear(in_features=200, out_features=100)\n            self.layer_3\
          \ = nn.Linear(in_features=100, out_features=1)\n            self.relu =\
          \ nn.ReLU() # <- add in ReLU activation function\n            # Can also\
          \ put sigmoid in the model\n            # This would mean you don't need\
          \ to use it on the predictions\n            # self.sigmoid = nn.Sigmoid()\n\
          \n        def forward(self, x):\n            # Intersperse the ReLU activation\
          \ function between layers\n            return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\
          \n    # Helper functions\n    def accuracy_fn(y_true, y_pred):\n       \
          \ correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates\
          \ where two tensors are equal\n        acc = (correct / len(y_pred)) * 100\n\
          \        return acc\n\n    def minio_setup(minio_url, access_key, secret_key):\n\
          \        # Initialize Minio client with just the base URL (without path)\n\
          \        client = Minio(\n            minio_url,  # Ensure minio_url does\
          \ not include a path, only the base URL (e.g., http://localhost:9000)\n\
          \            access_key=access_key,\n            secret_key=secret_key,\n\
          \            secure=False  # Minio is using HTTP on localhost:9000\n   \
          \     )\n        return client\n\n    def upload_file(client, bucket_name,\
          \ object_name, filepath):\n        # Create the bucket if it does not exist\n\
          \        if not client.bucket_exists(bucket_name):\n            client.make_bucket(bucket_name)\n\
          \            create_bucket_result = f\"Successfully created bucket: {bucket_name}\"\
          \n        else:\n            create_bucket_result = f\"Bucket {bucket_name}\
          \ already exists\"\n\n        try:\n            # Upload the file to the\
          \ specified path in the bucket\n            client.fput_object(bucket_name,\
          \ object_name, filepath)\n            return (f'Successfully uploaded {filepath}\
          \ to {bucket_name}/{object_name}')\n\n        except Exception as e:\n \
          \           # Log and raise any upload errors\n            raise Exception(f'Failed\
          \ to upload model to Minio: {e}')\n\n    def read_from_minio(client, bucket_name,\
          \ object_name):\n        \"\"\"\n        Function to read a file from a\
          \ MinIO bucket and convert its single content to a float.\n        If the\
          \ file is not found or is empty, it returns 0.0.\n\n        Args:\n    \
          \        client: minio client\n            bucket_name (str): The name of\
          \ the bucket in MinIO.\n            object_name (str): The name of the object\
          \ (file) in the bucket.\n\n        Returns:\n            float: The float\
          \ value converted from the file content, or 0.0 if the file is not found\
          \ or empty.\n        \"\"\"\n        try:\n            # Get the file from\
          \ the MinIO bucket\n            response = client.get_object(bucket_name,\
          \ object_name)\n\n            # Read the file content into a buffer\n  \
          \          file_data = response.read()\n\n            # Decode file content\
          \ and strip whitespace\n            content = file_data.decode('utf-8').strip()\n\
          \n            # If the content is empty, return 0.0\n            if not\
          \ content:\n                print(f\"File {object_name} is empty.\")\n \
          \               return 0.0\n\n            # Convert the content to a float\n\
          \            float_value = float(content)\n            return float_value\n\
          \n        except Exception as e:\n            # Handle file not found or\
          \ any other errors\n            print(f\"Error occurred: {e}\")\n      \
          \      return 0.0\n\n    def save_float_to_tempfile(float_value, dir_name,\
          \ file_name):\n        \"\"\"\n        Saves a float value to a specified\
          \ directory and file name.\n\n        Args:\n            float_value (float):\
          \ The float value to save.\n            dir_name (str): The name of the\
          \ directory to save the file in.\n            file_name (str): The name\
          \ of the file.\n\n        Returns:\n            str: The path to the file.\n\
          \        \"\"\"\n        # Ensure the directory exists\n        os.makedirs(dir_name,\
          \ exist_ok=True)\n        temp_file_path = os.path.join(dir_name, file_name)\n\
          \n        with open(temp_file_path, 'w') as temp_file:\n            # Convert\
          \ the float to a string, then write to file\n            temp_file.write(str(float_value))\n\
          \n        return temp_file_path\n\n    def get_accuracy_in_last_run(client,\
          \ bucket_name, object_name):\n        accuracy_in_last_run = read_from_minio(client,\
          \ bucket_name, object_name)\n        return accuracy_in_last_run\n\n   \
          \ def update_accuracy_in_last_run(client, bucket_name, object_name, new_value,\
          \ tmp_dir, tmp_file):\n        filepath = save_float_to_tempfile(new_value,\
          \ tmp_dir, tmp_file)\n        upload_file(client, bucket_name, object_name,\
          \ filepath)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"\
          cpu\"\n    model = InterruptionModel().to(device)\n\n    # Setup loss and\
          \ optimizer\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(),\
          \ lr=lr)\n\n    # Fit the model\n    torch.manual_seed(42)\n    epochs =\
          \ epochs\n\n    # Put all data on target device\n    X_train = torch.load(X_train_artifact.path)\n\
          \    X_test = torch.load(X_test_artifact.path)\n    y_train = torch.load(y_train_artifact.path)\n\
          \    y_test = torch.load(y_test_artifact.path)\n    X_train, y_train = X_train.to(device),\
          \ y_train.to(device)\n    X_test, y_test = X_test.to(device), y_test.to(device)\n\
          \n    for epoch in range(epochs):\n        # 1. Forward pass\n        y_logits\
          \ = model(X_train).squeeze()\n\n        y_pred = torch.round(torch.sigmoid(y_logits))\
          \ # logits -> prediction probabilities -> prediction labels\n\n        #\
          \ 2. Calculate loss and accuracy\n        loss = loss_fn(y_logits, y_train)\
          \ # BCEWithLogitsLoss calculates loss using logits\n        acc = accuracy_fn(y_true=y_train,\n\
          \                        y_pred=y_pred)\n\n        # 3. Optimizer zero grad\n\
          \        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\
          \n        # 5. Optimizer step\n        optimizer.step()\n\n        ### Testing\n\
          \        model.eval()\n        with torch.no_grad():\n        # 1. Forward\
          \ pass\n            test_logits = model(X_test).squeeze()\n            #print(test_logits.shape)\n\
          \            test_pred = torch.round(torch.sigmoid(test_logits)) # logits\
          \ -> prediction probabilities -> prediction labels\n            # 2. Calcuate\
          \ loss and accuracy\n            test_loss = loss_fn(test_logits, y_test)\n\
          \            test_acc = accuracy_fn(y_true=y_test,\n                   \
          \             y_pred=test_pred)\n\n\n        # Print out what's happening\n\
          \        if epoch % print_frequency == 0:\n            print(f\"Epoch: {epoch}\
          \ | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f},\
          \ Test Accuracy: {test_acc:.2f}%\")\n\n    model.eval()\n    with torch.no_grad():\n\
          \        y_preds = torch.round(torch.sigmoid(model(X_test))).squeeze()\n\
          \n    if device == \"cuda\":\n        predictions = y_preds.cpu().numpy()\
          \ #if it is cuda, then this, otherwise y_pred.numpy()\n        true_labels\
          \ = y_test.cpu().numpy()\n    else:\n        predictions = y_preds.numpy()\n\
          \        true_labels = y_test.numpy()\n\n    # Confusion Matrix\n    cmatrix\
          \ = confusion_matrix(true_labels, predictions)\n    #print(\"Confusion Matrix:\"\
          , cmatrix)\n\n    # Metrics\n    accuracy = accuracy_score(true_labels,\
          \ predictions)\n    metrics.log_metric(\"Accuracy\", accuracy)\n    #print('Accuracy:\
          \ %f' % accuracy)\n\n    # test accuracy\n    print(type(accuracy))\n\n\
          \    precision = precision_score(true_labels,  predictions, average='weighted')\n\
          \    metrics.log_metric(\"Precision\", precision)\n    #print('Precision:\
          \ %f' % precision)\n\n    recall = recall_score(true_labels, predictions,\
          \ average='weighted')\n    metrics.log_metric(\"Recall\", recall)\n    #print('Recall:\
          \ %f' % recall)\n\n    microf1 = f1_score(true_labels, predictions, average='micro')\n\
          \    metrics.log_metric(\"Micro F1 score\", microf1)\n    #print('Micro\
          \ F1 score: %f' % microf1)\n\n    macrof1 = f1_score(true_labels, predictions,\
          \ average='macro')\n    metrics.log_metric(\"Macro F1 score\", macrof1)\n\
          \    #print('Macro F1 score: %f' % macrof1)\n\n    target_names = ['No-Stall',\
          \ 'Stall']\n    # Print precision-recall report\n    #print(classification_report(true_labels,\
          \ predictions, target_names=target_names))\n\n    # Classification Metrics\
          \ artifact\n    cmatrix = cmatrix.tolist()\n    target_names = ['No-Stall',\
          \ 'Stall']\n    classification_metrics.log_confusion_matrix(target_names,\
          \ cmatrix)\n\n    # Save model\n    model_path = \"/tmp/model.pt\"\n   \
          \ torch.save(model.state_dict(), model_path)\n    os.rename(model_path,\
          \ model_trained_artifact.path)\n\n    # Setup minio client to upload and\
          \ read files\n    client = minio_setup(minio_url, access_key, secret_key)\n\
          \n    previous_accuracy = get_accuracy_in_last_run(client, bucket_name,\
          \ last_accuracy_object_name)\n\n    print(accuracy-previous_accuracy)\n\
          \    metrics.log_metric(\"current-previous accuracy\", accuracy-previous_accuracy)\n\
          \n    if trigger_type == '1' or trigger_type == '2':\n        up_model =\
          \ True\n    elif trigger_type == '3':\n        if accuracy - previous_accuracy\
          \ > performance_factor:\n            up_model = True\n            update_accuracy_in_last_run(client,\
          \ bucket_name, last_accuracy_object_name, accuracy, tmp_dir, tmp_file_last_acc)\n\
          \    else:\n        up_model = False\n        metrics.log_metric(\"up model\"\
          , '0')\n        with open(up_model_cond.path, 'w') as f:\n            f.write('0')\n\
          \n    if up_model:\n        metrics.log_metric(\"up model\", '1')\n    \
          \    with open(up_model_cond.path, 'w') as f:\n            f.write('1')\n\
          \        upload_model_result = upload_file(client, bucket_name, model_object_name,\
          \ model_trained_artifact.path)\n\n"
        image: python:3.11.9
pipelineInfo:
  name: my-pipeline
root:
  dag:
    outputs:
      artifacts:
        data-ingestion-dataset_info:
          artifactSelectors:
          - outputArtifactKey: dataset_info
            producerSubtask: data-ingestion
        model-serving-cond_info:
          artifactSelectors:
          - outputArtifactKey: cond_info
            producerSubtask: model-serving
        model-training-classification_metrics:
          artifactSelectors:
          - outputArtifactKey: classification_metrics
            producerSubtask: model-training
        model-training-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: model-training
    tasks:
      data-ingestion:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-ingestion
        inputs:
          parameters:
            access_key:
              componentInputParameter: access_key
            branch_name:
              componentInputParameter: branch_name
            cloned_dir:
              componentInputParameter: cloned_dir
            dvc_file_dir:
              componentInputParameter: dvc_file_dir
            dvc_file_name:
              componentInputParameter: dvc_file_name
            github_repo_url:
              componentInputParameter: github_repo_url
            github_token:
              componentInputParameter: github_token
            github_username:
              componentInputParameter: github_username
            minio_url:
              componentInputParameter: minio_url
            remote_name:
              componentInputParameter: remote_name
            remote_url:
              componentInputParameter: remote_url
            secret_key:
              componentInputParameter: secret_key
        taskInfo:
          name: data-ingestion
      data-preparation:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-data-preparation
        dependentTasks:
        - data-ingestion
        inputs:
          artifacts:
            dataset_artifact:
              taskOutputArtifact:
                outputArtifactKey: dataset_artifact
                producerTask: data-ingestion
        taskInfo:
          name: data-preparation
      model-serving:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-serving
        dependentTasks:
        - model-training
        inputs:
          artifacts:
            up_model_cond:
              taskOutputArtifact:
                outputArtifactKey: up_model_cond
                producerTask: model-training
          parameters:
            bucket_name:
              componentInputParameter: bucket_name
            model_name:
              componentInputParameter: model_name
            namespace:
              componentInputParameter: namespace
            svc_acc:
              componentInputParameter: svc_acc
        taskInfo:
          name: model-serving
      model-training:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-model-training
        dependentTasks:
        - data-preparation
        inputs:
          artifacts:
            X_test_artifact:
              taskOutputArtifact:
                outputArtifactKey: X_test_artifact
                producerTask: data-preparation
            X_train_artifact:
              taskOutputArtifact:
                outputArtifactKey: X_train_artifact
                producerTask: data-preparation
            y_test_artifact:
              taskOutputArtifact:
                outputArtifactKey: y_test_artifact
                producerTask: data-preparation
            y_train_artifact:
              taskOutputArtifact:
                outputArtifactKey: y_train_artifact
                producerTask: data-preparation
          parameters:
            access_key:
              componentInputParameter: access_key
            bucket_name:
              componentInputParameter: bucket_name
            epochs:
              componentInputParameter: epochs
            last_accuracy_object_name:
              componentInputParameter: last_accuracy_object_name
            lr:
              componentInputParameter: lr
            minio_url:
              componentInputParameter: minio_url
            model_object_name:
              componentInputParameter: model_object_name
            performance_factor:
              componentInputParameter: performance_factor
            print_frequency:
              componentInputParameter: print_frequency
            secret_key:
              componentInputParameter: secret_key
            tmp_dir:
              componentInputParameter: tmp_dir
            tmp_file_last_acc:
              componentInputParameter: tmp_file_last_acc
            trigger_type:
              componentInputParameter: trigger_type
        taskInfo:
          name: model-training
  inputDefinitions:
    parameters:
      access_key:
        parameterType: STRING
      branch_name:
        parameterType: STRING
      bucket_name:
        parameterType: STRING
      cloned_dir:
        parameterType: STRING
      dvc_file_dir:
        parameterType: STRING
      dvc_file_name:
        parameterType: STRING
      epochs:
        parameterType: NUMBER_INTEGER
      github_repo_url:
        parameterType: STRING
      github_token:
        parameterType: STRING
      github_username:
        parameterType: STRING
      last_accuracy_object_name:
        parameterType: STRING
      lr:
        parameterType: NUMBER_DOUBLE
      minio_url:
        parameterType: STRING
      model_name:
        parameterType: STRING
      model_object_name:
        parameterType: STRING
      namespace:
        parameterType: STRING
      performance_factor:
        parameterType: NUMBER_DOUBLE
      print_frequency:
        parameterType: NUMBER_INTEGER
      remote_name:
        parameterType: STRING
      remote_url:
        parameterType: STRING
      secret_key:
        parameterType: STRING
      svc_acc:
        parameterType: STRING
      tmp_dir:
        parameterType: STRING
      tmp_file_last_acc:
        parameterType: STRING
      trigger_type:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      data-ingestion-dataset_info:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-serving-cond_info:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      model-training-classification_metrics:
        artifactType:
          schemaTitle: system.ClassificationMetrics
          schemaVersion: 0.0.1
      model-training-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
