# PIPELINE DEFINITION
# Name: my-pipeline
# Inputs:
#    model_name: str
#    model_uri: str
#    namespace: str
components:
  comp-serve-a-model-with-kserve:
    executorLabel: exec-serve-a-model-with-kserve
    inputDefinitions:
      parameters:
        action:
          defaultValue: create
          isOptional: true
          parameterType: STRING
        autoscaling_target:
          defaultValue: '0'
          isOptional: true
          parameterType: STRING
        canary_traffic_percent:
          defaultValue: '100'
          isOptional: true
          parameterType: STRING
        custom_model_spec:
          defaultValue: '{}'
          isOptional: true
          parameterType: STRING
        enable_istio_sidecar:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        enable_isvc_status:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        framework:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        inferenceservice_yaml:
          defaultValue: '{}'
          isOptional: true
          parameterType: STRING
        max_replicas:
          defaultValue: '-1'
          isOptional: true
          parameterType: STRING
        min_replicas:
          defaultValue: '-1'
          isOptional: true
          parameterType: STRING
        model_name:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        model_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        namespace:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        request_timeout:
          defaultValue: '60'
          isOptional: true
          parameterType: STRING
        resource_limits:
          defaultValue: '{"cpu": "1", "memory": "1Gi"}'
          isOptional: true
          parameterType: STRING
        resource_requests:
          defaultValue: '{"cpu": "0.5", "memory": "512Mi"}'
          isOptional: true
          parameterType: STRING
        runtime_version:
          defaultValue: latest
          isOptional: true
          parameterType: STRING
        service_account:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        watch_timeout:
          defaultValue: '300'
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        inferenceservice_status:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-serve-a-model-with-kserve:
      container:
        args:
        - -u
        - kservedeployer.py
        - --action
        - '{{$.inputs.parameters[''action'']}}'
        - --model-name
        - '{{$.inputs.parameters[''model_name'']}}'
        - --model-uri
        - '{{$.inputs.parameters[''model_uri'']}}'
        - --canary-traffic-percent
        - '{{$.inputs.parameters[''canary_traffic_percent'']}}'
        - --namespace
        - '{{$.inputs.parameters[''namespace'']}}'
        - --framework
        - '{{$.inputs.parameters[''framework'']}}'
        - --runtime-version
        - '{{$.inputs.parameters[''runtime_version'']}}'
        - --resource-requests
        - '{{$.inputs.parameters[''resource_requests'']}}'
        - --resource-limits
        - '{{$.inputs.parameters[''resource_limits'']}}'
        - --custom-model-spec
        - '{{$.inputs.parameters[''custom_model_spec'']}}'
        - --autoscaling-target
        - '{{$.inputs.parameters[''autoscaling_target'']}}'
        - --service-account
        - '{{$.inputs.parameters[''service_account'']}}'
        - --enable-istio-sidecar
        - '{{$.inputs.parameters[''enable_istio_sidecar'']}}'
        - --output-path
        - '{{$.outputs.parameters[''inferenceservice_status''].output_file}}'
        - --inferenceservice-yaml
        - '{{$.inputs.parameters[''inferenceservice_yaml'']}}'
        - --watch-timeout
        - '{{$.inputs.parameters[''watch_timeout'']}}'
        - --min-replicas
        - '{{$.inputs.parameters[''min_replicas'']}}'
        - --max-replicas
        - '{{$.inputs.parameters[''max_replicas'']}}'
        - --request-timeout
        - '{{$.inputs.parameters[''request_timeout'']}}'
        - --enable-isvc-status
        - '{{$.inputs.parameters[''enable_isvc_status'']}}'
        command:
        - python
        image: quay.io/aipipeline/kserve-component:v0.12.0
pipelineInfo:
  name: my-pipeline
root:
  dag:
    tasks:
      serve-a-model-with-kserve:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-serve-a-model-with-kserve
        inputs:
          parameters:
            action:
              runtimeValue:
                constant: apply
            inferenceservice_yaml:
              runtimeValue:
                constant: "\napiVersion: serving.kserve.io/v1beta1\nkind: \"InferenceService\"\
                  \nmetadata:\n  name: {{$.inputs.parameters['pipelinechannel--model_name']}}\n\
                  \  namespace: {{$.inputs.parameters['pipelinechannel--namespace']}}\n\
                  spec:\n  predictor:\n    model:\n      modelFormat:\n        name:\
                  \ pytorch\n      storageUri: {{$.inputs.parameters['pipelinechannel--model_uri']}}\n"
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--model_uri:
              componentInputParameter: model_uri
            pipelinechannel--namespace:
              componentInputParameter: namespace
        taskInfo:
          name: serve-a-model-with-kserve
  inputDefinitions:
    parameters:
      model_name:
        parameterType: STRING
      model_uri:
        parameterType: STRING
      namespace:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.4.0
