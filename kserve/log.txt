WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.
2024-07-01T18:31:59,133 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties
2024-07-01T18:31:59,136 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...
2024-07-01T18:31:59,410 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml
2024-07-01T18:31:59,640 [INFO ] main org.pytorch.serve.ModelServer - 
Torchserve version: 0.9.0
TS Home: /home/venv/lib/python3.9/site-packages
Current directory: /home/model-server
Temp directory: /home/model-server/tmp
Metrics config path: /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml
Number of GPUs: 0
Number of CPUs: 1
Max heap size: 494 M
Python executable: /home/venv/bin/python
Config file: /mnt/models/config/config.properties
Inference address: http://0.0.0.0:8085
Management address: http://0.0.0.0:8081
Metrics address: http://0.0.0.0:8082
Model Store: /mnt/models/model-store
Initial Models: N/A
Log dir: /home/model-server/logs
Metrics dir: /home/model-server/logs
Netty threads: 4
Netty client threads: 0
Default workers per model: 1
Blacklist Regex: N/A
Maximum Response Size: 6553500
Maximum Request Size: 6553500
Limit Maximum Image Pixels: true
Prefer direct buffer: false
Allowed Urls: [file://.*|http(s)?://.*]
Custom python dependency for model allowed: true
Enable metrics API: true
Metrics mode: prometheus
Disable system metrics: false
Workflow Store: /mnt/models/model-store
Model config: N/A
2024-07-01T18:31:59,649 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...
2024-07-01T18:31:59,733 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {"name":"startup.cfg","modelCount":1,"models":{"youtubegoes5g":{"1.0":{"defaultVersion":true,"marName":"youtubegoes5g.mar","minWorkers":1,"maxWorkers":5,"batchSize":1,"maxBatchDelay":5000,"responseTimeout":120}}}}
2024-07-01T18:31:59,803 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot startup.cfg
2024-07-01T18:31:59,805 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot startup.cfg validated successfully
2024-07-01T18:31:59,913 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model youtubegoes5g
2024-07-01T18:31:59,913 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model youtubegoes5g
2024-07-01T18:31:59,913 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model youtubegoes5g
2024-07-01T18:31:59,914 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model youtubegoes5g loaded.
2024-07-01T18:31:59,914 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: youtubegoes5g, count: 1
2024-07-01T18:31:59,924 [DEBUG] W-9000-youtubegoes5g_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/home/venv/bin/python, /home/venv/lib/python3.9/site-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /home/model-server/tmp/.ts.sock.9000, --metrics-config, /home/venv/lib/python3.9/site-packages/ts/configs/metrics.yaml]
2024-07-01T18:31:59,924 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.
2024-07-01T18:32:00,217 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8085
2024-07-01T18:32:00,218 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.
2024-07-01T18:32:00,220 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://0.0.0.0:8081
2024-07-01T18:32:00,220 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.
2024-07-01T18:32:00,304 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://0.0.0.0:8082
Model server started.
2024-07-01T18:32:01,330 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.
INFO:root:Wrapper : Model names ['youtubegoes5g'], inference address http://0.0.0.0:8085, management address http://0.0.0.0:8081, grpc_inference_address, 0.0.0.0:7070, model store /mnt/models/model-store
INFO:root:Predict URL set to 0.0.0.0:8085
INFO:root:Explain URL set to 0.0.0.0:8085
INFO:root:Protocol version is v1
INFO:root:Copying contents of /mnt/models/model-store to local
INFO:root:TSModelRepo is initialized
INFO:kserve:Registering model: youtubegoes5g
INFO:kserve:Setting max asyncio worker threads as 10
INFO:kserve:Starting uvicorn with 1 workers
2024-07-01 18:32:02.626 uvicorn.error INFO:     Started server process [10]
2024-07-01 18:32:02.626 uvicorn.error INFO:     Waiting for application startup.
E0701 18:32:02.629642696      10 chttp2_server.cc:1045]                UNKNOWN:No address added out of total 1 resolved for '[::]:8081' {created_time:"2024-07-01T18:32:02.629542013+00:00", children:[UNKNOWN:Failed to add any wildcard listeners {created_time:"2024-07-01T18:32:02.62952986+00:00", children:[UNKNOWN:Unable to configure socket {fd:10, created_time:"2024-07-01T18:32:02.629482377+00:00", children:[UNKNOWN:Address already in use {syscall:"bind", os_error:"Address already in use", errno:98, created_time:"2024-07-01T18:32:02.629456227+00:00"}]}, UNKNOWN:Unable to configure socket {created_time:"2024-07-01T18:32:02.629525286+00:00", fd:10, children:[UNKNOWN:Address already in use {syscall:"bind", os_error:"Address already in use", errno:98, created_time:"2024-07-01T18:32:02.629520178+00:00"}]}]}]}
2024-07-01 18:32:02.629 uvicorn.error INFO:     Application startup complete.
2024-07-01 18:32:02.631 uvicorn.error ERROR:    Traceback (most recent call last):
  File "/usr/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/home/venv/lib/python3.9/site-packages/kserve/model_server.py", line 195, in servers_task
    await asyncio.gather(*servers)
  File "/home/venv/lib/python3.9/site-packages/kserve/protocol/grpc/server.py", line 61, in start
    self._server.add_insecure_port(listen_addr)
  File "/home/venv/lib/python3.9/site-packages/grpc/aio/_server.py", line 84, in add_insecure_port
    return _common.validate_port_binding_result(
  File "/home/venv/lib/python3.9/site-packages/grpc/_common.py", line 175, in validate_port_binding_result
    raise RuntimeError(_ERROR_MESSAGE_PORT_BINDING_FAILED % address)
RuntimeError: Failed to bind to address [::]:8081; set GRPC_VERBOSITY=debug environment variable to see detailed error message.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/venv/lib/python3.9/site-packages/starlette/routing.py", line 686, in lifespan
    await receive()
  File "/home/venv/lib/python3.9/site-packages/uvicorn/lifespan/on.py", line 137, in receive
    return await self.receive_queue.get()
  File "/usr/lib/python3.9/asyncio/queues.py", line 166, in get
    await getter
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/home/model-server/kserve_wrapper/__main__.py", line 126, in <module>
    ModelServer(
  File "/home/venv/lib/python3.9/site-packages/kserve/model_server.py", line 197, in start
    asyncio.run(servers_task())
  File "/usr/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/home/venv/lib/python3.9/site-packages/kserve/model_server.py", line 195, in servers_task
    await asyncio.gather(*servers)
  File "/home/venv/lib/python3.9/site-packages/kserve/protocol/grpc/server.py", line 61, in start
    self._server.add_insecure_port(listen_addr)
  File "/home/venv/lib/python3.9/site-packages/grpc/aio/_server.py", line 84, in add_insecure_port
    return _common.validate_port_binding_result(
  File "/home/venv/lib/python3.9/site-packages/grpc/_common.py", line 175, in validate_port_binding_result
    raise RuntimeError(_ERROR_MESSAGE_PORT_BINDING_FAILED % address)
RuntimeError: Failed to bind to address [::]:8081; set GRPC_VERBOSITY=debug environment variable to see detailed error message.
